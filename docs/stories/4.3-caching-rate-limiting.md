# Story 4.3: Caching Semantico e Rate Limiting Configurabili

**Status:** Draft

## Metadata
- **ID**: 4.3
- **Type**: Feature / Post-MVP Enhancement / Cost Optimization
- **Epic**: Epic 4 ‚Äî Post-MVP Enhancements
- **Priority**: High (Cost Management)
- **Complexity**: High
- **Effort Estimate**: 12-16 ore

---

## Story

**As a** Amministratore di Sistema / Professore (Admin),  
**I want** configurare caching semantico per query simili e rate limiting per utenti/IP,  
**so that** posso controllare i costi operativi LLM/embedding, prevenire abuse, e garantire performance ottimali del sistema RAG.

**Business Value**: Riduzione costi operativi mensili del 20-40% tramite cache hit su query ripetute/simili, protezione da abuse tramite throttling configurabile, e visibilit√† completa su consumo risorse API.

---

## Context & Background

### Current State (Post-MVP)
- Sistema RAG operativo con endpoint chat (`/api/v1/chat/sessions/{session_id}/messages`)
- Ogni query utente genera:
  - **Embedding API call** (OpenAI text-embedding-3-small)
  - **Similarity search** su Supabase vector store
  - **LLM API call** (OpenAI GPT-4 / GPT-3.5-turbo) per generation
- Rate limiting basico presente (Story 4.1: 10 req/hour admin debug)
- Nessun caching semantico ‚Üí query duplicate/simili generano costi ripetuti
- Nessuna visibilit√† aggregata costi API

### Problem Statement
- **Costi operativi non ottimizzati**: query ripetute/simili (es. "cos'√® la spalla congelata?" ripetuta da 10 studenti) generano costi evitabili
- **Rischio abuse**: studenti/bot potrebbero inviare query massive senza throttling
- **Scarsa visibilit√† costi**: admin non ha dashboard per monitorare consumo API tokens
- **Configurazione rigida**: limiti rate hardcoded, no flessibilit√† per adattare a pattern utilizzo reale

### Desired State
- **Caching semantico intelligente**: query con similarit√† embedding > threshold (es. 0.95) recuperano risposta da cache invece di invocare LLM
- **Rate limiting configurabile**: limiti per IP, session, ruolo (admin/student) con configurazione centralizzata
- **Dashboard costi**: visibilit√† real-time su embedding calls, LLM tokens consumati, cache hit rate
- **Alert automatici**: notifiche quando consumo supera soglie configurabili

---

## Acceptance Criteria

### Caching Semantico

1. **Cache Layer Implementation**: Implementare cache semantico con:
   - Storage: Redis (in-memory) o Supabase table `semantic_cache`
   - Key: embedding query (vector 1536 dimensioni)
   - Value: risposta LLM cached + metadata (timestamp, session_id_hash, hit_count)

2. **Cache Hit Logic**: Prima di invocare LLM:
   - Calcolare embedding query input
   - Eseguire similarity search su cache store
   - Se similarit√† > threshold configurabile (default 0.95) ‚Üí restituire cached response
   - Altrimenti ‚Üí invocare LLM e salvare in cache

3. **Cache Invalidation**: 
   - TTL configurabile (default: 7 giorni)
   - Manual flush via admin endpoint `/api/v1/admin/cache/flush`
   - Auto-eviction quando cache size > limit (LRU policy)

4. **Cache Metadata**: Response include header `X-Cache-Status: HIT|MISS` per trasparenza

5. **Performance**: Cache lookup < 50ms (p95)

### Rate Limiting

6. **Multi-Level Throttling**: Implementare rate limits configurabili per:
   - **IP**: limiti per indirizzo IP (protezione DDoS/abuse)
   - **Session**: limiti per session_id (protezione abuse studente singolo)
   - **Ruolo**: limiti differenziati admin/student (admin pi√π permissivi)

7. **Rate Limit Configuration**: File `config/rate_limits.yaml`:
   ```yaml
   rate_limits:
     students:
       per_ip: 30/hour
       per_session: 20/hour
     admin:
       per_ip: 100/hour
       per_session: 50/hour
   ```

8. **Rate Limit Headers**: Response HTTP include headers standard:
   - `X-RateLimit-Limit`: limite massimo
   - `X-RateLimit-Remaining`: richieste rimanenti
   - `X-RateLimit-Reset`: timestamp reset contatore
   - `Retry-After`: secondi attesa se 429 Too Many Requests

9. **Rate Limit Bypass**: Admin pu√≤ temporaneamente disabilitare limiti per session specifica via endpoint `/api/v1/admin/rate-limits/bypass`

### Cost Monitoring Dashboard

10. **Endpoint Metriche**: `GET /api/v1/admin/costs/metrics` restituisce:
    ```json
    {
      "period": "last_7_days",
      "embedding_api_calls": 1250,
      "llm_tokens_consumed": {
        "input": 45000,
        "output": 32000,
        "total": 77000
      },
      "cache_hit_rate": 0.35,
      "estimated_cost_usd": 12.45,
      "cost_breakdown": {
        "embedding": 2.50,
        "llm": 9.95
      }
    }
    ```

11. **Dashboard UI**: Sezione `/admin/costs` con:
    - KPI cards: total API calls, cache hit rate, estimated cost
    - Line chart: costi giornalieri ultimi 30 giorni
    - Breakdown chart: costi per tipo (embedding vs LLM)
    - Table top 10 query costose (ordinata per tokens consumati)

12. **Alert Configuration**: Admin pu√≤ configurare soglie alert via UI:
    - Budget mensile (es. $50)
    - Notifica quando consumo > 80% budget
    - Alert via log strutturato (future: email/Slack integration)

### Configuration Management

13. **Centralized Config**: File `config/cache_rate_limits.yaml` con parametri:
    ```yaml
    semantic_cache:
      enabled: true
      similarity_threshold: 0.95
      ttl_days: 7
      max_entries: 10000
    
    rate_limits:
      students:
        per_ip: "30/hour"
        per_session: "20/hour"
      admin:
        per_ip: "100/hour"
        per_session: "50/hour"
    
    cost_monitoring:
      alert_threshold_usd: 50
      alert_enabled: true
    ```

14. **Hot Reload**: Modifiche config file applicate senza restart server (graceful reload)

15. **Admin Override UI**: Pagina `/admin/settings` permette modifica parametri via form (alternative a edit manuale YAML)

---

## Technical Implementation

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Client    ‚îÇ
‚îÇ  (Student)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ POST /api/v1/chat/sessions/{id}/messages
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Rate Limiter Middleware             ‚îÇ
‚îÇ  (Check IP + Session limits)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ PASS
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       Semantic Cache Lookup                 ‚îÇ
‚îÇ  1. Generate query embedding                ‚îÇ
‚îÇ  2. Search cache with similarity > 0.95     ‚îÇ
‚îÇ  3. If HIT ‚Üí return cached response         ‚îÇ
‚îÇ  4. If MISS ‚Üí proceed to RAG pipeline       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ MISS
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       Standard RAG Pipeline                 ‚îÇ
‚îÇ  1. Semantic search (vector store)          ‚îÇ
‚îÇ  2. LLM generation (OpenAI)                 ‚îÇ
‚îÇ  3. Store response in cache                 ‚îÇ
‚îÇ  4. Log metrics (tokens, latency)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Cost Tracking Module                    ‚îÇ
‚îÇ  (Update counters: API calls, tokens)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Backend Implementation

#### 1. Semantic Cache Module

**File**: `apps/api/api/cache/semantic_cache.py` (nuovo)

```python
from typing import Optional
import asyncpg
import numpy as np
from datetime import datetime, timedelta
from api.database import get_db_connection

class SemanticCache:
    """
    Cache semantico basato su similarit√† embedding.
    Storage: Supabase table semantic_cache con pgvector.
    """
    
    def __init__(
        self, 
        similarity_threshold: float = 0.95,
        ttl_days: int = 7
    ):
        self.similarity_threshold = similarity_threshold
        self.ttl_days = ttl_days
    
    async def lookup(
        self, 
        query_embedding: list[float],
        conn: asyncpg.Connection
    ) -> Optional[dict]:
        """
        Cerca in cache query con embedding simile.
        
        Returns:
            dict con cached_response e metadata se HIT, None se MISS
        """
        query = """
            SELECT 
                response_text,
                response_metadata,
                hit_count,
                created_at
            FROM semantic_cache
            WHERE 
                created_at > NOW() - INTERVAL '$1 days'
                AND (1 - (query_embedding <=> $2::vector)) > $3
            ORDER BY (1 - (query_embedding <=> $2::vector)) DESC
            LIMIT 1
        """
        
        row = await conn.fetchrow(
            query,
            self.ttl_days,
            query_embedding,
            self.similarity_threshold
        )
        
        if row:
            # Incrementa hit_count
            await self._increment_hit_count(row['cache_id'], conn)
            
            return {
                "response_text": row['response_text'],
                "metadata": row['response_metadata'],
                "cache_hit": True,
                "cached_at": row['created_at'].isoformat()
            }
        
        return None
    
    async def store(
        self,
        query_text: str,
        query_embedding: list[float],
        response_text: str,
        response_metadata: dict,
        conn: asyncpg.Connection
    ):
        """Salva response in cache."""
        query = """
            INSERT INTO semantic_cache (
                query_text,
                query_embedding,
                response_text,
                response_metadata,
                hit_count,
                created_at
            ) VALUES ($1, $2::vector, $3, $4, 0, NOW())
        """
        
        await conn.execute(
            query,
            query_text,
            query_embedding,
            response_text,
            response_metadata
        )
    
    async def _increment_hit_count(
        self, 
        cache_id: str, 
        conn: asyncpg.Connection
    ):
        """Incrementa contatore hit per cache entry."""
        await conn.execute(
            "UPDATE semantic_cache SET hit_count = hit_count + 1 WHERE id = $1",
            cache_id
        )
```

**Database Migration**: `supabase/migrations/20251006000000_create_semantic_cache.sql`

```sql
-- Semantic Cache Table
CREATE TABLE IF NOT EXISTS semantic_cache (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    query_text TEXT NOT NULL,
    query_embedding VECTOR(1536) NOT NULL,
    response_text TEXT NOT NULL,
    response_metadata JSONB,
    hit_count INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Index per similarity search ottimizzato
CREATE INDEX idx_semantic_cache_embedding ON semantic_cache 
USING ivfflat (query_embedding vector_cosine_ops)
WITH (lists = 100);

-- Index per TTL cleanup
CREATE INDEX idx_semantic_cache_created_at ON semantic_cache(created_at DESC);

-- RLS Policy: Solo service_role pu√≤ scrivere/leggere
ALTER TABLE semantic_cache ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Service role full access"
    ON semantic_cache
    FOR ALL
    USING (auth.jwt() ->> 'role' = 'service_role');
```

---

#### 2. Rate Limiting Enhancement

**File**: `apps/api/api/middleware/rate_limiter.py` (nuovo)

```python
from typing import Annotated
from fastapi import Depends, HTTPException, status, Request
from slowapi import Limiter
from slowapi.util import get_remote_address
import yaml
import os

# Load config
with open("config/rate_limits.yaml") as f:
    RATE_LIMITS_CONFIG = yaml.safe_load(f)

limiter = Limiter(key_func=get_remote_address)

def get_rate_limit_for_user(role: str, limit_type: str) -> str:
    """
    Recupera rate limit da config per ruolo e tipo.
    
    Args:
        role: "admin" | "student"
        limit_type: "per_ip" | "per_session"
    
    Returns:
        Rate limit string (es. "30/hour")
    """
    return RATE_LIMITS_CONFIG['rate_limits'][role][limit_type]

async def check_rate_limit(
    request: Request,
    role: str = "student"
):
    """
    Dependency per verificare rate limit multi-level.
    
    Verifica:
    - Rate limit per IP
    - Rate limit per session_id (header X-Session-ID)
    """
    ip_limit = get_rate_limit_for_user(role, "per_ip")
    session_limit = get_rate_limit_for_user(role, "per_session")
    
    # Check IP limit (SlowAPI automatico)
    # check_rate_limit viene chiamato automaticamente da @limiter.limit()
    
    # Check session limit (custom logic)
    session_id = request.headers.get("X-Session-ID")
    if session_id:
        # Redis counter per session_id
        # TODO: implementare contatore Redis con expiry
        pass
    
    return True
```

**Integration in main.py**:

```python
from api.middleware.rate_limiter import check_rate_limit, limiter

@app.post("/api/v1/chat/sessions/{session_id}/messages")
@limiter.limit(lambda: get_rate_limit_for_user("student", "per_ip"))
async def create_message(
    session_id: str,
    request: ChatRequest,
    rate_check: Annotated[bool, Depends(check_rate_limit)]
):
    # Existing chat logic...
    pass
```

---

#### 3. Cost Monitoring Module

**File**: `apps/api/api/costs/tracker.py` (nuovo)

```python
from typing import Dict, Any
from datetime import datetime, timedelta
import asyncpg

class CostTracker:
    """
    Traccia costi API per embedding e LLM calls.
    """
    
    # Prezzi OpenAI (aggiornare con pricing reali)
    PRICING = {
        "embedding": 0.0001 / 1000,  # $0.0001 per 1K tokens
        "llm_input": 0.03 / 1000,     # GPT-4 Turbo input
        "llm_output": 0.06 / 1000     # GPT-4 Turbo output
    }
    
    async def log_api_call(
        self,
        call_type: str,  # "embedding" | "llm"
        tokens_consumed: int,
        session_id_hash: str,
        conn: asyncpg.Connection
    ):
        """Registra chiamata API con tokens consumati."""
        query = """
            INSERT INTO api_cost_log (
                call_type,
                tokens_consumed,
                estimated_cost_usd,
                session_id_hash,
                timestamp
            ) VALUES ($1, $2, $3, $4, NOW())
        """
        
        cost = self._calculate_cost(call_type, tokens_consumed)
        
        await conn.execute(
            query,
            call_type,
            tokens_consumed,
            cost,
            session_id_hash
        )
    
    def _calculate_cost(self, call_type: str, tokens: int) -> float:
        """Calcola costo stimato per chiamata API."""
        if call_type == "embedding":
            return tokens * self.PRICING["embedding"]
        elif call_type == "llm_input":
            return tokens * self.PRICING["llm_input"]
        elif call_type == "llm_output":
            return tokens * self.PRICING["llm_output"]
        return 0.0
    
    async def get_cost_metrics(
        self,
        period_days: int,
        conn: asyncpg.Connection
    ) -> Dict[str, Any]:
        """Recupera metriche costi per periodo specificato."""
        query = """
            SELECT 
                call_type,
                COUNT(*) as call_count,
                SUM(tokens_consumed) as total_tokens,
                SUM(estimated_cost_usd) as total_cost
            FROM api_cost_log
            WHERE timestamp > NOW() - INTERVAL '$1 days'
            GROUP BY call_type
        """
        
        rows = await conn.fetch(query, period_days)
        
        # Calcola cache hit rate
        cache_hits = await self._get_cache_hits(period_days, conn)
        total_queries = await self._get_total_queries(period_days, conn)
        cache_hit_rate = cache_hits / total_queries if total_queries > 0 else 0.0
        
        return {
            "period": f"last_{period_days}_days",
            "embedding_api_calls": next((r['call_count'] for r in rows if r['call_type'] == 'embedding'), 0),
            "llm_tokens_consumed": {
                "input": next((r['total_tokens'] for r in rows if r['call_type'] == 'llm_input'), 0),
                "output": next((r['total_tokens'] for r in rows if r['call_type'] == 'llm_output'), 0),
            },
            "cache_hit_rate": cache_hit_rate,
            "estimated_cost_usd": sum(r['total_cost'] for r in rows),
            "cost_breakdown": {
                call['call_type']: call['total_cost']
                for call in rows
            }
        }
```

**Database Migration**: `supabase/migrations/20251006000001_create_api_cost_log.sql`

```sql
-- API Cost Log Table
CREATE TABLE IF NOT EXISTS api_cost_log (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    call_type TEXT NOT NULL CHECK (call_type IN ('embedding', 'llm_input', 'llm_output')),
    tokens_consumed INTEGER NOT NULL,
    estimated_cost_usd NUMERIC(10, 6) NOT NULL,
    session_id_hash TEXT,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Indexes per query aggregazioni
CREATE INDEX idx_api_cost_log_timestamp ON api_cost_log(timestamp DESC);
CREATE INDEX idx_api_cost_log_call_type ON api_cost_log(call_type);

-- RLS Policy: Admin read-only
ALTER TABLE api_cost_log ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Admin can read cost logs"
    ON api_cost_log
    FOR SELECT
    USING (auth.jwt() ->> 'role' = 'admin');
```

---

### Frontend Implementation

#### Cost Monitoring Dashboard

**File**: `apps/web/src/pages/CostMonitoringPage.tsx` (nuovo)

```tsx
import { useEffect, useState } from "react";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { Badge } from "@/components/ui/badge";
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from "recharts";
import { authService } from "@/services/authService";

interface CostMetrics {
  period: string;
  embedding_api_calls: number;
  llm_tokens_consumed: {
    input: number;
    output: number;
  };
  cache_hit_rate: number;
  estimated_cost_usd: number;
  cost_breakdown: Record<string, number>;
}

export default function CostMonitoringPage() {
  const [metrics, setMetrics] = useState<CostMetrics | null>(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    fetchMetrics();
  }, []);

  const fetchMetrics = async () => {
    try {
      setLoading(true);
      const { data } = await authService.getSession();
      
      const res = await fetch("/api/v1/admin/costs/metrics?period_days=7", {
        headers: {
          Authorization: `Bearer ${data.session?.access_token}`,
        },
      });
      
      const data_metrics = await res.json();
      setMetrics(data_metrics);
    } catch (err) {
      console.error(err);
    } finally {
      setLoading(false);
    }
  };

  if (loading) return <div>Caricamento...</div>;
  if (!metrics) return <div>Nessun dato</div>;

  return (
    <div className="mx-auto max-w-7xl space-y-6 p-4">
      <h1 className="text-3xl font-bold">Monitoraggio Costi API</h1>
      
      {/* KPI Cards */}
      <div className="grid gap-4 md:grid-cols-4">
        <Card>
          <CardHeader>
            <CardTitle className="text-sm">Costo Stimato</CardTitle>
          </CardHeader>
          <CardContent>
            <p className="text-2xl font-bold">${metrics.estimated_cost_usd.toFixed(2)}</p>
            <p className="text-xs text-muted-foreground">{metrics.period}</p>
          </CardContent>
        </Card>
        
        <Card>
          <CardHeader>
            <CardTitle className="text-sm">Cache Hit Rate</CardTitle>
          </CardHeader>
          <CardContent>
            <p className="text-2xl font-bold">{(metrics.cache_hit_rate * 100).toFixed(1)}%</p>
            <Badge variant={metrics.cache_hit_rate > 0.3 ? "default" : "secondary"}>
              {metrics.cache_hit_rate > 0.3 ? "Ottimo" : "Basso"}
            </Badge>
          </CardContent>
        </Card>
        
        <Card>
          <CardHeader>
            <CardTitle className="text-sm">Embedding Calls</CardTitle>
          </CardHeader>
          <CardContent>
            <p className="text-2xl font-bold">{metrics.embedding_api_calls}</p>
          </CardContent>
        </Card>
        
        <Card>
          <CardHeader>
            <CardTitle className="text-sm">LLM Tokens</CardTitle>
          </CardHeader>
          <CardContent>
            <p className="text-2xl font-bold">
              {(metrics.llm_tokens_consumed.input + metrics.llm_tokens_consumed.output).toLocaleString()}
            </p>
            <p className="text-xs text-muted-foreground">
              Input: {metrics.llm_tokens_consumed.input.toLocaleString()} | 
              Output: {metrics.llm_tokens_consumed.output.toLocaleString()}
            </p>
          </CardContent>
        </Card>
      </div>
      
      {/* Cost Breakdown Chart */}
      <Card>
        <CardHeader>
          <CardTitle>Breakdown Costi</CardTitle>
        </CardHeader>
        <CardContent>
          <ResponsiveContainer width="100%" height={300}>
            <LineChart data={/* TODO: daily cost data */}>
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis dataKey="date" />
              <YAxis />
              <Tooltip />
              <Legend />
              <Line type="monotone" dataKey="embedding_cost" stroke="#8884d8" name="Embedding" />
              <Line type="monotone" dataKey="llm_cost" stroke="#82ca9d" name="LLM" />
            </LineChart>
          </ResponsiveContainer>
        </CardContent>
      </Card>
    </div>
  );
}
```

**Route Integration**: `apps/web/src/App.tsx`

```tsx
import CostMonitoringPage from "./pages/CostMonitoringPage";

// Aggiungere route protetta:
<Route
  path="/admin/costs"
  element={
    <AdminGuard>
      <CostMonitoringPage />
    </AdminGuard>
  }
/>
```

---

## Dependencies

**Prerequisiti**:
- ‚úÖ Story 3.2 (Augmented Generation Endpoint) - Done (chat endpoint esistente)
- ‚úÖ Story 4.2 (Analytics Dashboard) - Done (pattern dashboard UI)
- ‚úÖ Supabase pgvector extension abilitata
- ‚úÖ Redis deployment (opzionale, alternative: Supabase-based rate limiting)

**Dipendenze Esterne**:
- ‚úÖ OpenAI Python SDK (gi√† installato)
- ‚úÖ PyYAML per config parsing: `poetry add pyyaml`
- ‚ö†Ô∏è **Redis Python client** (opzionale): `poetry add redis` se si sceglie Redis per rate limiting
- ‚úÖ Recharts (gi√† installato per Story 4.2)

---

## Out of Scope

- **Email/Slack notifications**: alert solo via log strutturato (future Story 4.3.1)
- **Multi-tenant cost tracking**: costi aggregati globali, no per-course breakdown (future enhancement)
- **Advanced cache strategies**: LRU eviction basico, no ML-based cache warming (future Epic 5)
- **Real-time cost dashboard**: refresh manuale, no WebSocket streaming (future enhancement)
- **Custom pricing models**: hard-coded OpenAI pricing, no support altri provider (future enhancement)

---

## Risks

| ID | Description | Probability | Impact | Mitigation |
|----|-------------|-------------|--------|------------|
| R-4.3-1 | Redis dependency aumenta complessit√† deployment | Media | Medio | Alternative: rate limiting basato su Supabase table con TTL; Redis opzionale |
| R-4.3-2 | Semantic cache miss rate alto (cache inefficace) | Media | Alto | Tuning similarity threshold; A/B testing 0.90 vs 0.95; analytics cache performance |
| R-4.3-3 | Cache storage growth unbounded | Bassa | Medio | TTL 7 giorni + LRU eviction; monitoring size cache table |
| R-4.3-4 | Cost tracking overhead performance | Bassa | Medio | Async logging; batch insert; minimizzare DB writes |
| R-4.3-5 | Config hot reload failure ‚Üí service disruption | Bassa | Alto | Validation schema YAML; graceful fallback su config corrente se reload fail |
| R-4.3-6 | False positive rate limiting ‚Üí UX degradata | Media | Alto | Generous default limits; bypass mechanism per admin; monitoring 429 rate |

---

## Testing Strategy

### Unit Tests (Backend)

**File**: `apps/api/tests/test_semantic_cache.py`

**Test Cases**: 10 test
1. Cache lookup: HIT con similarit√† 0.96
2. Cache lookup: MISS con similarit√† 0.85
3. Cache store: insert corretto
4. Cache hit_count: increment su lookup
5. TTL: cache entry > 7 giorni ignorata
6. Similarity threshold: config 0.90 vs 0.95
7. LRU eviction: max_entries limit
8. Concurrent access: thread-safe operations
9. Embedding vector validation
10. Performance: lookup < 50ms

---

**File**: `apps/api/tests/test_rate_limiter.py`

**Test Cases**: 8 test
1. Rate limit IP: block dopo 30 req/hour
2. Rate limit session: block dopo 20 req/hour
3. Rate limit admin: 100 req/hour permessi
4. Response headers: X-RateLimit-* corretti
5. Retry-After header: presente su 429
6. Config reload: limiti aggiornati senza restart
7. Bypass mechanism: admin override funziona
8. Multi-level: IP + session limiti applicati contemporaneamente

---

**File**: `apps/api/tests/test_cost_tracker.py`

**Test Cases**: 8 test
1. Log API call: insert embedding call
2. Log API call: insert LLM call con input/output tokens
3. Cost calculation: pricing corretto
4. Metrics aggregation: period_days=7 dati corretti
5. Cache hit rate: calcolo accurato
6. Cost breakdown: embedding vs LLM separati
7. Alert threshold: trigger quando costo > limite
8. Performance: metrics query < 200ms

---

### Integration Tests

**File**: `apps/api/tests/test_cache_integration.py`

**Test Cases**: 5 test
1. End-to-end cache HIT: query ripetuta usa cache
2. End-to-end cache MISS: query nuova bypassa cache
3. Cost savings: cache HIT riduce LLM calls
4. Performance: cache lookup + RAG pipeline timing
5. Concurrency: 10 query simultanee gestite correttamente

---

### E2E Tests

**File**: `apps/web/tests/story-4.3.spec.ts`

**Scenarios**: 8 test
1. Admin login ‚Üí navigazione `/admin/costs` ‚Üí dashboard visible
2. KPI cards: costo stimato, cache hit rate, API calls renderizzati
3. Cost breakdown chart: dati visualizzati
4. Refresh button: re-fetch dati aggiornati
5. Rate limit: studente > 30 req/hour ‚Üí 429 error
6. Cache behavior: query ripetuta veloce (cache HIT)
7. Admin bypass: admin pu√≤ superare rate limits
8. Config reload: modifica YAML ‚Üí limiti aggiornati

**Duration Target**: < 45 secondi totali

---

## Success Metrics

- **Cost Reduction**: ‚â•25% riduzione costi mensili API entro 30 giorni deploy
- **Cache Hit Rate**: ‚â•30% cache hit rate su query studenti dopo 2 settimane
- **Performance**: cache lookup < 50ms (p95), no degradation RAG pipeline
- **Abuse Prevention**: zero incident abuse/DDoS post-deploy rate limiting
- **Admin Adoption**: ‚â•80% admin sessions visitano cost dashboard almeno 1 volta/settimana

---

## Implementation Notes

### Fase 1: Semantic Cache Core (Giorno 1-3)

**Prerequisiti**:
1. [ ] Creare migration Supabase `semantic_cache` table
2. [ ] Installare dipendenze: `poetry add pyyaml`
3. [ ] Configurare `config/cache_rate_limits.yaml`

**Implementation**:
1. Implementare `SemanticCache` class con lookup/store
2. Integrare in chat endpoint: cache check before LLM
3. Unit tests cache logic (10 test case)
4. Performance benchmark: cache lookup timing

**Acceptance**: Cache funzionante, unit tests PASS, lookup < 50ms

---

### Fase 2: Rate Limiting Enhancement (Giorno 4-6)

**Prerequisiti**:
1. [ ] Decidere storage rate limits: Redis vs Supabase table
2. [ ] Setup Redis deployment (se scelto)

**Implementation**:
1. Implementare `rate_limiter.py` middleware multi-level
2. Config YAML parsing e hot reload
3. Integration in chat endpoint con decorators
4. Response headers standard (X-RateLimit-*)
5. Unit tests rate limiter (8 test case)

**Acceptance**: Rate limiting funzionante, headers corretti, unit tests PASS

---

### Fase 3: Cost Monitoring (Giorno 7-10)

**Prerequisiti**:
1. [ ] Creare migration `api_cost_log` table
2. [ ] Definire pricing OpenAI constants

**Implementation**:
1. Implementare `CostTracker` class
2. Integrare logging in chat endpoint (embedding + LLM calls)
3. Endpoint `/api/v1/admin/costs/metrics`
4. Frontend `CostMonitoringPage.tsx`
5. Dashboard charts con Recharts
6. Unit tests cost tracker (8 test case)

**Acceptance**: Dashboard costi visibile, metriche accurate, unit tests PASS

---

### Fase 4: Integration & Testing (Giorno 11-14)

**Prerequisiti**:
1. [ ] Tutti moduli implementati
2. [ ] Dataset mock 100+ query per testing

**Implementation**:
1. Integration tests cache + rate limiting + cost tracking (5 test case)
2. E2E tests full flow (8 scenarios)
3. Performance testing: 100 concurrent users simulation
4. A/B testing similarity threshold (0.90 vs 0.95)
5. Regression tests: Story 3.2, 4.2 unchanged

**Acceptance**: E2E tests PASS, performance target raggiunto, zero regressioni

---

### Fase 5: Documentation & Deployment (Giorno 15-16)

**Implementation**:
1. Update `admin-setup-guide.md` sezione cost management
2. Create `config/cache_rate_limits.yaml` template
3. Deployment runbook: migration sequence, config setup
4. Admin training documentation: cost dashboard usage
5. Monitoring setup: alert logs, cache metrics dashboard

**Acceptance**: Documentazione completa, deployment guide pronta

---

## File Locations

- **Backend Cache**: `apps/api/api/cache/semantic_cache.py` (nuovo)
- **Backend Rate Limiter**: `apps/api/api/middleware/rate_limiter.py` (nuovo)
- **Backend Cost Tracker**: `apps/api/api/costs/tracker.py` (nuovo)
- **Backend Endpoint**: `apps/api/api/main.py` (modifiche integration)
- **Frontend Page**: `apps/web/src/pages/CostMonitoringPage.tsx` (nuovo)
- **Config**: `config/cache_rate_limits.yaml` (nuovo)
- **Migrations**: 
  - `supabase/migrations/20251006000000_create_semantic_cache.sql`
  - `supabase/migrations/20251006000001_create_api_cost_log.sql`
- **Tests Backend**: 
  - `apps/api/tests/test_semantic_cache.py`
  - `apps/api/tests/test_rate_limiter.py`
  - `apps/api/tests/test_cost_tracker.py`
  - `apps/api/tests/test_cache_integration.py`
- **Tests E2E**: `apps/web/tests/story-4.3.spec.ts`
- **Documentation**: `docs/admin-setup-guide.md` (aggiornamento)

---

## Tasks / Subtasks

### Pre-Implementation
- [ ] Decisione architetturale: Redis vs Supabase per rate limiting
- [ ] Spike: performance test pgvector similarity search su cache table (10K entries)
- [ ] Definire pricing constants OpenAI aggiornati

### Database Setup
- [ ] Migration `semantic_cache` table con pgvector index
- [ ] Migration `api_cost_log` table
- [ ] Test manual insert/select su dev environment
- [ ] Verificare performance similarity search < 50ms

### Backend Implementation
- [ ] Implementare `SemanticCache` class
- [ ] Implementare `rate_limiter.py` middleware
- [ ] Implementare `CostTracker` class
- [ ] Integrare cache check in chat endpoint
- [ ] Integrare rate limiting in chat endpoint
- [ ] Integrare cost logging in chat endpoint
- [ ] Endpoint `GET /api/v1/admin/costs/metrics`
- [ ] Endpoint `POST /api/v1/admin/cache/flush`
- [ ] Unit tests (26 test case totali)
- [ ] Integration tests (5 test case)

### Frontend Implementation
- [ ] Creare `CostMonitoringPage.tsx`
- [ ] Sezione KPI cards (4 card)
- [ ] Cost breakdown chart (Recharts)
- [ ] Table top 10 query costose
- [ ] Refresh button logic
- [ ] Loading/error states
- [ ] Route integration `App.tsx`
- [ ] Dashboard navigation card in `DashboardPage.tsx`

### Configuration
- [ ] Creare template `config/cache_rate_limits.yaml`
- [ ] Implementare YAML parser e validation
- [ ] Hot reload logic
- [ ] Default values fallback

### Testing & Validation
- [ ] Backend unit tests: 26/26 PASS
- [ ] Integration tests: 5/5 PASS
- [ ] E2E tests: 8/8 PASS
- [ ] Performance test: 100 concurrent users
- [ ] A/B test: similarity threshold tuning
- [ ] Regression tests: Story 3.2, 4.2 unchanged

### Documentation
- [ ] Update `admin-setup-guide.md` sezione cost management
- [ ] Config template documentation
- [ ] Deployment runbook
- [ ] Admin user guide: cost dashboard usage
- [ ] Troubleshooting guide: cache miss rate, rate limit tuning

---

## References

### Parent Epic
- Epic 4: `docs/prd/sezione-epic-4-dettagli-post-mvp-enhancements.md`

### Related Stories
- Story 3.2: `docs/stories/3.2.augmented-generation-endpoint.md` (chat endpoint integration point)
- Story 4.2: `docs/stories/4.2.analytics-dashboard.md` (dashboard UI pattern)

### Architecture
- Supabase pgvector: `docs/architecture/addendum-pgvector-langchain-supabase.md`
- Rate Limiting Pattern: `docs/architecture/sezione-10-sicurezza-e-performance.md`
- Testing Strategy: `docs/architecture/sezione-11-strategia-di-testing.md`

### Tech References
- **Rate Limiting Backend**: `docs/tech-reference/04-rate-limiting-backend.md` ‚Äî Guida completa slowapi: storage backends (Redis/Memory), decorator patterns, testing, best practices

### External Documentation
- OpenAI Pricing: https://openai.com/api/pricing/
- pgvector Similarity Search: https://github.com/pgvector/pgvector
- SlowAPI Rate Limiting: https://github.com/laurents/slowapi
- PyYAML: https://pyyaml.org/wiki/PyYAMLDocumentation
- Redis Rate Limiting: https://redis.io/docs/manual/patterns/rate-limiter/

---

## Change Log

| Date | Author | Change Description |
|------|--------|-------------------|
| 2025-10-05 | AI | Initial draft - Story 4.3 Caching e Rate Limiting Configurabili |

---

**Status**: üü° Draft ‚Äî Ready for Review  
**Priority**: High (Cost Optimization)  
**Dependencies**: Story 3.2 Done ‚úÖ, Story 4.2 Done ‚úÖ  
**Estimated Start Date**: TBD (post Story 4.4 completion)  
**Target Completion**: 12-16 ore (~2 sprints)

---

## Appendix: Decision Tree - Redis vs Supabase Rate Limiting

### Option A: Redis-based Rate Limiting

**Pros**:
- Performance ottimale: in-memory, < 1ms lookup
- Pattern consolidato: INCR + EXPIRE native Redis
- Scalabilit√†: supporta milioni req/sec

**Cons**:
- Deployment complexity: nuovo servizio da gestire
- Costo aggiuntivo: Redis Cloud/hosting
- Single point of failure: richiede HA setup

**Use Case**: Progetti con alta concorrenza (> 1000 concurrent users)

---

### Option B: Supabase Table-based Rate Limiting

**Pros**:
- Zero dipendenze aggiuntive: usa DB esistente
- Semplicit√† deployment: no nuovo servizio
- Costo zero: incluso in Supabase plan

**Cons**:
- Performance inferiore: ~10-20ms per lookup
- Scalabilit√† limitata: < 100 concurrent users
- Overhead DB: incremento load su Supabase

**Use Case**: Progetti MVP/small-scale (< 500 concurrent users)

---

### **Raccomandazione per FisioRAG**

**Option B: Supabase Table-based**

**Rationale**:
- Deployment target: singolo corso universitario (~50-100 studenti)
- Concurrent users peak: < 30 studenti (lezioni sincrone)
- Performance acceptable: 10-20ms overhead non critico
- Deployment simplicity: priorit√† MVP velocity

**Migration Path**: se crescita utenti > 500, migrare a Redis in Story 4.3.1 (enhancement)

**Implementation**: Tabella `rate_limit_counters` con TTL PostgreSQL

```sql
CREATE TABLE rate_limit_counters (
    key TEXT PRIMARY KEY,  -- "ip:{ip}" o "session:{session_id}"
    count INTEGER DEFAULT 0,
    window_start TIMESTAMPTZ NOT NULL,
    expires_at TIMESTAMPTZ NOT NULL
);

CREATE INDEX idx_rate_limit_expires ON rate_limit_counters(expires_at);
```

