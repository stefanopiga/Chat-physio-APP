# Story 4.3: Caching Semantico e Rate Limiting Configurabili

**Status:** Draft

## Metadata
- **ID**: 4.3
- **Type**: Feature / Post-MVP Enhancement / Cost Optimization
- **Epic**: Epic 4 — Post-MVP Enhancements
- **Priority**: High (Cost Management)
- **Complexity**: High
- **Effort Estimate**: 12-16 ore

---

## Story

**As a** Amministratore di Sistema / Professore (Admin),  
**I want** configurare caching semantico per query simili e rate limiting per utenti/IP,  
**so that** posso controllare i costi operativi LLM/embedding, prevenire abuse, e garantire performance ottimali del sistema RAG.

**Business Value**: Riduzione costi operativi mensili del 20-40% tramite cache hit su query ripetute/simili, protezione da abuse tramite throttling configurabile, e visibilità completa su consumo risorse API.

---

## Context & Background

### Current State (Post-MVP)
- Sistema RAG operativo con endpoint chat (`/api/v1/chat/sessions/{session_id}/messages`)
- Ogni query utente genera:
  - **Embedding API call** (OpenAI text-embedding-3-small)
  - **Similarity search** su Supabase vector store
  - **LLM API call** (OpenAI GPT-4 / GPT-3.5-turbo) per generation
- Rate limiting basico presente (Story 4.1: 10 req/hour admin debug)
- Nessun caching semantico → query duplicate/simili generano costi ripetuti
- Nessuna visibilità aggregata costi API

### Problem Statement
- **Costi operativi non ottimizzati**: query ripetute/simili (es. "cos'è la spalla congelata?" ripetuta da 10 studenti) generano costi evitabili
- **Rischio abuse**: studenti/bot potrebbero inviare query massive senza throttling
- **Scarsa visibilità costi**: admin non ha dashboard per monitorare consumo API tokens
- **Configurazione rigida**: limiti rate hardcoded, no flessibilità per adattare a pattern utilizzo reale

### Desired State
- **Caching semantico intelligente**: query con similarità embedding > threshold (es. 0.95) recuperano risposta da cache invece di invocare LLM
- **Rate limiting configurabile**: limiti per IP, session, ruolo (admin/student) con configurazione centralizzata
- **Dashboard costi**: visibilità real-time su embedding calls, LLM tokens consumati, cache hit rate
- **Alert automatici**: notifiche quando consumo supera soglie configurabili

---

## Acceptance Criteria

### Caching Semantico

1. **Cache Layer Implementation**: Implementare cache semantico con:
   - Storage: Redis (in-memory) o Supabase table `semantic_cache`
   - Key: embedding query (vector 1536 dimensioni)
   - Value: risposta LLM cached + metadata (timestamp, session_id_hash, hit_count)

2. **Cache Hit Logic**: Prima di invocare LLM:
   - Calcolare embedding query input
   - Eseguire similarity search su cache store
   - Se similarità > threshold configurabile (default 0.95) → restituire cached response
   - Altrimenti → invocare LLM e salvare in cache

3. **Cache Invalidation**: 
   - TTL configurabile (default: 7 giorni)
   - Manual flush via admin endpoint `/api/v1/admin/cache/flush`
   - Auto-eviction quando cache size > limit (LRU policy)

4. **Cache Metadata**: Response include header `X-Cache-Status: HIT|MISS` per trasparenza

5. **Performance**: Cache lookup < 50ms (p95)

### Rate Limiting

6. **Multi-Level Throttling**: Implementare rate limits configurabili per:
   - **IP**: limiti per indirizzo IP (protezione DDoS/abuse)
   - **Session**: limiti per session_id (protezione abuse studente singolo)
   - **Ruolo**: limiti differenziati admin/student (admin più permissivi)

7. **Rate Limit Configuration**: File `config/rate_limits.yaml`:
   ```yaml
   rate_limits:
     students:
       per_ip: 30/hour
       per_session: 20/hour
     admin:
       per_ip: 100/hour
       per_session: 50/hour
   ```

8. **Rate Limit Headers**: Response HTTP include headers standard:
   - `X-RateLimit-Limit`: limite massimo
   - `X-RateLimit-Remaining`: richieste rimanenti
   - `X-RateLimit-Reset`: timestamp reset contatore
   - `Retry-After`: secondi attesa se 429 Too Many Requests

9. **Rate Limit Bypass**: Admin può temporaneamente disabilitare limiti per session specifica via endpoint `/api/v1/admin/rate-limits/bypass`

### Cost Monitoring Dashboard

10. **Endpoint Metriche**: `GET /api/v1/admin/costs/metrics` restituisce:
    ```json
    {
      "period": "last_7_days",
      "embedding_api_calls": 1250,
      "llm_tokens_consumed": {
        "input": 45000,
        "output": 32000,
        "total": 77000
      },
      "cache_hit_rate": 0.35,
      "estimated_cost_usd": 12.45,
      "cost_breakdown": {
        "embedding": 2.50,
        "llm": 9.95
      }
    }
    ```

11. **Dashboard UI**: Sezione `/admin/costs` con:
    - KPI cards: total API calls, cache hit rate, estimated cost
    - Line chart: costi giornalieri ultimi 30 giorni
    - Breakdown chart: costi per tipo (embedding vs LLM)
    - Table top 10 query costose (ordinata per tokens consumati)

12. **Alert Configuration**: Admin può configurare soglie alert via UI:
    - Budget mensile (es. $50)
    - Notifica quando consumo > 80% budget
    - Alert via log strutturato (future: email/Slack integration)

### Configuration Management

13. **Centralized Config**: File `config/cache_rate_limits.yaml` con parametri:
    ```yaml
    semantic_cache:
      enabled: true
      similarity_threshold: 0.95
      ttl_days: 7
      max_entries: 10000
    
    rate_limits:
      students:
        per_ip: "30/hour"
        per_session: "20/hour"
      admin:
        per_ip: "100/hour"
        per_session: "50/hour"
    
    cost_monitoring:
      alert_threshold_usd: 50
      alert_enabled: true
    ```

14. **Hot Reload**: Modifiche config file applicate senza restart server (graceful reload)

15. **Admin Override UI**: Pagina `/admin/settings` permette modifica parametri via form (alternative a edit manuale YAML)

---

## Technical Implementation

### Architecture Overview

```
┌─────────────┐
│   Client    │
│  (Student)  │
└──────┬──────┘
       │ POST /api/v1/chat/sessions/{id}/messages
       ▼
┌─────────────────────────────────────────────┐
│         Rate Limiter Middleware             │
│  (Check IP + Session limits)                │
└──────┬──────────────────────────────────────┘
       │ PASS
       ▼
┌─────────────────────────────────────────────┐
│       Semantic Cache Lookup                 │
│  1. Generate query embedding                │
│  2. Search cache with similarity > 0.95     │
│  3. If HIT → return cached response         │
│  4. If MISS → proceed to RAG pipeline       │
└──────┬──────────────────────────────────────┘
       │ MISS
       ▼
┌─────────────────────────────────────────────┐
│       Standard RAG Pipeline                 │
│  1. Semantic search (vector store)          │
│  2. LLM generation (OpenAI)                 │
│  3. Store response in cache                 │
│  4. Log metrics (tokens, latency)           │
└──────┬──────────────────────────────────────┘
       │
       ▼
┌─────────────────────────────────────────────┐
│     Cost Tracking Module                    │
│  (Update counters: API calls, tokens)       │
└─────────────────────────────────────────────┘
```

---

### Backend Implementation

#### 1. Semantic Cache Module

**File**: `apps/api/api/cache/semantic_cache.py` (nuovo)

```python
from typing import Optional
import asyncpg
import numpy as np
from datetime import datetime, timedelta
from api.database import get_db_connection

class SemanticCache:
    """
    Cache semantico basato su similarità embedding.
    Storage: Supabase table semantic_cache con pgvector.
    """
    
    def __init__(
        self, 
        similarity_threshold: float = 0.95,
        ttl_days: int = 7
    ):
        self.similarity_threshold = similarity_threshold
        self.ttl_days = ttl_days
    
    async def lookup(
        self, 
        query_embedding: list[float],
        conn: asyncpg.Connection
    ) -> Optional[dict]:
        """
        Cerca in cache query con embedding simile.
        
        Returns:
            dict con cached_response e metadata se HIT, None se MISS
        """
        query = """
            SELECT 
                response_text,
                response_metadata,
                hit_count,
                created_at
            FROM semantic_cache
            WHERE 
                created_at > NOW() - INTERVAL '$1 days'
                AND (1 - (query_embedding <=> $2::vector)) > $3
            ORDER BY (1 - (query_embedding <=> $2::vector)) DESC
            LIMIT 1
        """
        
        row = await conn.fetchrow(
            query,
            self.ttl_days,
            query_embedding,
            self.similarity_threshold
        )
        
        if row:
            # Incrementa hit_count
            await self._increment_hit_count(row['cache_id'], conn)
            
            return {
                "response_text": row['response_text'],
                "metadata": row['response_metadata'],
                "cache_hit": True,
                "cached_at": row['created_at'].isoformat()
            }
        
        return None
    
    async def store(
        self,
        query_text: str,
        query_embedding: list[float],
        response_text: str,
        response_metadata: dict,
        conn: asyncpg.Connection
    ):
        """Salva response in cache."""
        query = """
            INSERT INTO semantic_cache (
                query_text,
                query_embedding,
                response_text,
                response_metadata,
                hit_count,
                created_at
            ) VALUES ($1, $2::vector, $3, $4, 0, NOW())
        """
        
        await conn.execute(
            query,
            query_text,
            query_embedding,
            response_text,
            response_metadata
        )
    
    async def _increment_hit_count(
        self, 
        cache_id: str, 
        conn: asyncpg.Connection
    ):
        """Incrementa contatore hit per cache entry."""
        await conn.execute(
            "UPDATE semantic_cache SET hit_count = hit_count + 1 WHERE id = $1",
            cache_id
        )
```

**Database Migration**: `supabase/migrations/20251006000000_create_semantic_cache.sql`

```sql
-- Semantic Cache Table
CREATE TABLE IF NOT EXISTS semantic_cache (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    query_text TEXT NOT NULL,
    query_embedding VECTOR(1536) NOT NULL,
    response_text TEXT NOT NULL,
    response_metadata JSONB,
    hit_count INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Index per similarity search ottimizzato
CREATE INDEX idx_semantic_cache_embedding ON semantic_cache 
USING ivfflat (query_embedding vector_cosine_ops)
WITH (lists = 100);

-- Index per TTL cleanup
CREATE INDEX idx_semantic_cache_created_at ON semantic_cache(created_at DESC);

-- RLS Policy: Solo service_role può scrivere/leggere
ALTER TABLE semantic_cache ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Service role full access"
    ON semantic_cache
    FOR ALL
    USING (auth.jwt() ->> 'role' = 'service_role');
```

---

#### 2. Rate Limiting Enhancement

**File**: `apps/api/api/middleware/rate_limiter.py` (nuovo)

```python
from typing import Annotated
from fastapi import Depends, HTTPException, status, Request
from slowapi import Limiter
from slowapi.util import get_remote_address
import yaml
import os

# Load config
with open("config/rate_limits.yaml") as f:
    RATE_LIMITS_CONFIG = yaml.safe_load(f)

limiter = Limiter(key_func=get_remote_address)

def get_rate_limit_for_user(role: str, limit_type: str) -> str:
    """
    Recupera rate limit da config per ruolo e tipo.
    
    Args:
        role: "admin" | "student"
        limit_type: "per_ip" | "per_session"
    
    Returns:
        Rate limit string (es. "30/hour")
    """
    return RATE_LIMITS_CONFIG['rate_limits'][role][limit_type]

async def check_rate_limit(
    request: Request,
    role: str = "student"
):
    """
    Dependency per verificare rate limit multi-level.
    
    Verifica:
    - Rate limit per IP
    - Rate limit per session_id (header X-Session-ID)
    """
    ip_limit = get_rate_limit_for_user(role, "per_ip")
    session_limit = get_rate_limit_for_user(role, "per_session")
    
    # Check IP limit (SlowAPI automatico)
    # check_rate_limit viene chiamato automaticamente da @limiter.limit()
    
    # Check session limit (custom logic)
    session_id = request.headers.get("X-Session-ID")
    if session_id:
        # Redis counter per session_id
        # TODO: implementare contatore Redis con expiry
        pass
    
    return True
```

**Integration in main.py**:

```python
from api.middleware.rate_limiter import check_rate_limit, limiter

@app.post("/api/v1/chat/sessions/{session_id}/messages")
@limiter.limit(lambda: get_rate_limit_for_user("student", "per_ip"))
async def create_message(
    session_id: str,
    request: ChatRequest,
    rate_check: Annotated[bool, Depends(check_rate_limit)]
):
    # Existing chat logic...
    pass
```

---

#### 3. Cost Monitoring Module

**File**: `apps/api/api/costs/tracker.py` (nuovo)

```python
from typing import Dict, Any
from datetime import datetime, timedelta
import asyncpg

class CostTracker:
    """
    Traccia costi API per embedding e LLM calls.
    """
    
    # Prezzi OpenAI (aggiornare con pricing reali)
    PRICING = {
        "embedding": 0.0001 / 1000,  # $0.0001 per 1K tokens
        "llm_input": 0.03 / 1000,     # GPT-4 Turbo input
        "llm_output": 0.06 / 1000     # GPT-4 Turbo output
    }
    
    async def log_api_call(
        self,
        call_type: str,  # "embedding" | "llm"
        tokens_consumed: int,
        session_id_hash: str,
        conn: asyncpg.Connection
    ):
        """Registra chiamata API con tokens consumati."""
        query = """
            INSERT INTO api_cost_log (
                call_type,
                tokens_consumed,
                estimated_cost_usd,
                session_id_hash,
                timestamp
            ) VALUES ($1, $2, $3, $4, NOW())
        """
        
        cost = self._calculate_cost(call_type, tokens_consumed)
        
        await conn.execute(
            query,
            call_type,
            tokens_consumed,
            cost,
            session_id_hash
        )
    
    def _calculate_cost(self, call_type: str, tokens: int) -> float:
        """Calcola costo stimato per chiamata API."""
        if call_type == "embedding":
            return tokens * self.PRICING["embedding"]
        elif call_type == "llm_input":
            return tokens * self.PRICING["llm_input"]
        elif call_type == "llm_output":
            return tokens * self.PRICING["llm_output"]
        return 0.0
    
    async def get_cost_metrics(
        self,
        period_days: int,
        conn: asyncpg.Connection
    ) -> Dict[str, Any]:
        """Recupera metriche costi per periodo specificato."""
        query = """
            SELECT 
                call_type,
                COUNT(*) as call_count,
                SUM(tokens_consumed) as total_tokens,
                SUM(estimated_cost_usd) as total_cost
            FROM api_cost_log
            WHERE timestamp > NOW() - INTERVAL '$1 days'
            GROUP BY call_type
        """
        
        rows = await conn.fetch(query, period_days)
        
        # Calcola cache hit rate
        cache_hits = await self._get_cache_hits(period_days, conn)
        total_queries = await self._get_total_queries(period_days, conn)
        cache_hit_rate = cache_hits / total_queries if total_queries > 0 else 0.0
        
        return {
            "period": f"last_{period_days}_days",
            "embedding_api_calls": next((r['call_count'] for r in rows if r['call_type'] == 'embedding'), 0),
            "llm_tokens_consumed": {
                "input": next((r['total_tokens'] for r in rows if r['call_type'] == 'llm_input'), 0),
                "output": next((r['total_tokens'] for r in rows if r['call_type'] == 'llm_output'), 0),
            },
            "cache_hit_rate": cache_hit_rate,
            "estimated_cost_usd": sum(r['total_cost'] for r in rows),
            "cost_breakdown": {
                call['call_type']: call['total_cost']
                for call in rows
            }
        }
```

**Database Migration**: `supabase/migrations/20251006000001_create_api_cost_log.sql`

```sql
-- API Cost Log Table
CREATE TABLE IF NOT EXISTS api_cost_log (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    call_type TEXT NOT NULL CHECK (call_type IN ('embedding', 'llm_input', 'llm_output')),
    tokens_consumed INTEGER NOT NULL,
    estimated_cost_usd NUMERIC(10, 6) NOT NULL,
    session_id_hash TEXT,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Indexes per query aggregazioni
CREATE INDEX idx_api_cost_log_timestamp ON api_cost_log(timestamp DESC);
CREATE INDEX idx_api_cost_log_call_type ON api_cost_log(call_type);

-- RLS Policy: Admin read-only
ALTER TABLE api_cost_log ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Admin can read cost logs"
    ON api_cost_log
    FOR SELECT
    USING (auth.jwt() ->> 'role' = 'admin');
```

---

### Frontend Implementation

#### Cost Monitoring Dashboard

**File**: `apps/web/src/pages/CostMonitoringPage.tsx` (nuovo)

```tsx
import { useEffect, useState } from "react";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { Badge } from "@/components/ui/badge";
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from "recharts";
import { authService } from "@/services/authService";

interface CostMetrics {
  period: string;
  embedding_api_calls: number;
  llm_tokens_consumed: {
    input: number;
    output: number;
  };
  cache_hit_rate: number;
  estimated_cost_usd: number;
  cost_breakdown: Record<string, number>;
}

export default function CostMonitoringPage() {
  const [metrics, setMetrics] = useState<CostMetrics | null>(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    fetchMetrics();
  }, []);

  const fetchMetrics = async () => {
    try {
      setLoading(true);
      const { data } = await authService.getSession();
      
      const res = await fetch("/api/v1/admin/costs/metrics?period_days=7", {
        headers: {
          Authorization: `Bearer ${data.session?.access_token}`,
        },
      });
      
      const data_metrics = await res.json();
      setMetrics(data_metrics);
    } catch (err) {
      console.error(err);
    } finally {
      setLoading(false);
    }
  };

  if (loading) return <div>Caricamento...</div>;
  if (!metrics) return <div>Nessun dato</div>;

  return (
    <div className="mx-auto max-w-7xl space-y-6 p-4">
      <h1 className="text-3xl font-bold">Monitoraggio Costi API</h1>
      
      {/* KPI Cards */}
      <div className="grid gap-4 md:grid-cols-4">
        <Card>
          <CardHeader>
            <CardTitle className="text-sm">Costo Stimato</CardTitle>
          </CardHeader>
          <CardContent>
            <p className="text-2xl font-bold">${metrics.estimated_cost_usd.toFixed(2)}</p>
            <p className="text-xs text-muted-foreground">{metrics.period}</p>
          </CardContent>
        </Card>
        
        <Card>
          <CardHeader>
            <CardTitle className="text-sm">Cache Hit Rate</CardTitle>
          </CardHeader>
          <CardContent>
            <p className="text-2xl font-bold">{(metrics.cache_hit_rate * 100).toFixed(1)}%</p>
            <Badge variant={metrics.cache_hit_rate > 0.3 ? "default" : "secondary"}>
              {metrics.cache_hit_rate > 0.3 ? "Ottimo" : "Basso"}
            </Badge>
          </CardContent>
        </Card>
        
        <Card>
          <CardHeader>
            <CardTitle className="text-sm">Embedding Calls</CardTitle>
          </CardHeader>
          <CardContent>
            <p className="text-2xl font-bold">{metrics.embedding_api_calls}</p>
          </CardContent>
        </Card>
        
        <Card>
          <CardHeader>
            <CardTitle className="text-sm">LLM Tokens</CardTitle>
          </CardHeader>
          <CardContent>
            <p className="text-2xl font-bold">
              {(metrics.llm_tokens_consumed.input + metrics.llm_tokens_consumed.output).toLocaleString()}
            </p>
            <p className="text-xs text-muted-foreground">
              Input: {metrics.llm_tokens_consumed.input.toLocaleString()} | 
              Output: {metrics.llm_tokens_consumed.output.toLocaleString()}
            </p>
          </CardContent>
        </Card>
      </div>
      
      {/* Cost Breakdown Chart */}
      <Card>
        <CardHeader>
          <CardTitle>Breakdown Costi</CardTitle>
        </CardHeader>
        <CardContent>
          <ResponsiveContainer width="100%" height={300}>
            <LineChart data={/* TODO: daily cost data */}>
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis dataKey="date" />
              <YAxis />
              <Tooltip />
              <Legend />
              <Line type="monotone" dataKey="embedding_cost" stroke="#8884d8" name="Embedding" />
              <Line type="monotone" dataKey="llm_cost" stroke="#82ca9d" name="LLM" />
            </LineChart>
          </ResponsiveContainer>
        </CardContent>
      </Card>
    </div>
  );
}
```

**Route Integration**: `apps/web/src/App.tsx`

```tsx
import CostMonitoringPage from "./pages/CostMonitoringPage";

// Aggiungere route protetta:
<Route
  path="/admin/costs"
  element={
    <AdminGuard>
      <CostMonitoringPage />
    </AdminGuard>
  }
/>
```

---

## Dependencies

**Prerequisiti**:
- ✅ Story 3.2 (Augmented Generation Endpoint) - Done (chat endpoint esistente)
- ✅ Story 4.2 (Analytics Dashboard) - Done (pattern dashboard UI)
- ✅ Supabase pgvector extension abilitata
- ✅ Redis deployment (opzionale, alternative: Supabase-based rate limiting)

**Dipendenze Esterne**:
- ✅ OpenAI Python SDK (già installato)
- ✅ PyYAML per config parsing: `poetry add pyyaml`
- ⚠️ **Redis Python client** (opzionale): `poetry add redis` se si sceglie Redis per rate limiting
- ✅ Recharts (già installato per Story 4.2)

---

## Out of Scope

- **Email/Slack notifications**: alert solo via log strutturato (future Story 4.3.1)
- **Multi-tenant cost tracking**: costi aggregati globali, no per-course breakdown (future enhancement)
- **Advanced cache strategies**: LRU eviction basico, no ML-based cache warming (future Epic 5)
- **Real-time cost dashboard**: refresh manuale, no WebSocket streaming (future enhancement)
- **Custom pricing models**: hard-coded OpenAI pricing, no support altri provider (future enhancement)

---

## Risks

| ID | Description | Probability | Impact | Mitigation |
|----|-------------|-------------|--------|------------|
| R-4.3-1 | Redis dependency aumenta complessità deployment | Media | Medio | Alternative: rate limiting basato su Supabase table con TTL; Redis opzionale |
| R-4.3-2 | Semantic cache miss rate alto (cache inefficace) | Media | Alto | Tuning similarity threshold; A/B testing 0.90 vs 0.95; analytics cache performance |
| R-4.3-3 | Cache storage growth unbounded | Bassa | Medio | TTL 7 giorni + LRU eviction; monitoring size cache table |
| R-4.3-4 | Cost tracking overhead performance | Bassa | Medio | Async logging; batch insert; minimizzare DB writes |
| R-4.3-5 | Config hot reload failure → service disruption | Bassa | Alto | Validation schema YAML; graceful fallback su config corrente se reload fail |
| R-4.3-6 | False positive rate limiting → UX degradata | Media | Alto | Generous default limits; bypass mechanism per admin; monitoring 429 rate |

---

## Testing Strategy

### Unit Tests (Backend)

**File**: `apps/api/tests/test_semantic_cache.py`

**Test Cases**: 10 test
1. Cache lookup: HIT con similarità 0.96
2. Cache lookup: MISS con similarità 0.85
3. Cache store: insert corretto
4. Cache hit_count: increment su lookup
5. TTL: cache entry > 7 giorni ignorata
6. Similarity threshold: config 0.90 vs 0.95
7. LRU eviction: max_entries limit
8. Concurrent access: thread-safe operations
9. Embedding vector validation
10. Performance: lookup < 50ms

---

**File**: `apps/api/tests/test_rate_limiter.py`

**Test Cases**: 8 test
1. Rate limit IP: block dopo 30 req/hour
2. Rate limit session: block dopo 20 req/hour
3. Rate limit admin: 100 req/hour permessi
4. Response headers: X-RateLimit-* corretti
5. Retry-After header: presente su 429
6. Config reload: limiti aggiornati senza restart
7. Bypass mechanism: admin override funziona
8. Multi-level: IP + session limiti applicati contemporaneamente

---

**File**: `apps/api/tests/test_cost_tracker.py`

**Test Cases**: 8 test
1. Log API call: insert embedding call
2. Log API call: insert LLM call con input/output tokens
3. Cost calculation: pricing corretto
4. Metrics aggregation: period_days=7 dati corretti
5. Cache hit rate: calcolo accurato
6. Cost breakdown: embedding vs LLM separati
7. Alert threshold: trigger quando costo > limite
8. Performance: metrics query < 200ms

---

### Integration Tests

**File**: `apps/api/tests/test_cache_integration.py`

**Test Cases**: 5 test
1. End-to-end cache HIT: query ripetuta usa cache
2. End-to-end cache MISS: query nuova bypassa cache
3. Cost savings: cache HIT riduce LLM calls
4. Performance: cache lookup + RAG pipeline timing
5. Concurrency: 10 query simultanee gestite correttamente

---

### E2E Tests

**File**: `apps/web/tests/story-4.3.spec.ts`

**Scenarios**: 8 test
1. Admin login → navigazione `/admin/costs` → dashboard visible
2. KPI cards: costo stimato, cache hit rate, API calls renderizzati
3. Cost breakdown chart: dati visualizzati
4. Refresh button: re-fetch dati aggiornati
5. Rate limit: studente > 30 req/hour → 429 error
6. Cache behavior: query ripetuta veloce (cache HIT)
7. Admin bypass: admin può superare rate limits
8. Config reload: modifica YAML → limiti aggiornati

**Duration Target**: < 45 secondi totali

---

## Success Metrics

- **Cost Reduction**: ≥25% riduzione costi mensili API entro 30 giorni deploy
- **Cache Hit Rate**: ≥30% cache hit rate su query studenti dopo 2 settimane
- **Performance**: cache lookup < 50ms (p95), no degradation RAG pipeline
- **Abuse Prevention**: zero incident abuse/DDoS post-deploy rate limiting
- **Admin Adoption**: ≥80% admin sessions visitano cost dashboard almeno 1 volta/settimana

---

## Implementation Notes

### Fase 1: Semantic Cache Core (Giorno 1-3)

**Prerequisiti**:
1. [ ] Creare migration Supabase `semantic_cache` table
2. [ ] Installare dipendenze: `poetry add pyyaml`
3. [ ] Configurare `config/cache_rate_limits.yaml`

**Implementation**:
1. Implementare `SemanticCache` class con lookup/store
2. Integrare in chat endpoint: cache check before LLM
3. Unit tests cache logic (10 test case)
4. Performance benchmark: cache lookup timing

**Acceptance**: Cache funzionante, unit tests PASS, lookup < 50ms

---

### Fase 2: Rate Limiting Enhancement (Giorno 4-6)

**Prerequisiti**:
1. [ ] Decidere storage rate limits: Redis vs Supabase table
2. [ ] Setup Redis deployment (se scelto)

**Implementation**:
1. Implementare `rate_limiter.py` middleware multi-level
2. Config YAML parsing e hot reload
3. Integration in chat endpoint con decorators
4. Response headers standard (X-RateLimit-*)
5. Unit tests rate limiter (8 test case)

**Acceptance**: Rate limiting funzionante, headers corretti, unit tests PASS

---

### Fase 3: Cost Monitoring (Giorno 7-10)

**Prerequisiti**:
1. [ ] Creare migration `api_cost_log` table
2. [ ] Definire pricing OpenAI constants

**Implementation**:
1. Implementare `CostTracker` class
2. Integrare logging in chat endpoint (embedding + LLM calls)
3. Endpoint `/api/v1/admin/costs/metrics`
4. Frontend `CostMonitoringPage.tsx`
5. Dashboard charts con Recharts
6. Unit tests cost tracker (8 test case)

**Acceptance**: Dashboard costi visibile, metriche accurate, unit tests PASS

---

### Fase 4: Integration & Testing (Giorno 11-14)

**Prerequisiti**:
1. [ ] Tutti moduli implementati
2. [ ] Dataset mock 100+ query per testing

**Implementation**:
1. Integration tests cache + rate limiting + cost tracking (5 test case)
2. E2E tests full flow (8 scenarios)
3. Performance testing: 100 concurrent users simulation
4. A/B testing similarity threshold (0.90 vs 0.95)
5. Regression tests: Story 3.2, 4.2 unchanged

**Acceptance**: E2E tests PASS, performance target raggiunto, zero regressioni

---

### Fase 5: Documentation & Deployment (Giorno 15-16)

**Implementation**:
1. Update `admin-setup-guide.md` sezione cost management
2. Create `config/cache_rate_limits.yaml` template
3. Deployment runbook: migration sequence, config setup
4. Admin training documentation: cost dashboard usage
5. Monitoring setup: alert logs, cache metrics dashboard

**Acceptance**: Documentazione completa, deployment guide pronta

---

## File Locations

- **Backend Cache**: `apps/api/api/cache/semantic_cache.py` (nuovo)
- **Backend Rate Limiter**: `apps/api/api/middleware/rate_limiter.py` (nuovo)
- **Backend Cost Tracker**: `apps/api/api/costs/tracker.py` (nuovo)
- **Backend Endpoint**: `apps/api/api/main.py` (modifiche integration)
- **Frontend Page**: `apps/web/src/pages/CostMonitoringPage.tsx` (nuovo)
- **Config**: `config/cache_rate_limits.yaml` (nuovo)
- **Migrations**: 
  - `supabase/migrations/20251006000000_create_semantic_cache.sql`
  - `supabase/migrations/20251006000001_create_api_cost_log.sql`
- **Tests Backend**: 
  - `apps/api/tests/test_semantic_cache.py`
  - `apps/api/tests/test_rate_limiter.py`
  - `apps/api/tests/test_cost_tracker.py`
  - `apps/api/tests/test_cache_integration.py`
- **Tests E2E**: `apps/web/tests/story-4.3.spec.ts`
- **Documentation**: `docs/admin-setup-guide.md` (aggiornamento)

---

## Tasks / Subtasks

### Pre-Implementation
- [ ] Decisione architetturale: Redis vs Supabase per rate limiting
- [ ] Spike: performance test pgvector similarity search su cache table (10K entries)
- [ ] Definire pricing constants OpenAI aggiornati

### Database Setup
- [ ] Migration `semantic_cache` table con pgvector index
- [ ] Migration `api_cost_log` table
- [ ] Test manual insert/select su dev environment
- [ ] Verificare performance similarity search < 50ms

### Backend Implementation
- [ ] Implementare `SemanticCache` class
- [ ] Implementare `rate_limiter.py` middleware
- [ ] Implementare `CostTracker` class
- [ ] Integrare cache check in chat endpoint
- [ ] Integrare rate limiting in chat endpoint
- [ ] Integrare cost logging in chat endpoint
- [ ] Endpoint `GET /api/v1/admin/costs/metrics`
- [ ] Endpoint `POST /api/v1/admin/cache/flush`
- [ ] Unit tests (26 test case totali)
- [ ] Integration tests (5 test case)

### Frontend Implementation
- [ ] Creare `CostMonitoringPage.tsx`
- [ ] Sezione KPI cards (4 card)
- [ ] Cost breakdown chart (Recharts)
- [ ] Table top 10 query costose
- [ ] Refresh button logic
- [ ] Loading/error states
- [ ] Route integration `App.tsx`
- [ ] Dashboard navigation card in `DashboardPage.tsx`

### Configuration
- [ ] Creare template `config/cache_rate_limits.yaml`
- [ ] Implementare YAML parser e validation
- [ ] Hot reload logic
- [ ] Default values fallback

### Testing & Validation
- [ ] Backend unit tests: 26/26 PASS
- [ ] Integration tests: 5/5 PASS
- [ ] E2E tests: 8/8 PASS
- [ ] Performance test: 100 concurrent users
- [ ] A/B test: similarity threshold tuning
- [ ] Regression tests: Story 3.2, 4.2 unchanged

### Documentation
- [ ] Update `admin-setup-guide.md` sezione cost management
- [ ] Config template documentation
- [ ] Deployment runbook
- [ ] Admin user guide: cost dashboard usage
- [ ] Troubleshooting guide: cache miss rate, rate limit tuning

---

## References

### Parent Epic
- Epic 4: `docs/prd/sezione-epic-4-dettagli-post-mvp-enhancements.md`

### Related Stories
- Story 3.2: `docs/stories/3.2.augmented-generation-endpoint.md` (chat endpoint integration point)
- Story 4.2: `docs/stories/4.2.analytics-dashboard.md` (dashboard UI pattern)

### Architecture
- Supabase pgvector: `docs/architecture/addendum-pgvector-langchain-supabase.md`
- Rate Limiting Pattern: `docs/architecture/sezione-10-sicurezza-e-performance.md`
- Testing Strategy: `docs/architecture/sezione-11-strategia-di-testing.md`

### External Documentation
- OpenAI Pricing: https://openai.com/api/pricing/
- pgvector Similarity Search: https://github.com/pgvector/pgvector
- SlowAPI Rate Limiting: https://github.com/laurents/slowapi
- PyYAML: https://pyyaml.org/wiki/PyYAMLDocumentation
- Redis Rate Limiting: https://redis.io/docs/manual/patterns/rate-limiter/

---

## Change Log

| Date | Author | Change Description |
|------|--------|-------------------|
| 2025-10-05 | AI | Initial draft - Story 4.3 Caching e Rate Limiting Configurabili |

---

**Status**: 🟡 Draft — Ready for Review  
**Priority**: High (Cost Optimization)  
**Dependencies**: Story 3.2 Done ✅, Story 4.2 Done ✅  
**Estimated Start Date**: TBD (post Story 4.4 completion)  
**Target Completion**: 12-16 ore (~2 sprints)

---

## Appendix: Decision Tree - Redis vs Supabase Rate Limiting

### Option A: Redis-based Rate Limiting

**Pros**:
- Performance ottimale: in-memory, < 1ms lookup
- Pattern consolidato: INCR + EXPIRE native Redis
- Scalabilità: supporta milioni req/sec

**Cons**:
- Deployment complexity: nuovo servizio da gestire
- Costo aggiuntivo: Redis Cloud/hosting
- Single point of failure: richiede HA setup

**Use Case**: Progetti con alta concorrenza (> 1000 concurrent users)

---

### Option B: Supabase Table-based Rate Limiting

**Pros**:
- Zero dipendenze aggiuntive: usa DB esistente
- Semplicità deployment: no nuovo servizio
- Costo zero: incluso in Supabase plan

**Cons**:
- Performance inferiore: ~10-20ms per lookup
- Scalabilità limitata: < 100 concurrent users
- Overhead DB: incremento load su Supabase

**Use Case**: Progetti MVP/small-scale (< 500 concurrent users)

---

### **Raccomandazione per FisioRAG**

**Option B: Supabase Table-based**

**Rationale**:
- Deployment target: singolo corso universitario (~50-100 studenti)
- Concurrent users peak: < 30 studenti (lezioni sincrone)
- Performance acceptable: 10-20ms overhead non critico
- Deployment simplicity: priorità MVP velocity

**Migration Path**: se crescita utenti > 500, migrare a Redis in Story 4.3.1 (enhancement)

**Implementation**: Tabella `rate_limit_counters` con TTL PostgreSQL

```sql
CREATE TABLE rate_limit_counters (
    key TEXT PRIMARY KEY,  -- "ip:{ip}" o "session:{session_id}"
    count INTEGER DEFAULT 0,
    window_start TIMESTAMPTZ NOT NULL,
    expires_at TIMESTAMPTZ NOT NULL
);

CREATE INDEX idx_rate_limit_expires ON rate_limit_counters(expires_at);
```

