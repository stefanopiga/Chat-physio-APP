# Story 7.1: Academic Conversational RAG Foundation

**Status:** Done  
**Priority:** P0 (Must-Have for MVP)  
**Effort Estimate:** 11-12 ore  
**Epic:** Epic 7 — Enhanced RAG Experience

---

## Story

**As a** Studente di fisioterapia,  
**I want** interagire con un tutor medico-accademico che mantiene memoria della conversazione e fornisce risposte strutturate propedeuticamente,  
**so that** ottenga un vantaggio reale nello studio rispetto alla consultazione di materiale statico, con possibilità di approfondimenti progressivi e follow-up contestualizzati.

[Fonte: `docs/stories/INTEGRATION-GUIDE-enhanced-rag-story-7.1.md`, `docs/architecture/addendum-rag-enhancement-analysis.md`]

---

## Acceptance Criteria

### AC1: Prompt Medico-Accademico Propedeutico
Implementare system prompt che definisce persona "medico fisioterapista con esperienza accademica" con guida esplicita per struttura risposta propedeutica (introduzione → concetti chiave → sviluppo → note cliniche → limitazioni contesto).  
[Fonte: `docs/architecture/addendum-academic-medical-prompting.md`]

### AC2: Risposte Strutturate con Pydantic
Output LLM deve seguire schema `EnhancedAcademicResponse` con campi validati:
- `introduzione` (20-500 char)
- `concetti_chiave` (2-5 items)
- `spiegazione_dettagliata` (>100 char)
- `note_cliniche` (optional)
- `limitazioni_contesto` (optional)
- `citazioni` (array `CitationMetadata` con document_name, page_number, relevance_score, excerpt)
- `confidenza_risposta` ("alta" | "media" | "bassa")

[Fonte: `docs/architecture/addendum-structured-academic-responses.md`]

### AC3: Memoria Conversazionale (Context Window)
Sistema mantiene ultimi 3 turni conversazionali (6 messaggi: user + assistant) per session, con:
- Token counting per budget management (max 2000 token)
- Truncation automatica se eccede limite
- Formatting per injection in prompt LLM
- Persistence in `chat_messages_store`

[Fonte: `docs/architecture/addendum-conversational-memory-patterns.md`]

### AC4: Follow-up Contestualizzati
Studente può fare domande di seguito che referenziano conversazione precedente:
- "Approfondisci il punto 2" → LLM usa cronologia per identificare contesto
- "Come si collega a quanto detto prima?" → LLM mantiene coerenza
- Query ambigue disambiguate tramite history (es. "Quali sono i gradi?" dopo discussione su spondilolistesi)

[Fonte: `docs/architecture/addendum-conversational-memory-patterns.md` §Use Cases]

### AC5: Citazioni Arricchite
Ogni citazione include metadata completi per source verification:
- `chunk_id`, `document_id`, `document_name`
- `page_number` (se disponibile)
- `relevance_score` (0.0-1.0)
- `excerpt` (max 200 char per preview)
- `content_type` (theory | clinical_example | guideline | definition | procedure)

Frontend può renderizzare popover informativi per ogni fonte.  
[Fonte: `docs/architecture/addendum-structured-academic-responses.md` §CitationMetadata]

### AC6: Feature Flags per Rollout Graduale
Sistema configurabile via environment variables per enable/disable singolarmente:
- `ENABLE_ENHANCED_RESPONSE_MODEL` (default: false)
- `ENABLE_CONVERSATIONAL_MEMORY` (default: false)
- `ENABLE_ACADEMIC_PROMPT` (default: false)

Permette A/B testing e rollout incrementale.  
[Fonte: `docs/stories/INTEGRATION-GUIDE-enhanced-rag-story-7.1.md` §Feature Flags]

### AC7: Metriche & Monitoring
Tracciare metriche specifiche per validazione enhancement:
- `conversation_turns_per_session` (avg)
- `context_window_tokens` (per session)
- `structured_response_parsing_success_rate`
- `follow_up_query_rate` (% sessioni con 2+ messaggi)
- Logging eventi: `conversation_turn_added`, `context_window_loaded`, `enhanced_response_generated`

[Fonte: `docs/architecture/addendum-conversational-memory-patterns.md` §Testing, `docs/qa/methodologies/rag-ab-testing-framework.md`]

---

## Dev Notes

### Previous Story Insights
- Story 3.1 (Semantic Search): Endpoint `POST /api/v1/chat/query` fornisce chunk retrieved baseline. [Fonte: `docs/stories/3.1.semantic-search-endpoint.md`]
- Story 3.2 (Augmented Generation): Endpoint `POST /api/v1/chat/sessions/{sessionId}/messages` implementa RAG chain ma con prompt generico e nessuna memoria conversazionale. [Fonte: `docs/stories/3.2.augmented-generation-endpoint.md`]
- `chat_messages_store` esiste ma NON utilizzato nel prompt generation (sistema attualmente stateless). [Fonte: `apps/api/api/stores.py`, `docs/architecture/addendum-rag-enhancement-analysis.md` §4]

### Current State Analysis
**Prompt attuale** (`apps/api/api/routers/chat.py:271-280`):
```python
"Sei un assistente che risponde SOLO usando il CONTEXT fornito..."
```
Gap: Ruolo generico, nessuna guida struttura pedagogica, tono non autorevole.

**Output attuale**:
```python
class AnswerWithCitations(BaseModel):
    risposta: str  # Blob testuale
    citazioni: List[str]  # Solo ID chunk
```
Gap: Nessuna struttura semantica, citazioni povere.

**Memoria**: Nessuna (ogni query elaborata indipendentemente).

[Fonte: `docs/architecture/addendum-rag-enhancement-analysis.md` §1-4]

### Component Specifications

#### Conversational Memory Service
- `ConversationManager` class: gestisce context window per session
- `ConversationMessage` model: singolo messaggio con role, content, timestamp, chunk_ids
- `ChatContextWindow` model: container per session con max 6 messages, total_tokens tracking
- Token counting via `tiktoken` (encoding for gpt-3.5-turbo)

[Fonte: `docs/architecture/addendum-conversational-memory-patterns.md`]

#### Enhanced Response Models
- `EnhancedAcademicResponse`: 7 campi strutturati con validazioni Pydantic
- `CitationMetadata`: metadata completi per citazioni
- `ResponseMetadata`: metadata tecnici (response_id, generation_time_ms, model_name)

[Fonte: `docs/architecture/addendum-structured-academic-responses.md`]

#### Prompt Engineering
- `ACADEMIC_MEDICAL_SYSTEM_PROMPT`: template con persona medico-accademico
- Conversation history injection: `{conversation_history}` variable in prompt
- Mode-specific instructions: focused (default) vs exploratory

[Fonte: `docs/architecture/addendum-academic-medical-prompting.md`]

### Data Models

**Existing** (utilizzati):
- `ChatMessage`: `id`, `session_id`, `role`, `content`, `source_chunk_ids`, `metadata`, `created_at` [Fonte: `docs/architecture/sezione-4-modelli-di-dati.md`]
- `AnswerWithCitations`: modello baseline da deprecare [Fonte: `apps/api/api/models/answer_with_citations.py`]

**New** (creati in questa story):
- `EnhancedAcademicResponse`: risposta strutturata con 7 campi
- `CitationMetadata`: citazioni arricchite
- `ConversationMessage`: messaggio conversazionale con timestamp
- `ChatContextWindow`: context window per session
- `ResponseMetadata`: metadata tecnici risposta

[Fonte: `docs/architecture/addendum-structured-academic-responses.md`, `docs/architecture/addendum-conversational-memory-patterns.md`]

### API Specifications

**Endpoint modificato**:
- `POST /api/v1/chat/sessions/{sessionId}/messages`
  - Request: `ChatMessageCreateRequest` (unchanged)
  - Response: `ChatMessageCreateResponse` con enhanced structure
  - Behavior: Include conversation history in prompt generation

**Nuovo endpoint** (opzionale debug):
- `GET /api/v1/chat/sessions/{sessionId}/history`
  - Response: Lista `ConversationMessage` per session
  - Auth: JWT required
  - Use case: Frontend debugging, admin monitoring

[Fonte: `docs/architecture/addendum-conversational-memory-patterns.md` §Best Practices]

### File Locations
- Monorepo root: `/`
- Backend: `/apps/api/`
- Docs: `/docs/`
- New modules:
  - `apps/api/api/models/enhanced_response.py`
  - `apps/api/api/models/conversation.py`
  - `apps/api/api/services/conversation_service.py`
  - `apps/api/api/prompts/academic_medical.py`

[Fonte: `docs/stories/INTEGRATION-GUIDE-enhanced-rag-story-7.1.md`]

### Testing Requirements

#### Unit Tests
- `test_enhanced_response_validation.py`: Pydantic validation rules
- `test_conversation_manager.py`: Context window, token counting, truncation
- `test_citation_metadata_validation.py`: Citation enrichment

#### Integration Tests
- `test_conversational_followup.py`: Multi-turn conversation flow (3 turni)
- `test_enhanced_prompt_generation.py`: Prompt formatting con history
- `test_structured_output_parsing.py`: LLM → Pydantic parsing

#### A/B Test Setup
- Framework implementation per confronto baseline vs enhanced
- Sample size calculation (target 200 sessions per gruppo)
- Metrics collection infrastructure

[Fonte: `docs/architecture/addendum-conversational-memory-patterns.md` §Testing, `docs/qa/methodologies/rag-ab-testing-framework.md`]

### Technical Constraints

**Token Budget** (gpt-5-nano limit: 4096 input):
- System prompt: ~800 token
- Conversation history: ~1500 token (ultimi 3 turni, compacted)
- Retrieved chunks: ~1500 token (6 chunk @ 250 token avg)
- User question: ~200 token
- Format instructions: ~100 token
- **Total**: ~4100 token → tight, requires aggressive history truncation

**Mitigation**: Compact message format (max 150 char/message in history), preserve only last 2-3 turns.

[Fonte: `docs/architecture/addendum-conversational-memory-patterns.md` §Token Budget Management]

**Dependencies**:
- `tiktoken = "^0.5.1"` (already present in poetry.lock, make explicit in dependencies)
- No new heavy dependencies (sentence-transformers deferred to Story 7.2)

[Fonte: `docs/stories/INTEGRATION-GUIDE-enhanced-rag-story-7.1.md`]

### Variabili d'Ambiente

**Existing**:
- `OPENAI_API_KEY`: LLM provider
- `OPENAI_MODEL`: default `gpt-5-nano`
- `OPENAI_TEMPERATURE_CHAT`: temperature override

**New (Story 7.1)**:
```bash
# Feature flags
ENABLE_ENHANCED_RESPONSE_MODEL=false
ENABLE_CONVERSATIONAL_MEMORY=false
ENABLE_ACADEMIC_PROMPT=false

# Conversation config
CONVERSATION_MAX_TURNS=3
CONVERSATION_MAX_TOKENS=2000
CONVERSATION_MESSAGE_COMPACT_LENGTH=150
```

[Fonte: `docs/stories/INTEGRATION-GUIDE-enhanced-rag-story-7.1.md` §Configuration]

### Edge Cases / Errori

#### Conversational Memory
- Session non trovata → return empty context window (first interaction)
- Token overflow → truncate oldest messages, ensure ≥ 1 turno mantenuto
- Parsing error conversation history → fallback "Nessuna cronologia disponibile"

#### Structured Output
- LLM non rispetta schema JSON → fallback `StrOutputParser`, wrap in minimal structure
- Pydantic validation error → log error, retry con prompt refinement o return fallback
- Missing optional fields (`note_cliniche`, `limitazioni_contesto`) → null, non empty string

#### Token Counting
- `tiktoken` import fail → fallback approssimazione (1 token ~= 3 char italiano)
- Token count > limit ma no messaggi da rimuovere → truncate current message content

[Fonte: `docs/architecture/addendum-conversational-memory-patterns.md` §Best Practices, `docs/architecture/addendum-structured-academic-responses.md` §Best Practices]

### Performance Targets

**Latency** (Story 7.1 scope, no retrieval optimization):
- Generation time (LLM): ~800-1200ms (baseline, no change expected)
- Context window overhead: +50-100ms (loading + formatting)
- Structured parsing overhead: +50ms (Pydantic validation)
- **Target p95**: < 2500ms (generation + overhead, no retrieval)

**Memory**:
- Conversation store (in-memory): ~1KB per session × 100 concurrent sessions = ~100KB
- Token encoder (tiktoken): ~5MB RAM

[Fonte: `docs/architecture/addendum-rag-enhancement-analysis.md` §Success Metrics]

### Glossario/Riferimenti

- **Context Window**: Sliding window di N turni conversazionali più recenti mantenuti in memoria
- **Turno conversazionale**: Coppia (user_message, assistant_response)
- **Token budget**: Limite token input LLM (4096 per gpt-5-nano)
- **Propedeutico**: Approccio didattico che introduce concetti in sequenza logica base → avanzato
- **Persona**: Ruolo/carattere definito per LLM (es. "medico fisioterapista accademico")

### Pattern Standard

#### Pydantic Structured Output
```python
from langchain_core.output_parsers import PydanticOutputParser

parser = PydanticOutputParser(pydantic_object=EnhancedAcademicResponse)
format_instructions = parser.get_format_instructions()
```

#### Conversational Memory Management
```python
conv_manager = get_conversation_manager()
context_window = conv_manager.get_context_window(session_id)
history_formatted = conv_manager.format_for_prompt(context_window)

# Post-generation
conv_manager.add_turn(session_id, user_msg, assistant_msg, chunk_ids)
```

#### Feature Flag Pattern
```python
if settings.enable_conversational_memory:
    # Enhanced with memory
else:
    # Baseline stateless
```

[Fonte: `docs/architecture/addendum-conversational-memory-patterns.md`, `docs/architecture/addendum-structured-academic-responses.md`]

### Riferimenti Tecnici Dettagliati

**Academic Prompting**:
- System prompt template: `docs/architecture/addendum-academic-medical-prompting.md` §Enhanced Prompt Template
- Persona definition: §Target Persona
- Examples before/after: §Examples
- Common pitfalls: §Common Pitfalls & Solutions

**Structured Responses**:
- Pydantic models full spec: `docs/architecture/addendum-structured-academic-responses.md` §Enhanced Models
- Frontend integration: §Frontend Integration Examples
- Validation rules: §Testing
- Migration strategy: §Migration Strategy

**Conversational Memory**:
- Architecture diagram: `docs/architecture/addendum-conversational-memory-patterns.md` §Architecture
- Implementation: §ConversationManager Service
- Token budget breakdown: §Token Budget Management
- Use cases: §Use Cases

**A/B Testing**:
- Methodology: `docs/qa/methodologies/rag-ab-testing-framework.md`
- Sample size calculation: §Sample Size Calculation
- Statistical testing: §Statistical Testing
- Decision framework: §Decision Framework

---

## Tasks / Subtasks

### Task 1: Academic Medical Prompt Implementation (2-3h)
- [ ] Creare `apps/api/api/prompts/academic_medical.py` con `ACADEMIC_MEDICAL_SYSTEM_PROMPT` template [AC1]
  - [ ] Definire persona: "medico fisioterapista con esperienza accademica e clinica"
  - [ ] Guida struttura: introduzione → concetti chiave → spiegazione → note cliniche → limitazioni
  - [ ] Conversation history injection: `{conversation_history}` placeholder
- [ ] Aggiornare `apps/api/api/routers/chat.py` per usare nuovo prompt [AC1]
  - [ ] Import prompt template
  - [ ] Conditional usage basato su feature flag `ENABLE_ACADEMIC_PROMPT`
- [ ] A/B test manuale: confrontare 10 risposte baseline vs nuovo prompt
  - [ ] Valutare tono, struttura, autorevolezza

**Files**:
- `apps/api/api/prompts/academic_medical.py` (new)
- `apps/api/api/routers/chat.py` (modified)

### Task 2: Enhanced Pydantic Models (2h)
- [ ] Creare `apps/api/api/models/enhanced_response.py` [AC2, AC5]
  - [ ] `EnhancedAcademicResponse` con 7 campi validati
  - [ ] `CitationMetadata` con metadata completi (document_name, page_number, excerpt, etc.)
  - [ ] `ResponseMetadata` per analytics
  - [ ] Validators Pydantic per constraints (min/max lengths, ranges)
- [ ] Aggiornare parser in `chat.py` [AC2]
  - [ ] `PydanticOutputParser(pydantic_object=EnhancedAcademicResponse)`
  - [ ] Inject `format_instructions` in system prompt
  - [ ] Fallback handling: se parsing fallisce, wrap in minimal structure
- [ ] Unit tests: `apps/api/tests/models/test_enhanced_response.py`
  - [ ] Test validation rules (es. spiegazione_dettagliata > 100 char)
  - [ ] Test optional fields (null vs empty string)

**Files**:
- `apps/api/api/models/enhanced_response.py` (new)
- `apps/api/api/routers/chat.py` (parser update)
- `apps/api/tests/models/test_enhanced_response.py` (new)

### Task 3: Conversational Memory Service (4h)
- [ ] Ensure `tiktoken` esplicito in `pyproject.toml` dependencies [AC3]
  ```toml
  tiktoken = "^0.5.1"
  ```
- [ ] Creare `apps/api/api/models/conversation.py` [AC3]
  - [ ] `ConversationMessage` model (role, content, timestamp, chunk_ids)
  - [ ] `ChatContextWindow` model (session_id, messages, total_tokens)
- [ ] Creare `apps/api/api/services/conversation_service.py` [AC3, AC4]
  - [ ] `ConversationManager` class
  - [ ] `get_context_window(session_id)`: retrieve last 3 turns
  - [ ] `add_turn(session_id, user_msg, assistant_msg, chunk_ids)`: persist
  - [ ] `format_for_prompt(context_window)`: format per injection
  - [ ] `_count_tokens(messages)`: token counting con tiktoken
  - [ ] `_truncate_to_budget(messages)`: aggressive truncation se > 2000 token
- [ ] Integrare in `apps/api/api/routers/chat.py` [AC3, AC4]
  - [ ] Load context window all'inizio request
  - [ ] Format history e inject in prompt
  - [ ] Save turn al completamento generation
  - [ ] Conditional basato su `ENABLE_CONVERSATIONAL_MEMORY` flag
- [ ] Unit tests: `apps/api/tests/services/test_conversation_service.py`
  - [ ] Test sliding window (max 6 messages)
  - [ ] Test token counting
  - [ ] Test truncation logica
  - [ ] Test format_for_prompt output
- [ ] Integration test: `apps/api/tests/test_conversational_followup.py`
  - [ ] Simulare 3 turni conversazionali
  - [ ] Verificare follow-up "approfondisci punto 2" usa cronologia

**Files**:
- `apps/api/pyproject.toml` (tiktoken explicit)
- `apps/api/api/models/conversation.py` (new)
- `apps/api/api/services/conversation_service.py` (new)
- `apps/api/api/routers/chat.py` (integration)
- `apps/api/tests/services/test_conversation_service.py` (new)
- `apps/api/tests/test_conversational_followup.py` (new)

### Task 4: Configuration & Feature Flags (1h)
- [ ] Aggiornare `apps/api/api/config.py` [AC6]
  - [ ] `enable_enhanced_response_model: bool` (default False)
  - [ ] `enable_conversational_memory: bool` (default False)
  - [ ] `enable_academic_prompt: bool` (default False)
  - [ ] `conversation_max_turns: int` (default 3)
  - [ ] `conversation_max_tokens: int` (default 2000)
- [ ] Documentare env vars in `apps/api/ENV_TEST_TEMPLATE.txt`
- [ ] Update `.env` locale per development

**Files**:
- `apps/api/api/config.py` (modified)
- `apps/api/ENV_TEST_TEMPLATE.txt` (modified)

### Task 5: Monitoring & Analytics (1h)
- [ ] Aggiungere logging eventi conversazionali [AC7]
  - [ ] `conversation_turn_added`: session_id, turn_number, user_msg_length, assistant_msg_length
  - [ ] `context_window_loaded`: session_id, messages_count, total_tokens
  - [ ] `enhanced_response_generated`: has_clinical_notes, has_limitations, concepts_count, citations_count
- [ ] Metriche aggregate tracking [AC7]
  - [ ] `conversation_turns_per_session` (gauge)
  - [ ] `context_window_tokens` (histogram)
  - [ ] `structured_parsing_success_rate` (gauge)
  - [ ] `follow_up_query_rate` (gauge)
- [ ] Dashboard query (Grafana/internal): metriche key per A/B test

**Files**:
- `apps/api/api/routers/chat.py` (logging)
- `apps/api/api/services/conversation_service.py` (logging)
- `apps/api/api/analytics/` (metrics, se directory esiste)

### Task 6: Testing & Validation (2h)
- [ ] Coverage target: ≥80% per nuovo codice
- [ ] Unit tests completati (Task 2, 3)
- [ ] Integration test multi-turn conversation
- [ ] Manual QA: test 10 conversazioni multi-turno
  - [ ] Verificare follow-up contestualizzati funzionano
  - [ ] Verificare risposta strutturata rendering
  - [ ] Verificare citazioni arricchite
- [ ] Setup A/B test infrastructure [AC7]
  - [ ] Implementare `apps/api/api/services/ab_testing.py` (variant assignment)
  - [ ] User-level randomization (deterministic hash)
  - [ ] Logging variant per ogni request
- [ ] Preparare A/B test plan: baseline vs treatment (7.1 enhancements)

**Files**:
- All test files from Task 2, 3
- `apps/api/api/services/ab_testing.py` (new)
- `docs/qa/ab-test-plan-story-7.1.md` (new, test plan)

### Task 7: Documentation (1h)
- [ ] Update `docs/architecture/sezione-4-modelli-di-dati.md`
  - [ ] Aggiungere `EnhancedAcademicResponse` model
  - [ ] Aggiungere `CitationMetadata` model
  - [ ] Aggiungere `ConversationMessage` e `ChatContextWindow`
- [ ] Creare user guide: `docs/user-guides/chat-study-guide.md`
  - [ ] Come fare domande mirate
  - [ ] Come fare follow-up contestualizzati
  - [ ] Esempi di conversazioni efficaci
  - [ ] Best practices per studiare con chat
- [ ] Update INTEGRATION-GUIDE con completion status Task 1-7

**Files**:
- `docs/architecture/sezione-4-modelli-di-dati.md` (modified)
- `docs/user-guides/chat-study-guide.md` (new)
- `docs/stories/INTEGRATION-GUIDE-enhanced-rag-story-7.1.md` (status update)

---

## Dev Agent Record

### Agent Model Used
[TBD post-implementation]

### Completion Notes
[TBD post-implementation]

### File List
**New files**:
- `apps/api/api/prompts/academic_medical.py`
- `apps/api/api/models/enhanced_response.py`
- `apps/api/api/models/conversation.py`
- `apps/api/api/services/conversation_service.py`
- `apps/api/api/services/ab_testing.py`
- `apps/api/tests/models/test_enhanced_response.py`
- `apps/api/tests/services/test_conversation_service.py`
- `apps/api/tests/test_conversational_followup.py`
- `docs/user-guides/chat-study-guide.md`
- `docs/qa/ab-test-plan-story-7.1.md`

**Modified files**:
- `apps/api/pyproject.toml` (tiktoken explicit dependency)
- `apps/api/api/config.py` (feature flags)
- `apps/api/api/routers/chat.py` (prompt, models, memory integration)
- `apps/api/ENV_TEST_TEMPLATE.txt` (env vars documentation)
- `docs/architecture/sezione-4-modelli-di-dati.md` (new models)

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-22 | 0.1 | Story 7.1 draft created | SM |
| [TBD] | 1.0 | Implementation completed | Dev |

---

## QA Results

**Status**: [TBD post-implementation]

**Gate Decision**: [TBD]

**Tracciabilità AC**:
- AC1 (Prompt): [TBD]
- AC2 (Structured responses): [TBD]
- AC3 (Memory): [TBD]
- AC4 (Follow-up): [TBD]
- AC5 (Citations): [TBD]
- AC6 (Feature flags): [TBD]
- AC7 (Metrics): [TBD]

**NFR Validation**:
- Performance: Target p95 < 2500ms (generation + overhead) [TBD]
- Memory: In-memory store ~100KB per 100 sessions [TBD]
- Reliability: Fallback strategies tested [TBD]
- Maintainability: Coverage ≥80% [TBD]

**A/B Test Results** (post-implementation):
- User satisfaction: [TBD]
- Messages per session: [TBD]
- Follow-up rate: [TBD]
- Session duration: [TBD]

---

## References

### Documentation Addenda (Story 7.1 Specific)
- `docs/architecture/addendum-academic-medical-prompting.md` — Prompt engineering guide
- `docs/architecture/addendum-structured-academic-responses.md` — Pydantic models specification
- `docs/architecture/addendum-conversational-memory-patterns.md` — Memory management implementation
- `docs/qa/methodologies/rag-ab-testing-framework.md` — A/B testing methodology

### Integration Guide
- `docs/stories/INTEGRATION-GUIDE-enhanced-rag-story-7.1.md` — Master guide per Story 7.1 + 7.2

### Analysis
- `docs/architecture/addendum-rag-enhancement-analysis.md` — Gap analysis sistema attuale vs target

### Related Stories
- Story 3.1: Semantic Search Endpoint (baseline retrieval)
- Story 3.2: Augmented Generation Endpoint (baseline RAG)
- Story 7.2: Advanced Retrieval Optimization (follow-up, re-ranking)

---

**Story Owner**: [TBD]  
**Reviewers**: Tech Lead, Backend Team, Product Owner  
**Approved**: [TBD]  
**Sprint Target**: [TBD]

