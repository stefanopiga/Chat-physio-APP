# Story 2.6: RAG System Production Readiness Validation

**Status:** Approved

**Last Updated:** 2025-10-11 (Post Story 5.* Refactoring)

## Story

**As a** Technical Lead / System Administrator,
**I want** validazione production readiness del sistema RAG post-refactoring (Story 5.*), con focus su environment configuration, infrastructure resilience, database integrity e performance benchmarks,
**so that** sistema sia deployable in produzione con garanzie su affidabilit√†, performance e sicurezza.

## Context

**Stato Attuale (Post Story 5.5 - Ottobre 2025):**
- ‚úÖ Architettura refactorizzata modulare (Story 5.2: main.py 2086‚Üí112 righe)
- ‚úÖ Test suite 100% pass rate (Story 5.5: 179/179 test attivi, 0 failed)
- ‚úÖ Pipeline RAG implementata e testata (Story 2.5: Quality Gate PASS 90/100)
- ‚úÖ Coverage 93% su apps/api
- ‚úÖ Known issues risolti (FK constraints, rate limiting, embedding NULL)

**Gap Production Readiness:**
Sistema tecnicamente validato ma richiede audit finale pre-deployment production:
1. Environment configuration completa e sicura (secrets management, .env validation)
2. Infrastructure resilience (Docker health, Celery stability, Redis performance)
3. Database integrity audit (indici pgvector, orphan records, query performance)
4. Performance benchmarks (latency p95, throughput pipeline)
5. Security posture (API authentication, rate limiting production-ready)

**Stack Tecnologico Validato (Story 5.*):**
- ‚úÖ **LangChain**: embeddings, vector stores, chain orchestration (test suite 93%)
- ‚úÖ **Celery**: async task processing, worker isolation (Story 5.4)
- ‚úÖ **Redis**: message broker operativo, rate limiting storage
- ‚úÖ **Docker**: container orchestration, network fisio-rag-net
- ‚úÖ **Pydantic**: data validation models (schemas/ refactorizzati Story 5.2)
- ‚úÖ **FastAPI**: API endpoints modulari (routers/ Story 5.2)
- ‚úÖ **Supabase/pgvector**: vector storage, similarity search (test coverage 93%)
- ‚úÖ **OpenAI**: embedding API (gpt-5-nano, text-embedding-3-small)

**Gap da Validare (Production):**
- Environment variables production vs development
- Secrets rotation policy
- Database backup strategy
- Monitoring & alerting setup
- Performance under load (10+ concurrent ingestion)

## Acceptance Criteria

### AC1: Environment Production Audit ‚úÖ MANTENUTO
**Given** sistema con .env development e docker-compose.yml locale
**When** audit environment production viene eseguito
**Then** documento contiene:
- Tutte variabili richieste per production (SUPABASE_*, OPENAI_*, CELERY_*)
- Secrets management strategy (rotazione, vault, encryption at rest)
- Differenze development vs staging vs production
- Missing variables con impact assessment
- Hardcoded values risk report

### AC2: Infrastructure Health Matrix ‚úÖ MANTENUTO
**Given** stack Docker completo (api, celery-worker, redis, supabase)
**When** health check matrix viene eseguita
**Then** report contiene per ogni servizio:
- Container status (Up/Restarting/Exited)
- Network connectivity inter-service (api‚Üíredis, celery‚Üíredis)
- Volume mounts integrity
- Logs health (ultimi 100 righe, error count)
- Resource usage (CPU%, Memory%)
- Restart policy configurato

### AC3: Database Integrity Audit ‚úÖ MANTENUTO
**Given** database Supabase con documenti e chunks indicizzati
**When** integrity audit viene eseguito
**Then** report SQL contiene:
- Orphan documents (status completed senza chunks)
- Orphan chunks (embedding NULL)
- Index pgvector performance (EXPLAIN ANALYZE)
- Foreign key constraint violations
- Table statistics (row counts, index usage)

### AC4: Performance Benchmarks üÜï AGGIUNTO
**Given** pipeline RAG operativa
**When** performance benchmarks vengono eseguiti
**Then** report contiene:
- **Ingestion**: latency per documento (p50, p95, p99) su 10 documenti test
- **Embedding**: latency OpenAI API (p50, p95) su 100 chunks
- **Search**: latency semantic search (p95 <1s) su 30 query
- **Chat**: latency augmented generation (p95 <2s) su 20 query
- **Throughput**: documenti/ora pipeline completa
- Bottleneck identification

### AC5: Security Posture Review üÜï AGGIUNTO
**Given** API esposta con autenticazione JWT
**When** security review viene eseguito
**Then** report contiene:
- JWT secret strength (entropy bits, rotation policy)
- Rate limiting production-ready (config per environment)
- API authentication coverage (endpoint protetti vs pubblici)
- CORS policy production-safe
- Secret exposure risk (git history scan, logs sanitization)
- OpenAI API key scope (project vs user key, spending limits)

### AC6: Deployment Checklist Production üÜï AGGIUNTO
**Given** sistema production-ready
**When** deployment checklist viene completato
**Then** documento contiene:
- Pre-deployment: backup DB, secrets rotation, container build
- Deployment: docker-compose up procedure, health check validation
- Post-deployment: smoke tests, monitoring setup, rollback plan
- Rollback triggers: failed health checks, error rate threshold
- Contact matrix: on-call rotation, escalation paths

### ~~AC7-10: Test Coverage & Pipeline Validation~~ ‚ùå RIMOSSI
**Rationale:** Gi√† validati in Story 5.* (test suite 100%, pipeline Story 2.5).
- Story 5.5: test suite 179/179 PASSED, 0 FAILED
- Story 2.5: pipeline RAG testata, Quality Gate PASS 90/100
- Coverage 93% su apps/api

## Tasks / Subtasks

**NOTA AMBIENTE DI SVILUPPO:** Tutti i comandi terminal in questa storia sono per **PowerShell** (Windows 10). Sistema operativo: win32 10.0.19045. Workspace: `C:\Users\user\Desktop\Claude-Code\fisio-rag-master\APPLICAZIONE`.

### Pre-Analisi: Setup Environment Investigation

- [ ] **Task 1: Environment Configuration Audit** (AC: 1)
  - [ ] **Subtask 1.1: Extraction Variabili da Codebase**
    - [ ] Estrarre tutte variabili `os.getenv()` da codebase Python:
      ```powershell
      cd apps/api
      grep -r "os.getenv" api/ --include="*.py" | Select-String -Pattern 'os\.getenv\(["\']([^"\']+)["\']' -AllMatches
      ```
    - [ ] Estrarre variabili `settings.py` o `config.py` (se esistenti)
    - [ ] Creare lista completa variabili richieste per runtime
  
  - [ ] **Subtask 1.2: Audit File Configurazione Multi-Ambiente**
    - [ ] **Root Project Environment** (`.env` root):
      - Template reference: `ENV_TEMPLATE.txt`
     - Verificare presenza: `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`, `SUPABASE_JWT_SECRET`
      - Verificare presenza: `DATABASE_URL` (formato: `postgresql://postgres.<project>:<pwd>@aws-1-eu-central-2.pooler.supabase.com:6543/postgres`)
     - Verificare presenza: `OPENAI_API_KEY`
      - Verificare presenza: `CELERY_ENABLED`, `CELERY_BROKER_URL` (redis://localhost:6379/0 o redis://redis:6379/0 in Docker)
      - Verificare presenza: `ADMIN_EMAIL`, `TOKEN_SUPABASE_CHAT_FISIO`
      - Note: File reale `.env` non visionabile da agent, template mostra struttura corretta
    
    - [ ] **API Test Environment** (`apps/api/.env.test.local`):
      - Template reference: `apps/api/ENV_TEST_TEMPLATE.txt`
      - Verificare differenze critiche vs root:
        - `CELERY_ENABLED=false` (sync execution test)
        - `TESTING=true`
        - `RATE_LIMITING_ENABLED=false`
        - `CLOCK_SKEW_LEEWAY_SECONDS=120`
      - Verificare rate limit config: `EXCHANGE_CODE_RATE_LIMIT_*`, `REFRESH_TOKEN_RATE_LIMIT_*`, `ADMIN_CREATE_TOKEN_RATE_LIMIT_*`
      - **CRITICAL**: Verificare `DATABASE_URL` punta a TEST database, NON production
      - Note: Variabile `AG_LATENCY_MAX_SAMPLES=200` (analytics) presente in template
    
    - [ ] **Web Frontend Environment** (`apps/web/.env`):
      - Template reference: `apps/web/ENV_WEB_TEMPLATE.txt`
      - Verificare presenza: `VITE_SUPABASE_URL`, `VITE_SUPABASE_ANON_KEY`, `VITE_API_BASE_URL`
      - Note: Solo public keys (ANON_KEY), nessun secret backend
  
  - [ ] **Subtask 1.3: Docker Compose Environment Mapping**
    - [ ] Verificare `docker-compose.yml` environment sections:
      - Service `api`: mapping da `.env` root
      - Service `celery-worker`: mapping da `.env` root
      - Service `redis`: config senza custom env (usa default)
    - [ ] Verificare network environment: `DATABASE_URL` usa hostname corretto (localhost vs postgres service name)
  
  - [ ] **Subtask 1.4: Gap Analysis & Security Audit**
    - [ ] Confrontare variabili estratte (1.1) vs template disponibili (1.2)
    - [ ] Identificare variabili missing o con placeholder `<your-*>` values
    - [ ] Verificare secrets strength:
      - `SUPABASE_JWT_SECRET`: min 32 chars, alta entropia
      - `OPENAI_API_KEY`: formato `sk-...`
      - `DATABASE_URL`: password non in plaintext logs
    - [ ] Documentare variabili hardcoded nel codice (security risk)
    - [ ] Creare matrice: Variabile | Required By | Template Location | Production Status | Risk Level
  
  - [ ] **Output Task 1:** Report `docs/reports/rag-environment-audit.md` con:
    - Tabella variabili complete (nome, scopo, file reference, status)
    - Gap list prioritizzata (P0: blockers, P1: production risk, P2: nice-to-have)
    - Secrets management recommendations
    - Diff development vs test vs production config

- [ ] **Task 2: Docker Infrastructure Validation** (AC: 2)
  - [ ] Eseguire `docker compose ps` ‚Üí verificare stato servizi (Up vs Restarting vs Exit)
  - [ ] Per ogni container, eseguire health check:
    - API: `curl http://localhost:8000/health`
    - Celery worker: `docker logs fisio-rag-celery-worker-1 | grep "ready"`
    - Redis: `docker exec -it fisio-rag-redis-1 redis-cli PING`
  - [ ] Verificare network Docker: `docker network inspect fisio-rag-master_default` (o nome effettivo)
  - [ ] Verificare volumes: `docker volume ls` e check mount points in logs
  - [ ] Analizzare logs ultimi 100 righe per ogni service: `docker logs <container> --tail=100`
  - [ ] Documentare errori/warning ricorrenti nei logs

### Fase 1: Celery & Async Processing Validation

- [ ] **Task 3: Celery Worker Status Validation** (AC: 3)
  - [ ] Verificare `celery_app.py` configurazione:
    - broker_url corretto (redis://redis:6379/0)
    - result_backend configurato
    - Task retry logic presente
  - [ ] Avviare Celery worker: `docker compose up celery-worker` (se non running)
  - [ ] Monitorare logs worker: attendere messaggio "celery@hostname ready"
  - [ ] Test task simple: creare endpoint `/api/v1/admin/test-celery` che lancia test_task.delay()
  - [ ] Verificare task execution con: `docker logs <celery-worker> | grep task`
  - [ ] Verificare result retrieval: controllare Redis con `redis-cli KEYS *`

- [ ] **Task 4: Celery-Redis Integration Test** (AC: 3)
  - [ ] Verificare connettivit√† API ‚Üí Redis: tentativo connessione da container API
  - [ ] Verificare connettivit√† Celery Worker ‚Üí Redis: idem da worker
  - [ ] Test task enqueue + result fetch:
    - POST sync job ‚Üí ottieni job_id
    - Query Redis: `redis-cli GET "celery-task-meta-<task_id>"`
    - Verificare task result in Redis dopo completion
  - [ ] Documentare latency enqueue ‚Üí processing start

### Fase 2: Performance Benchmarks (NUOVO)

**Prerequisito Fase 2**: Pipeline operativa, dataset test preparato

- [ ] **Task 4-bis: Prepare Test Dataset** (AC: 4)
  - [ ] Preparare 10 documenti test rappresentativi:
    - 3 documenti testo accademico denso (~500 words)
    - 3 paper scientifici con tabelle
    - 2 documenti tabellari (protocolli)
    - 2 documenti misti (anatomia + evidence)
  - [ ] Documentare caratteristiche dataset:
    - Dimensione media (bytes, chars)
    - Complessit√† (tables count, images count)
    - Expected chunks count per documento

- [ ] **Task 5: Ingestion Latency Benchmark** (AC: 4)
  - [ ] Eseguire ingestion 10 documenti test sequenziale:
    ```powershell
    cd apps/api
    $JWT = python ../../scripts/admin/generate_jwt.py --email admin@fisiorag.local --expires-days 1
    
    # Per ogni documento, misurare latency
    foreach ($doc in (Get-ChildItem "../../dataset-test/*.docx")) {
        $start = Get-Date
        curl -X POST http://localhost/api/v1/admin/knowledge-base/sync-jobs `
          -H "Authorization: Bearer $JWT" `
          -H "Content-Type: application/json" `
          -d "@payload_$($doc.Name).json"
        $latency = (Get-Date) - $start
        Write-Output "$($doc.Name): $($latency.TotalSeconds)s"
    }
    ```
  - [ ] Calcolare metriche:
    - p50 latency (mediana)
    - p95 latency (95 percentile)
    - p99 latency (99 percentile)
    - Max latency
    - Throughput (docs/hour)
  - [ ] Identificare bottleneck:
    - Extraction time % (da timing_metrics response)
    - Classification time %
    - Chunking time %
    - Embedding time %
    - Indexing time %
  - [ ] Documentare in report: `docs/reports/rag-performance-ingestion.md`

- [ ] **Task 5-bis: ~~Document Extraction Validation~~** ‚ùå RIMOSSO
  - **Rationale:** Gi√† validato in Story 2.5, test coverage 93%
  - [ ] Preparare documento test: `test_doc.docx` con contenuto noto (es. 3 paragrafi, 1 tabella)
  - [ ] Aggiungere logging in `DocumentExtractor.extract()`:
    - Prima di extraction: log file path, file type detected
    - Dopo extraction: log text length, images count, tables count
  - [ ] Eseguire extraction manualmente (Python REPL o script):
    ```python
    from pathlib import Path
    from api.knowledge_base.extractors import DocumentExtractor
    extractor = DocumentExtractor()
    result = extractor.extract(Path("/path/to/test_doc.docx"))
    print(result)
    ```
  - [ ] Verificare output structure matches expected: text, images[], tables[], metadata
  - [ ] Ripetere per PDF e TXT
  - [ ] Documentare discrepanze vs documentazione Story 2.5

- [ ] **Task 6: Classification Validation** (AC: 4)
  - [ ] Aggiungere logging in `classify_content_enhanced()`:
    - Prima LLM call: log prompt length, sample text (primi 200 chars)
    - Dopo LLM call: log domain, structure_type, confidence, reasoning
  - [ ] Eseguire classification manualmente con documento test:
    ```python
    from api.knowledge_base.classifier import classify_content_enhanced
    result = classify_content_enhanced(test_text)
    print(result)
    ```
  - [ ] Verificare output: `ContentDomain`, `DocumentStructureCategory`, confidence >= 0.7
  - [ ] Test con 3 tipologie documento: fisioterapia_clinica, anatomia, evidence_based
  - [ ] Documentare accuracy vs atteso (confidenza, domain corretto)

- [ ] **Task 7: Chunking Validation** (AC: 4)
  - [ ] Aggiungere logging in `ChunkRouter.route()`:
    - Input: classification type, content length
    - Output: strategy selected, chunks count, avg chunk size
  - [ ] Eseguire chunking manualmente:
    ```python
    from api.ingestion.chunk_router import ChunkRouter
    router = ChunkRouter()
    result = router.route(content=test_text, classification=classification_result)
    print(f"Strategy: {result.strategy_name}, Chunks: {len(result.chunks)}")
    ```
  - [ ] Verificare chunks non vuoti, dimensione ragionevole (500-2000 chars)
  - [ ] Confrontare strategia applicata vs attesa per classification type

- [ ] **Task 7-bis: End-to-End Preprocessing LLM Pipeline** (AC: 4)
  - [ ] **Obiettivo**: Verificare flusso completo Extraction ‚Üí Classification LLM ‚Üí Intelligent Chunking
  - [ ] Preparare 3 documenti test rappresentativi:
    - Doc A: Testo accademico denso (anatomia, ~500 words, no tabelle)
    - Doc B: Paper scientifico (RCT, con tabelle dati, references)
    - Doc C: Documento tabellare (protocolli esercizi con tabelle)
  - [ ] Per ogni documento, eseguire pipeline completa (Poetry REPL):
    ```python
    from pathlib import Path
    from api.knowledge_base.extractors import DocumentExtractor
    from api.knowledge_base.classifier import classify_content_enhanced
    from api.ingestion.chunk_router import ChunkRouter
    from api.ingestion.models import ClassificazioneOutput
    
    # Step 1: Extraction
    extractor = DocumentExtractor()
    extraction = extractor.extract(Path("../../conoscenza/fisioterapia/lombare/test-doc-A.docx"))
    text = extraction["text"]
    metadata = extraction["metadata"]
    print(f"Extracted: {len(text)} chars, images={metadata.get('images_count', 0)}, tables={metadata.get('tables_count', 0)}")
    
    # Step 2: Classification LLM (gpt-5-nano)
    classification = classify_content_enhanced(text, metadata)
    print(f"Domain: {classification.domain.value}")
    print(f"Structure: {classification.structure_type.value}")
    print(f"Confidence: {classification.confidence}")
    
    # Step 3: Intelligent Chunking
    router = ChunkRouter()
    classification_for_chunking = ClassificazioneOutput(
        classificazione=classification.structure_type,
        motivazione=classification.reasoning,
        confidenza=classification.confidence
    )
    chunks_result = router.route(text, classification_for_chunking)
    print(f"Strategy: {chunks_result.strategy_name}, Chunks: {len(chunks_result.chunks)}")
    ```
  - [ ] Verificare per **Doc A (Testo Accademico)**:
    - Domain: fisioterapia_clinica / anatomia / patologia (appropriato)
    - Structure: TESTO_ACCADEMICO_DENSO
    - Confidence >= 0.85
    - Strategy: recursive (no fallback)
    - Chunks count ragionevole (3-10 per ~500 words)
  - [ ] Verificare per **Doc B (Paper Scientifico)**:
    - Domain: evidence_based
    - Structure: PAPER_SCIENTIFICO_MISTO
    - Confidence >= 0.85
    - Strategy: tabular
    - Chunks preservano struttura tabellare
  - [ ] Verificare per **Doc C (Documento Tabellare)**:
    - Structure: DOCUMENTO_TABELLARE
    - Confidence >= 0.85
    - Strategy: tabular
  - [ ] Verificare mapping routing:
    - TESTO_ACCADEMICO_DENSO ‚Üí RecursiveCharacterStrategy
    - PAPER_SCIENTIFICO_MISTO ‚Üí TabularStructuralStrategy
    - DOCUMENTO_TABELLARE ‚Üí TabularStructuralStrategy
    - Fallback (confidence < 0.85) ‚Üí fallback::recursive
  - [ ] Verificare LLM performance:
    - Response time classification < 5s per documento
    - No RateLimitError
    - OPENAI_API_KEY valida (gpt-5-nano model)
  - [ ] Documentare risultati per ogni documento: domain, structure, confidence, strategy, chunks_count
  - [ ] Documentare discrepanze: fallback inatteso, confidence bassa, routing errato

- [ ] **Task 8: Document Persistence Validation** (AC: 4, 5)
  - [ ] Aggiungere logging in `save_document_to_db()`:
    - Document ID generato
    - Metadata salvato
    - Timestamp creazione
  - [ ] Dopo save, query database:
    ```sql
    SELECT id, file_name, status, chunking_strategy, created_at, metadata
    FROM documents
    WHERE id = '<document_id>';
    ```
  - [ ] Verificare record presente, status "processing", metadata JSON corretto
  - [ ] Verificare foreign key constraints attive

- [ ] **Task 9: Embedding Generation Validation** (AC: 4, 5, 6)
  - [ ] Aggiungere logging dettagliato in `index_chunks()`:
    - Prima embedding: chunks count, batch size
    - Durante retry (se avviene): attempt number, error type, backoff time
    - Dopo embedding: success, timing (embedding_ms)
  - [ ] Test embedding con chunk singolo (Python REPL o script):
    ```python
    from langchain_openai import OpenAIEmbeddings
    import os
    # Variabile gi√† caricata da .env file
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    test_chunk = "Testo test per embedding validation"
    vector = embeddings.embed_query(test_chunk)
    print(f"Vector dimension: {len(vector)}, First 5 values: {vector[:5]}")
    ```
  - [ ] Verificare dimensione vettore corretta (1536 per text-embedding-3-small)
  - [ ] Verificare OpenAI API key valida (da `.env` file root) e non rate-limited
  - [ ] Test retry logic con mock: simula RateLimitError ‚Üí verifica retry attivo

- [ ] **Task 10: Vector Indexing Validation** (AC: 4, 5)
  - [ ] Aggiungere logging in `SupabaseVectorStore.add_texts()`:
    - Prima insert: texts count, metadata count
    - Dopo insert: IDs returned, insert timing
  - [ ] Dopo indexing, query database:
    ```sql
    SELECT id, content, embedding, metadata, created_at
    FROM document_chunks
    WHERE document_id = '<document_id>'
    LIMIT 5;
    ```
  - [ ] Verificare chunks presenti
  - [ ] **CRITICO**: Verificare embedding NOT NULL (issue production: 121 chunks con NULL)
  - [ ] Se embedding NULL, investigare cause:
    - Celery task fallita senza error logging?
    - SupabaseVectorStore.add_texts() ritorn√≤ IDs ma non salv√≤ embeddings?
    - Transazione database rollback?
  - [ ] Verificare indice pgvector: `SELECT indexname FROM pg_indexes WHERE tablename = 'document_chunks';`

- [ ] **Task 11: Status Update Validation** (AC: 4)
  - [ ] Verificare `update_document_status()` chiamata correttamente
  - [ ] Query database dopo pipeline:
    ```sql
    SELECT status, updated_at, error
    FROM documents
    WHERE id = '<document_id>';
    ```
  - [ ] Verificare status "completed" se success, "error" se failure
  - [ ] Se status error, verificare campo `error` contiene messaggio significativo

### Fase 3: End-to-End Validation

- [ ] **Task 12: Database Integrity Audit** (AC: 5)
  - [ ] Query documenti "completed" senza chunks:
    ```sql
    SELECT d.id, d.file_name, d.status, COUNT(c.id) as chunk_count
    FROM documents d
    LEFT JOIN document_chunks c ON d.id = c.document_id
    WHERE d.status = 'completed'
    GROUP BY d.id
    HAVING COUNT(c.id) = 0;
    ```
  - [ ] Query chunks con embedding NULL:
    ```sql
    SELECT COUNT(*) as null_embeddings_count
    FROM document_chunks
    WHERE embedding IS NULL;
    ```
  - [ ] Verificare indice pgvector performance:
    ```sql
    EXPLAIN ANALYZE
    SELECT id, content, 1 - (embedding <=> '[0.1, 0.2, ...]'::vector) as similarity
    FROM document_chunks
    ORDER BY embedding <=> '[0.1, 0.2, ...]'::vector
    LIMIT 10;
    ```
  - [ ] Documentare anomalie trovate

- [ ] **Task 13: Semantic Search E2E Test** (AC: 6)
  - [ ] Ingerire documento test noto: "Lombalgia acuta: diagnosi e trattamento"
  - [ ] Attendere pipeline completion (status "completed")
  - [ ] Eseguire semantic search (PowerShell):
    ```powershell
    curl -X POST http://localhost:8000/api/v1/rag/search `
      -H "Authorization: Bearer $JWT_TOKEN" `
      -H "Content-Type: application/json" `
      -d '{"query": "Come si tratta la lombalgia acuta?", "match_count": 5}'
    ```
  - [ ] Verificare response:
    - Status 200 OK
    - Results array non vuoto
    - Score > 0 per top results
    - Content contiene informazioni rilevanti lombalgia
    - Metadata include document_id, document_name
  - [ ] Se results vuoti o irrilevanti, investigare:
    - Embedding query generato correttamente?
    - pgvector similarity search funziona?
    - match_threshold troppo restrittivo?
  - [ ] Aggiungere logging dettagliato in `perform_semantic_search()`:
    - Query embedding dimension
    - Supabase query executed
    - Results count before/after threshold filter

- [ ] **Task 14: Chat RAG E2E Test** (AC: 7)
  - [ ] Prerequisito: semantic search funzionante (Task 13 completato)
  - [ ] Eseguire chat query (PowerShell):
    ```powershell
    curl -X POST http://localhost:8000/api/v1/rag/chat `
      -H "Authorization: Bearer $JWT_TOKEN" `
      -H "Content-Type: application/json" `
      -d '{"query": "Quali esercizi sono consigliati per lombalgia?", "match_count": 8}'
    ```
  - [ ] Verificare response:
    - Status 200 OK
    - answer presente e non-vuoto
    - context_chunks array presente (chunks retrieved)
    - Risposta coerente con context chunks
    - Citazioni presenti (se implementate via Story 3.4)
  - [ ] Se risposta incoerente o generica (no RAG context usato), investigare:
    - LLM prompt include context chunks?
    - Context chunks correttamente formattati nel prompt?
    - LLM API call success?
  - [ ] Aggiungere logging in chat endpoint:
    - Chunks retrieved count
    - Context length in prompt
    - LLM model used
    - LLM response time

### Fase 4: Analysis & Reporting

- [ ] **Task 15: Configuration Gap Analysis** (AC: 8)
  - [ ] Analizzare `docker-compose.yml`:
    - Services dependencies verificati: api‚Üíredis, celery-worker‚Üíredis+api, web (independence)
    - Restart policies: verificare se definiti (default: no restart automatico)
    - Health checks: verificare se definiti per api, celery-worker
    - Network: `fisio-rag-net` presente
  - [ ] Verificare `.env` file root (esiste e funziona):
    - Variabili richieste presenti (riferimento: `ENV_TEST_TEMPLATE.txt`)
    - Nessun placeholder value (verificare con container env dump)
    - Secret management: verificare nessun .env committed in git
  - [ ] Verificare `apps/api/.env.test.local` per test:
    - Template disponibile: `ENV_TEST_TEMPLATE.txt`
    - CELERY_ENABLED=false per test sync
    - DATABASE_URL test DB (non production)
  - [ ] Confrontare con `docs/admin-setup-guide.md` o README:
    - Setup steps completezza
    - Environment variables documentation
  - [ ] Creare documento: `docs/troubleshooting/config-gaps-found.md`

- [ ] **Task 16: Test Coverage Gap Analysis** (AC: 9)
  - [ ] Analizzare test results Story 2.5:
    - 48 unit tests PASSED: cosa testano esattamente? (mock-based?)
    - 2 integration tests PASSED: quali scenari coprono?
    - 8 integration tests SKIPPED: perch√©? Infrastructure blockers?
  - [ ] Identificare test gaps:
    - Unit tests mock OpenAI ‚Üí integration con API reale non testata
    - Unit tests mock Supabase ‚Üí pgvector similarity non testata
    - Unit tests mock Celery ‚Üí async task execution non testata
  - [ ] Pianificare test mancanti:
    - Integration test con test DB reale
    - E2E test con Docker Compose testcontainers
    - Load test pipeline (10+ documenti concurrenti)
  - [ ] Creare documento: `docs/qa/test-coverage-gaps-analysis.md`

- [ ] **Task 17: Root Cause Analysis - Embedding NULL Issue** (AC: 10, focus su issue production)
  - [ ] Investigare issue "121 chunks con embedding NULL":
    - Query timestamp creazione chunks: quando furono creati?
    - Query logs API/Celery per quel timeframe
    - Celery worker era running in quel momento?
    - OpenAI API era down o rate-limiting?
  - [ ] Verificare se issue riproducibile:
    - Ingerire nuovo documento
    - Monitorare pipeline real-time
    - Verificare embedding salvato correttamente
  - [ ] Se riproducibile, identificare step fallente:
    - Celery task enqueued ma non executed?
    - OpenAI API call failed ma error non logged?
    - Supabase insert failed silently?
  - [ ] Se non riproducibile (fixed in Story 2.5?):
    - Confermare retry logic ora presente impedisce issue
    - Verificare troubleshooting guide documenta diagnostics
  - [ ] Documentare findings: `docs/troubleshooting/root-cause-embedding-null.md`

- [ ] **Task 18: Create Actionable Fix Backlog** (AC: 10)
  - [ ] Aggregare tutti issue trovati Tasks 1-17
  - [ ] Per ogni issue, creare entry:
    ```markdown
    ### Issue ID: <SEVERITY>-<NUMBER>
    **Severity**: P0 | P1 | P2 | P3
    **Component**: API | Celery | Docker | Database | Test Suite
    **Root Cause**: <descrizione tecnica>
    **Impact**: <cosa non funziona per l'utente>
    **Proposed Fix**: <soluzione tecnica>
    **Effort Estimate**: <ore di sviluppo>
    **Dependencies**: <altri issue da fixare prima>
    **Already Tracked**: Quality Gate 2.5 issue <ID> | New Issue
    ```
  - [ ] Ordinare per severity P0 ‚Üí P1 ‚Üí P2 ‚Üí P3
  - [ ] Raggruppare fix correlati (es. tutti Celery fixes in una story)
  - [ ] Creare documento: `docs/reports/rag-validation-fix-backlog.md`

- [ ] **Task 19: Executive Summary Report** (AC: tutti)
  - [ ] Creare report esecutivo: `docs/reports/rag-validation-summary.md`
  - [ ] Sezioni:
    - **Validation Status**: PASS | PARTIAL | FAIL per ogni AC
    - **Critical Issues Found**: lista P0 blockers con impact
    - **Root Causes Identified**: top 5 cause sistemiche
    - **Recommended Actions**: prioritized roadmap
    - **Timeline Estimate**: effort totale fix P0, P1, P2
    - **Risk Assessment**: cosa succede se non fixiamo (degradation scenarios)
  - [ ] Allegare logs chiave, screenshots, query results come evidence
  - [ ] Presentare finding a team per validazione e prioritization

## Dev Notes

### Architecture Context

**Pipeline RAG Attuale (Story 2.5 - Implemented & Tested)**:
```
Document Upload (POST /api/v1/admin/knowledge-base/sync-jobs)
    ‚Üì
1. Enhanced Extraction (DocumentExtractor) ‚Üí text, images[], tables[]
    ‚Üì
2. Enhanced Classification (classify_content_enhanced) ‚Üí domain, structure_type, confidence
    ‚Üì
3. Polymorphic Chunking (ChunkRouter) ‚Üí chunks[], strategy_name
    ‚Üì
4. Document Persistence (save_document_to_db) ‚Üí document_id, status "processing"
    ‚Üì
5. Indexing (index_chunks):
    ‚îú‚îÄ 5a. Batch Embedding (OpenAIEmbeddings.embed_documents) ‚Üí vectors[]
    ‚îú‚îÄ 5b. Vector Storage (SupabaseVectorStore.add_texts) ‚Üí chunk IDs
    ‚îî‚îÄ Retry logic (tenacity): RateLimitError ‚Üí exponential backoff
    ‚Üì
6. Status Update (update_document_status) ‚Üí status "completed"
    ‚Üì
Semantic Search (POST /api/v1/knowledge-base/search)
    ‚Üì
Chat RAG (POST /api/v1/chat/query + /chat/sessions/{id}/messages) ‚Üí answer + context_chunks
```

**Riferimenti Codice (Post Story 5.2 Refactoring):**
- **Pipeline:** `apps/api/api/routers/knowledge_base.py:start_sync_job()` (lines 178-403) ‚úÖ
- **Extraction:** `apps/api/api/knowledge_base/extractors.py:DocumentExtractor` ‚úÖ
- **Classification:** `apps/api/api/knowledge_base/classifier.py:classify_content_enhanced()` ‚úÖ
- **Chunking:** `apps/api/api/ingestion/chunk_router.py:ChunkRouter` ‚úÖ
- **Indexing:** `apps/api/api/knowledge_base/indexer.py:index_chunks()` ‚úÖ
- **Search:** `apps/api/api/knowledge_base/search.py:perform_semantic_search()` ‚úÖ
- **Chat:** `apps/api/api/routers/chat.py:chat_endpoint()` (line 311) + `create_chat_message()` (line 89) ‚úÖ
- **Classify Endpoint:** `apps/api/api/routers/knowledge_base.py:classify()` (line 112) ‚úÖ

**Riferimenti Documentazione:**
- **Story 2.5:** `docs/stories/2.5.intelligent-document-preprocessing.md` (Pipeline RAG implementata)
- **Story 5.2:** `docs/stories/5.2-fastapi-modularization-architecture-refactoring.md` (Architettura modulare)
- **Story 5.5:** `docs/stories/5.5-final-test-suite-cleanup.md` (Test suite 100% pass rate)
- **Quality Gate 2.5:** `docs/qa/gates/2.5-intelligent-document-preprocessing-pipeline-completion.yml` (PASS 90/100)
- **Architecture:** `docs/architecture.md` Sezione 7 (Struttura Unificata - aggiornata Story 5.2)
- **Troubleshooting:** `docs/troubleshooting/pipeline-ingestion.md` (se esistente)

**Riferimenti File Configurazione (Templates Non-Sensitive):**

Il progetto utilizza 3 file `.env` distinti per configurazione multi-ambiente. I file reali contengono secrets e sono gitignored. I template elencati mostrano nomi variabili e valori non-sensitive utili per reference:

1. **Root Project Environment** (`.env` nella root progetto)
   - **Template reference:** `ENV_TEMPLATE.txt`
   - **Location:** `C:\Users\user\Desktop\Claude-Code\fisio-rag-master\APPLICAZIONE\ENV_TEMPLATE.txt`
   - **Scope:** Variabili condivise per Docker Compose e orchestrazione servizi
   - **Key variables:** `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`, `SUPABASE_JWT_SECRET`, `DATABASE_URL`, `OPENAI_API_KEY`, `CELERY_ENABLED`, `CELERY_BROKER_URL`, `CELERY_RESULT_BACKEND`, `ADMIN_EMAIL`, `TOKEN_SUPABASE_CHAT_FISIO`
   - **Note:** File reale `.env` contiene valori production/development ma non visionabile da agent per policy sicurezza

2. **API Test Environment** (`apps/api/.env.test.local`)
   - **Template reference:** `apps/api/ENV_TEST_TEMPLATE.txt`
   - **Location:** `C:\Users\user\Desktop\Claude-Code\fisio-rag-master\APPLICAZIONE\apps\api\ENV_TEST_TEMPLATE.txt`
   - **Scope:** Variabili per test suite automatizzati (pytest)
   - **Key differences from root:**
     - `CELERY_ENABLED=false` (sync execution per test isolati)
     - `TESTING=true` (flag ambiente test)
     - `RATE_LIMITING_ENABLED=false` (bypass rate limiting test)
     - Include `CLOCK_SKEW_LEEWAY_SECONDS=120` (JWT validation test)
     - Rate limit config specifiche: `EXCHANGE_CODE_RATE_LIMIT_*`, `REFRESH_TOKEN_RATE_LIMIT_*`, `ADMIN_CREATE_TOKEN_RATE_LIMIT_*`
   - **Note:** Database deve puntare a TEST DB, NON production (critical security requirement)

3. **Web Frontend Environment** (`apps/web/.env`)
   - **Template reference:** `apps/web/ENV_WEB_TEMPLATE.txt`
   - **Location:** `C:\Users\user\Desktop\Claude-Code\fisio-rag-master\APPLICAZIONE\apps\web\ENV_WEB_TEMPLATE.txt`
   - **Scope:** Variabili Vite/React frontend (prefisso `VITE_*`)
   - **Key variables:** `VITE_SUPABASE_URL`, `VITE_SUPABASE_ANON_KEY`, `VITE_API_BASE_URL`
   - **Note:** Solo public keys (ANON_KEY), nessun secret backend esposto

**Variabili Critiche per Validation Story 2.6:**

| Variabile | Valore Template | Scopo | File Reference |
|-----------|-----------------|-------|----------------|
| `SUPABASE_URL` | `https://kqjneskjzzlhayrpnfcp.supabase.co` | Endpoint Supabase project | Root + API Test + Web |
| `DATABASE_URL` | `postgresql://postgres.kqjn...@aws-1-eu-central-2.pooler...` | PostgreSQL connection string | Root + API Test |
| `CELERY_ENABLED` | `false` (test) / non-set (prod assume true) | Toggle async task processing | Root + API Test |
| `CELERY_BROKER_URL` | `redis://localhost:6379/0` | Redis broker Celery | Root + API Test |
| `OPENAI_API_KEY` | `<your-openai-api-key>` (placeholder) | API key OpenAI | Root + API Test |
| `SUPABASE_JWT_SECRET` | `<your-jwt-secret>` (placeholder) | Secret per JWT token validation | Root + API Test |
| `ADMIN_EMAIL` | `stefanopiga1976@gmail.com` | Email admin default | Root + API Test |
| `TESTING` | `true` | Flag environment test | API Test only |
| `RATE_LIMITING_ENABLED` | `false` | Toggle rate limiting (disable test) | API Test only |

**Usage Notes per Agent AI:**
- Template files sono **safe to read** (contengono solo placeholders e nomi variabili)
- File reali `.env` **NON visionabili** per policy sicurezza
- Per audit environment (Task 1): confrontare variabili richieste nel codice (`os.getenv()`) con template disponibili
- Per troubleshooting: template mostrano valori corretti formato/struttura (URL patterns, connection strings format)
- Per setup nuovi environment: template contengono istruzioni inline (es. "Use TEST database, NOT production")

### Known Issues - Resolution Status (Post Story 5.*)

**Issues Risolti (Story 5.* - Verificato):**
- ‚úÖ **DATA-001**: Embedding NULL (121 chunks) ‚Üí **RISOLTO** Story 2.5 (retry logic tenacity implementato)
- ‚úÖ **OPS-001**: Celery worker instability ‚Üí **RISOLTO** Story 5.4 (test isolation, rate limiting bypass)
- ‚úÖ **PERF-001**: OpenAI rate limiting ‚Üí **MITIGATO** Story 2.5 (adaptive batching implementato)
- ‚úÖ **TEST-001**: FK constraint violations ‚Üí **RISOLTO** Story 5.4.2 (fixture upsert con user creation)
- ‚úÖ **TEST-002**: Rate limit pollution ‚Üí **RISOLTO** Story 5.4 (bypass test environment)
- ‚úÖ **TEST-003**: Auth override propagation ‚Üí **RISOLTO** Story 5.4.1 (client_admin/client_student fixtures)

**Test Coverage Status (Post Story 5.5):**
- ‚úÖ **Integration tests:** 22 SKIPPED (by design - RL tests, E2E infra)
- ‚úÖ **Test suite:** 179/179 PASSED (100% pass rate attivi)
- ‚úÖ **Coverage:** 93% su apps/api
- ‚úÖ **E2E tests:** Pipeline testata (test_pipeline_e2e.py passano)
- ‚úÖ **Real file validation:** Extraction PDF/DOCX/tables testata (Story 2.5)

**Production Readiness Gaps (Questa Storia):**
- ‚è≥ Performance benchmarks production load (10+ concurrent ingestion)
- ‚è≥ Security audit JWT/secrets/CORS
- ‚è≥ Database integrity audit orphan records
- ‚è≥ Infrastructure health matrix production
- ‚è≥ Deployment checklist e rollback plan

### Investigation Strategy

**Approccio Bottom-Up**:
1. Partire da infrastruttura base (Docker, Redis, Celery)
2. Salire verso pipeline steps (extraction ‚Üí indexing)
3. Validare integrazioni E2E (search ‚Üí chat)

**Approccio Top-Down (parallelo)**:
1. Tentare E2E test utente (document upload ‚Üí chat query)
2. Quando fallisce, identificare step fallente
3. Drill-down su quello step specifico

**Logging Strategy**:
- Aggiungere logging dettagliato (INFO level) ogni step pipeline
- Formato JSON strutturato per facilit√† parsing
- Include timing metrics, input/output sizes, error context
- Usare correlation ID per tracciare single document attraverso pipeline

**Evidence Collection**:
- Screenshot errori UI (se esistono)
- Docker logs (ultimi 1000 righe per service)
- Database query results (anonimizzati se contengono PII)
- curl requests/responses per API endpoints
- Environment variables dump (sanitized per secrets)

### Tools & Commands

**NOTA IMPORTANTE:** Tutti i comandi terminal in questa storia sono per **PowerShell** (Windows). Sistema operativo target: Windows 10 (win32 10.0.19045). Shell: `C:\Windows\System32\cmd.exe` ma esecuzione comandi via PowerShell.

**Docker** (PowerShell):
```powershell
# Container status
docker compose ps

# Logs per service (last 100 lines)
docker logs fisio-rag-api --tail=100
docker logs fisio-rag-celery-worker --tail=100
docker logs fisio-rag-redis --tail=100

# Follow logs real-time
docker logs -f fisio-rag-api

# Container shell access (bash dentro container Linux)
docker exec -it fisio-rag-api bash

# Redis CLI
docker exec fisio-rag-redis redis-cli
```

**Database (Supabase PostgreSQL)** (PowerShell):
```powershell
# Connect via psql (se credenziali disponibili e psql installato su Windows)
psql -h <supabase-host> -U postgres -d postgres

# Query via API (se pgAdmin o Supabase Studio - browser-based)
# Oppure tramite Python script con asyncpg
```

**Python Testing** (PowerShell):
```powershell
# Run unit tests (da project root)
cd apps/api
poetry run pytest tests/ -v

# Run specific test file
poetry run pytest tests/test_enhanced_extraction.py -v

# Run with coverage
poetry run pytest tests/ --cov=api --cov-report=html

# Or inside API container (da qualsiasi directory)
docker exec fisio-rag-api pytest tests/ -v
```

**API Testing** (PowerShell):
```powershell
# Health check (via Traefik proxy o diretto)
curl http://localhost/health
# Expected: {"status":"ok"}

# Generate admin JWT
python scripts/admin/generate_jwt.py --email admin@fisiorag.local --expires-days 1
# Output: JWT token (copia valore generato)

# Capture JWT in variabile (PowerShell)
$JWT = (python scripts/admin/generate_jwt.py --email admin@fisiorag.local --expires-days 1) -split "`n" | Select-String -Pattern "^eyJ" | ForEach-Object { $_.ToString() }

# Test endpoint sync-jobs (aggiornato endpoint path)
curl -X POST http://localhost/api/v1/admin/knowledge-base/sync-jobs `
  -H "Authorization: Bearer $JWT" `
  -H "Content-Type: application/json" `
  -d '@temp/payloads/payload_ingestion.json'
# Expected: {"job_id":"<uuid>","inserted":<count>,"timing":{...}}

# Test endpoint semantic search
curl -X POST http://localhost/api/v1/knowledge-base/search `
  -H "Content-Type: application/json" `
  -d '{"query":"lombalgia acuta trattamento","match_count":5}'
# Expected: {"results":[{"content":"...","similarity_score":0.85,...}]}

# Test endpoint chat query
curl -X POST http://localhost/api/v1/chat/query `
  -H "Authorization: Bearer $JWT" `
  -H "Content-Type: application/json" `
  -d '{"question":"Come si tratta la lombalgia?","match_count":8}'
# Expected: {"chunks":[{"id":"...","content":"...","similarity":0.82}]}
```

### Testing

**Unit Tests (New)**:
- `tests/test_environment_validation.py`: verifica tutte env vars definite
- `tests/test_docker_health.py`: mock health check responses
- `tests/test_pipeline_logging.py`: verifica logging output structure

**Integration Tests (New)**:
- `tests/integration/test_celery_task_execution.py`: test task enqueue + result fetch
- `tests/integration/test_embedding_real_api.py`: test OpenAI embedding con API reale (skip se key mancante)
- `tests/integration/test_supabase_vector_store.py`: test pgvector insert + search con test DB

**E2E Tests (Automated - 2025-10-11)**:
- `poetry run pytest -o "addopts=''" tests/test_pipeline_e2e.py::TestPipelineE2E::{test_full_pipeline_sync_mode,test_semantic_search_after_indexing}`
  - Risultato: PASS (Supabase/Postgres reali), durata ~22s per test.
  - Warning noti: `pytest.mark.timeout` non registrato, `supabase` Deprecation (`timeout`/`verify`).
  - Output chat dopo indexing ancora fallback ‚ÄúNessun contenuto rilevante‚Äù ‚Üí indagare match_threshold/query quality.
  - Fix teardown: `apps/api/api/database.py` forza `pool.terminate()` se `asyncpg` non chiude entro 5s (evita TimeoutError in shutdown).

**E2E Tests (Manual)**:
- Checklist manuale con expected results per ogni step
- Documentare in `docs/qa/manual-e2e-checklist.md`
- Screenshots evidence per pass/fail

**Load Tests (Future)**:
- Locust o pytest-benchmark per pipeline throughput
- Target: 10 documenti concorrenti senza degradation

### Deliverables

**Report Principali (Aggiornati per Production Readiness):**
1. `docs/reports/rag-production-readiness-summary.md` - Executive summary audit
2. `docs/reports/rag-performance-benchmarks.md` - Latency p95/throughput metrics
3. `docs/reports/rag-security-audit.md` - JWT/secrets/CORS audit findings
4. `docs/reports/rag-infrastructure-health.md` - Docker/Celery/Redis resilience
5. `docs/reports/rag-database-integrity.md` - DB audit (orphan records, indices)
6. `docs/reports/rag-deployment-checklist.md` - Pre/post deployment steps

**Evidence Artifacts**:
- `temp/logs/` - Docker logs salvati per analisi
- `temp/screenshots/` - UI behavior screenshots
- `temp/queries/` - SQL queries + results
- `temp/payloads/` - API request/response samples

**Code Changes (minimal, only logging additions)**:
- Logging enhancements in pipeline steps (non-breaking)
- Health check endpoints (se mancanti)
- Debug utility scripts (non-production)

### Out of Scope

**Non parte di questa story**:
- Fix implementativi (solo identification)
- Refactoring architetturale
- Performance optimization
- New features
- UI changes

**Rationale**: Questa √® storia di **validazione e analisi**, non implementazione. Output √® backlog prioritizzato per storie successive.

### Success Criteria

**Story considerata completata quando**:
1. Tutti 10 AC verificati (PASS/FAIL documentato)
2. Actionable backlog creato con P0/P1/P2/P3 priorities
3. Root cause identificata per discrepanza documentazione vs realt√†
4. Executive summary presentato e validato da team
5. Team ha roadmap chiara per fix (storie successive)

**Story considerata fallita se**:
- Impossibile riprodurre issue utente (ma allora perch√© segnalato?)
- Root cause non identificabile (troppo complesso, serve external expert?)
- Fix backlog troppo vago (no actionable items)

## Change Log

| Date | Version | Description | Author |
|---|---|---|---|
| 2025-10-07 | 0.1 | Draft iniziale Story 2.6 - RAG System Validation & Gap Analysis. Investigazione sistematica discrepanza documentazione (Story 2.5 PASS) vs comportamento reale (sistema non funzionante). Focus su pipeline completa: Celery, Redis, Docker, LangChain, Supabase. Output: fix backlog prioritizzato + root cause analysis. | SM |
| 2025-10-10 | 1.0 | **MAJOR UPDATE** - Allineamento post Story 5.* (5.1-5.5 completate). **Nuovo focus: Production Readiness Validation**. Aggiornamenti: (1) File locations modulari (routers/, services/, schemas/), (2) Test suite status 100% pass rate (179/179), (3) Known issues resolution status (tutti risolti), (4) AC aggiornati (performance benchmarks, security audit, deployment checklist), (5) Task ridondanti rimossi (pipeline gi√† validata). Riferimenti: Story 5.2 (architettura), 5.5 (test suite), report alignment analysis. | AI + QA |
| 2025-10-10 | 1.1 | Aggiunta sezione **Riferimenti File Configurazione (Templates Non-Sensitive)** con documentazione dettagliata dei 3 file environment: (1) `.env` root (template: `ENV_TEMPLATE.txt`), (2) `apps/api/.env.test.local` (template: `ENV_TEST_TEMPLATE.txt`), (3) `apps/web/.env` (template: `ENV_WEB_TEMPLATE.txt`). Espansione Task 1 con subtask granulari per audit multi-ambiente (extraction variabili, mapping Docker, gap analysis, security audit). Aggiunta tabella variabili critiche con valori template e scope. Note usage per agent AI su safety template files e policy non-visionabilit√† file reali. **Chiarimento shell environment:** Tutti comandi terminal specificati come PowerShell (Windows 10). Note aggiunte in sezioni Tasks e Tools & Commands. Conversione blocchi ```bash ‚Üí ```powershell (Task 13, 14, Docker commands). | AI |

