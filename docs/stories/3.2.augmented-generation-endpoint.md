# Story 3.2: Augmented Generation Endpoint

**Status:** Done

## Story

**As a** Sviluppatore Backend,
**I want** un endpoint che usi un LLM per generare una risposta contestualizzata,
**so that** l'utente riceva una risposta utile basata sulle fonti.

[Fonte: `docs/prd/sezione-9-epic-3-dettagli-interactive-rag-experience.md`]

## Acceptance Criteria

1. Prompt ingegnerizzato per rispondere solo dal contesto e citare fonti.
2. L'endpoint riceve domanda e chunk.
3. Invoca l'LLM.
4. Restituisce la risposta con citazioni.
5. Gestisce la memoria a breve termine.
6. Centralizza il modello dati dell'output AG (`AnswerWithCitations`) in un modulo riutilizzabile comune.

[Fonte: `docs/prd/sezione-9-epic-3-dettagli-interactive-rag-experience.md`]

## Dev Notes

### Previous Story Insights
- Endpoint semantico disponibile: `POST /api/v1/chat/query` con auth e rate limiting 60/min. Esporta chunk con `id`, `document_id`, `content`, `similarity`. 
  [Fonti: `apps/api/api/main.py` L492–L515; `docs/stories/3.1.semantic-search-endpoint.md`]

### Data Models
- `ChatMessage`: `id`, `session_id`, `role`, `content`, `source_chunk_ids`, `metadata`, `created_at`.
- `DocumentChunk`: `id`, `document_id`, `content`, `embedding`, `metadata`, `created_at`, `updated_at`.
[Fonti: `docs/architecture/sezione-4-modelli-di-dati.md` L15–L26]

### API Specifications
- Host: `/api/v1`.
- Sicurezza: JWT Bearer Token (Supabase Auth).
- Endpoint implementato: `POST /api/v1/chat/sessions/{sessionId}/messages` — invoca catena RAG e restituisce risposta con citazioni.
  [Fonti: `apps/api/api/main.py`]

#### Contratto attuale (implementato)
- Request model: `ChatMessageCreateRequest`
  - `question: str` (obbligatorio)
  - `chunks: list[ChatQueryChunk] | None` (opzionale; stessa forma di `ChatQueryResponse.chunks`)
- Response model: `ChatMessageCreateResponse`
  - `message_id: str`
  - `answer: str | None`
  - `citations: list[str] | None`
- Rate limiting: `60/minute`
- Auth: `Depends(_verify_jwt_token_runtime)`
- Logging: evento `ag_message_request` con `session_id` e `has_chunks`
- Validazioni: `400` se `sessionId` o `question` mancanti/blank; `400` se `chunks` assenti per AG
[Fonti: `apps/api/api/main.py`]

### LLM Orchestration

#### PromptTemplate (ChatPromptTemplate)
- Creare un `ChatPromptTemplate` che accetti le variabili `question` e `context`. Il prompt deve imporre all'LLM di usare esclusivamente il contenuto di `context`, di indicare quando l'informazione non è presente e di fornire citazioni delle fonti (ID dei chunk) usate. Usare `from_messages` con ruoli `system` e `user` e variabili `{context}` e `{question}`.

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "Sei un assistente che risponde SOLO usando il CONTEXT fornito. Se l'informazione non è nel CONTEXT, rispondi 'Non trovato nel contesto'. Includi le citazioni degli ID dei chunk usati."),
    ("user", "CONTEXT:\n{context}\n\nDOMANDA:\n{question}")
])
```

Documentazione: [Prompt Templates](https://python.langchain.com/docs/concepts/prompt_templates/)

#### LLM (ChatOpenAI)
- Istanziare `ChatOpenAI` dal pacchetto `langchain-openai`, impostando `model` e `temperature=0` per risposte deterministiche. L'istanza è un `Runnable` compatibile con LCEL.

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-5-nano", temperature=0)
```

Documentazione: [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

#### Output Parser
- Per output semplice usare `StrOutputParser`, che restituisce una stringa. Per estrazione strutturata (risposta e citazioni) usare `PydanticOutputParser` con un modello Pydantic che definisce i campi `risposta` e `citazioni`. `PydanticOutputParser` è raccomandato per questo caso d'uso.

```python
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
from api.models.answer_with_citations import AnswerWithCitations

# Parser semplice a stringa
str_parser = StrOutputParser()

# Parser strutturato Pydantic (modello centralizzato)
parser = PydanticOutputParser(pydantic_object=AnswerWithCitations)

# Suggerimento per il prompt (facoltativo):
# format_instructions = parser.get_format_instructions()
# e includere {format_instructions} nel messaggio system per guidare l'LLM a produrre JSON conforme.
```

Documentazione: [PydanticOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html)

#### LCEL Chain
- Comporre i componenti con LCEL usando l'operatore `|`. La catena invoca il prompt con gli input `{question, context}`, passa il messaggio risultante a `ChatOpenAI` e infine al parser. Esempio con `PydanticOutputParser` per ottenere un oggetto strutturato.

```python
chain = prompt | llm | parser

# Esecuzione
result = chain.invoke({
    "question": "Quali sono i prerequisiti?",
    "context": "[chunk_id=doc-1#p3] I prerequisiti includono...\n[chunk_id=doc-2#p1] Inoltre..."
})

# result è un'istanza AnswerWithCitations con campi: result.risposta, result.citazioni
```

Documentazione: [LCEL Cheatsheet](https://python.langchain.com/docs/how_to/lcel_cheatsheet/)

### Sicurezza e Performance
- Autenticazione: JWT Supabase; nessun secret hardcoded.
- Rate limiting: applicazione coerente con endpoint sensibili; target p95 < 500ms lato backend per operazioni di chat.
- Osservabilità: logging `ag_metrics` con `latency_ms`, `p95_ms` e finestra scorrevole di campioni.
[Fonti: `docs/architecture/sezione-10-sicurezza-e-performance.md` L3–L17, L25–L31]

### Testing
- Backend (Pytest + HTTPX): test async; mocking di LLM e dipendenze esterne; scenari positivi/negativi; verificare che le citazioni siano presenti in output.
- Coverage >= 80% per nuovo codice.
[Fonti: `docs/architecture/sezione-11-strategia-di-testing.md` L3–L10, L33–L37]

## File Locations
- Monorepo: `/`
- Backend: `/apps/api/`
- Docs: `/docs/`
[Fonti: `docs/architecture/sezione-7-struttura-unificata-del-progetto.md`]

## Tasks / Subtasks
- [x] Definire contratto endpoint AG: `POST /api/v1/chat/sessions/{sessionId}/messages` (body con `question`, `chunks` provenienti da 3.1). (AC: 2–4) [Fonte: `docs/architecture/sezione-5-specifica-api-sintesi.md` L8–L18]
- [x] Progettare prompt vincolato al contesto con richiesta di citazioni e stile sobrio. (AC: 1) [Fonte: `docs/prd/sezione-9-epic-3-dettagli-interactive-rag-experience.md`]
- [x] Implementare chain LLM con LangChain; iniettare chunk come contesto; includere citazioni nel risultato. (AC: 1–4) [Fonti: `docs/architecture/sezione-3-tech-stack.md` L11; `docs/architecture/langchain-overview.md`]
- [x] Gestire memoria breve per sessione (persistenza messaggi minimi in `ChatMessage`); definire schema e salvataggio. (AC: 5) [Fonte: `docs/architecture/sezione-4-modelli-di-dati.md` L19–L26]
- [x] Aggiungere test unit/integration: mocking LLM; validare presenza citazioni e corretto uso chunk. (Testing) [Fonte: `docs/architecture/sezione-11-strategia-di-testing.md` L13–L22]
- [x] Applicare rate limiting e logging coerenti con standard esistenti. (Sicurezza/Performance) [Fonte: `docs/architecture/sezione-10-sicurezza-e-performance.md` L18–L27]

### Dev Agent Record
- Agent Model Used: `.cursor/rules/bmad/dev.mdc` (`*develop-story`)
- Completion Notes:
  - Implementato endpoint AG con catena LCEL e fallback sicuro quando LLM non disponibile
  - Aggiunta persistenza in‑memory messaggi di chat per sessione
  - Introdotto `_auth_bridge` per facilitare monkeypatch nei test
  - Creato test `apps/api/tests/test_ag_endpoint.py`
  - Applicato rate limiting `60/minute` e logging evento `ag_message_request`
- File List:
  - `apps/api/api/main.py`
  - `apps/api/tests/test_ag_endpoint.py`

## Testing
- Test positivi: risposta include testo e citazioni riferite a `document_id/id` dei chunk di input.
- Test negativi: mancanza `chunks` → 400; token assente/invalid → 401; throttling → 429.
- Misure: coverage >= 80%.
[Fonti: `docs/architecture/sezione-11-strategia-di-testing.md` L3–L10, L33–L37]

## Change Log
| Date | Version | Description | Author |
|---|---|---|---|
| 2025-09-24 | 0.1 | Bozza iniziale Story 3.2 | SM |
| 2025-09-26 | 0.2 | Implementazione endpoint con fallback LLM, persistenza in‑memory e test di base | Dev |
