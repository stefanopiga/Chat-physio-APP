# Story 2.8: Ripristino Infrastruttura Esterna (OpenAI + Supabase) e Riattivazione Pipeline E2E/P95

**Status:** Draft — High Priority

**Last Updated:** 2025-10-13

## Story

As a Platform/Backend team,
I want to restore and validate external dependencies (OpenAI + Supabase) and re-enable the E2E test pipeline with real P95 metrics collection,
so that we can complete end-to-end validation and measure production-like performance.

## Context

Allineamento rapido codice/documentazione risulta coerente per variabili d’ambiente e flussi principali:
- Config chiave:
  - `OPENAI_API_KEY` per embeddings/chat (coerente con `apps/api/api/knowledge_base/indexer.py`).
  - `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY` per client/admin (coerente con `create_client(...)`).
  - Test env: documentato in `apps/api/ENV_TEST_SETUP.md` e template `apps/api/ENV_TEST_TEMPLATE.txt`.
- Script/validazioni disponibili:
  - Connettività/Integrità DB: `scripts/validation/database_connectivity_test.py`, `scripts/validation/database_integrity_audit.py`.
  - Sync job integrazione: `apps/api/tests/test_sync_job_integration.py` (richiede Supabase attivo).
  - P95 locale: `scripts/perf/p95_local_test.js`, `scripts/perf/summarize_p95.py`.

Nota: la Story 2.7 ha completato hardening/ottimizzazioni DB ma E2E/p95 reali sono bloccati finché OpenAI/Supabase non sono pienamente operativi in ambiente test/staging.

## Acceptance Criteria

1) OpenAI credenziali e secret management (test/staging)
- AC1.1: Chiave OpenAI valida e con quota attiva per ambiente di test; verifica via `curl /models` o script health.
- AC1.2: Secret configurati in CI/CD e locali: `TEST_OPENAI_API_KEY` (Actions/runner) e `.env.test.local` aggiornato (senza leakage in repo).
- AC1.3: Limiti di spesa e rate limit definiti per l'account di test (evidenza screenshot impostazioni budget).
- AC1.4: Pipeline/runner validati per assenza di leakage secrets nei log tramite scanner (es. `trufflehog`) con report archiviato.

2) Supabase stato e connettività
- AC2.1: `SUPABASE_URL` e `SUPABASE_SERVICE_ROLE_KEY` validi in test/staging; `scripts/validation/database_connectivity_test.py` PASS.
- AC2.2: Permessi/admin OK per job sync: `apps/api/tests/test_sync_job_integration.py` eseguibile e PASS (o evidenza motivata di skip controllato con piano di sblocco).
- AC2.3: Health check periodico per job di sync e canali (log + metriche Supabase disponibili); query critiche senza errori 5xx.
- AC2.4: Script health Supabase schedulati/automatizzati (CI o cron) con retention log e differenze monitorate.

3) Pipeline E2E riabilitata + P95 reale
- AC3.1: E2E pipeline attiva in CI con variabili d'ambiente test configurate (OpenAI + Supabase).
- AC3.2: Suite E2E esegue end-to-end sulle rotte principali (chat/KB ingest) e produce report.
- AC3.3: Raccolta P95 reale pubblicata in `reports/metrics-p95-YYYYMMDD.md` e screenshot dashboard Supabase salvati.
- AC3.4: Delta tra P95 script locale e dashboard Supabase <=10%; discordanze maggiori documentate con azioni correttive.
- AC3.5: Piano di fallback/retry per downtime OpenAI o Supabase esercitato almeno in uno scenario negativo controllato.

4) Compliance & sicurezza
- AC4.1: Nessun secret hard-coded nel repository; `.env*` ignorati da git (verificato) e secrets solo in vault/Actions secrets.
- AC4.2: Documentata la procedura di rotation chiavi e il contatto/owner degli account (OpenAI/Supabase) per test/staging.
- AC4.3: Vault/secrets manager centralizzato con audit trail attivo e ownership definita.
## Tasks (Order)

### Fase 0 - Preparazione
- [ ] Identificare/creare progetto Supabase "test/staging" dedicato (separato dal prod).
- [ ] Richiedere/creare chiavi OpenAI per test con budget/limiti configurati.
- [ ] Aggiornare `apps/api/ENV_TEST_TEMPLATE.txt` in base all'ambiente effettivo (solo template, nessun secret reale nel repo).
- [ ] Confermare secret manager centralizzato (GitHub Actions secrets + eventuale vault) e definire owner + frequenza rotation in `docs/operations/secrets-rotation.md`.

### Fase 1 - Secrets & Config
- [ ] Impostare in CI/CD i segreti: `TEST_SUPABASE_URL`, `TEST_SUPABASE_SERVICE_KEY`, `TEST_OPENAI_API_KEY` (+ eventuali DB vars).
- [ ] Preparare `.env.test.local` (non commit) per esecuzioni locali; validare caricamento variabili.
- [ ] Aggiungere step di validazione env in pipeline (echo safe + check placeholder vs reali).
- [ ] Integrare scanner secrets/log (es. `trufflehog`, `detect-secrets`) nella pipeline e archiviare report sanitizzato.
- [ ] Applicare controlli di masking log nei job CI per impedire stampa di secrets (`print(os.environ)` e simili vietati) e documentare la review log.

### Fase 2 - Verifica Supabase
- [ ] Eseguire `scripts/validation/database_connectivity_test.py` in test/staging e salvare log in `reports/`.
- [ ] Eseguire `scripts/validation/database_integrity_audit.py` e catturare esiti (warnings/ok) per il contesto attuale.
- [ ] Eseguire `apps/api/tests/test_sync_job_integration.py` (abilitandolo in CI) e raccogliere evidenze PASS.
- [ ] Automatizzare scheduling degli script health Supabase (CI nightly o cron) e monitorare differenze rispetto run precedenti.
- [ ] Documentare runbook fallback per downtime Supabase (retry/backoff, escalation) e validarne un test manuale.

### Fase 3 - Riattivazione E2E + P95
- [ ] Aggiornare workflow CI per includere E2E end-to-end con env test; rimuovere eventuali skip condizionali.
- [ ] Eseguire `scripts/perf/p95_local_test.js` contro l'ambiente test/staging; processare output con `scripts/perf/summarize_p95.py`.
- [ ] Raccogliere P95 reali dalla dashboard Supabase (query/HTTP se disponibili) e salvare screenshot.
- [ ] Garantire che il numero di richieste per run P95 soddisfi soglia minima (>=300) e documentarlo nel report.
- [ ] Analizzare delta tra script e dashboard Supabase; se >10% aprire issue/azione correttiva annotata nel report.
- [ ] Eseguire scenario di resilienza simulando quota esaurita/rate limit OpenAI per verificare retry/fallback e registrare esito.

### Fase 4 - Governance
- [ ] Documentare rotazione chiavi (OpenAI/Supabase), owner, e processo incident (quota/ban/rate limit) in `docs/operations/secrets-rotation.md`.
- [ ] Aggiornare `docs/reports/rag-production-readiness-summary.md` con stato E2E/P95.
- [ ] Aggiungere sezione monitoraggio in dashboard (quota OpenAI, errori 429/5xx, stato job sync) e definire alert attivi.
- [ ] Archiviare evidenze risk mitigation (report scanner secrets, screenshot quota, delta P95, fallback test) nel pacchetto story.
## Validazione & Verifiche

1) OpenAI
```bash
curl -s https://api.openai.com/v1/models \
 -H "Authorization: Bearer $OPENAI_API_KEY" | jq '.data | length' # > 0
```
- Evidenza: screenshot/quota usage da OpenAI dashboard.
- Report scanner secrets/log (es. `trufflehog`, `detect-secrets`) con 0 findings su repo/log pipeline.

2) Supabase connettività
```bash
poetry run python scripts/validation/database_connectivity_test.py \
  --url "$SUPABASE_URL" --service-key "$SUPABASE_SERVICE_ROLE_KEY" \
  --out reports/db_connectivity_test_YYYYMMDD.log
```
PASS richiesto, con note per eventuali skip motivati.
- Scheduler health Supabase attivo: log archiviati e differenze monitorate (report daily/cron).

3) Sync jobs health
- Esecuzione `apps/api/tests/test_sync_job_integration.py` in CI con env test: 100% PASS.
- Log annessi in `reports/test-sync-job-YYYYMMDD.log`.

4) E2E + P95
```bash
# E2E (runner CI): produce report JUnit/HTML
# P95 locale verso endpoint test/staging
node scripts/perf/p95_local_test.js --base-url "$BASE_URL_TEST" --out reports/p95_k6_YYYYMMDD.json
poetry run python scripts/perf/summarize_p95.py reports/p95_k6_YYYYMMDD.json > reports/metrics-p95-YYYYMMDD.md
```
- Screenshot P95 dal dashboard Supabase salvati in `docs/screenshots/p95-dashboard-YYYYMMDD.png`.
- Delta P95 script vs dashboard <=10%; se >10% allegare issue e nota nel report con piano correttivo.
- Numero richieste per run P95 registrato (>=300) e soglia soddisfatta.

5) Scenario resilienza servizi esterni
- Simulare quota OpenAI esaurita o risposta 429 e verificare che il retry/backoff documentato funzioni (log evidenza PASS).
- Documentare fallback per downtime Supabase con esito test manuale/automazione.

6) Audit secrets/vault
- Evidenza configurazione vault/secrets manager con audit trail e owner definiti.
- Review log CI per confermare masking dei secrets (nessun valore sensibile nei log salvati).
## Deliverables
- Log test/validazioni: `reports/db_connectivity_test_YYYYMMDD.log`, `reports/test-sync-job-YYYYMMDD.log`.
- Report P95: `reports/metrics-p95-YYYYMMDD.md` + `docs/screenshots/p95-dashboard-YYYYMMDD.png`.
- Aggiornamenti documentali: `docs/reports/rag-production-readiness-summary.md`, `docs/operations/secrets-rotation.md`.
- Evidenze budget/quota OpenAI (screenshot) e stato Supabase (health/metrics principali).
- Report scanner secrets/log (es. `reports/secrets-scan-YYYYMMDD.txt`) e checklist log sanitization.
- Issue/nota delta P95 (se >10%) con azione correttiva documentata.
- Runbook fallback downtime OpenAI/Supabase aggiornato (`docs/operations/openai-supabase-fallback-runbook.md`) e sezione alerting in `docs/monitoring/external-services-dashboard.md`.
## Dipendenze / Precondizioni
- Accesso agli account: OpenAI (chiave test con quota) e Supabase (URL + Service Role Key) per ambiente test/staging.
- Rete uscente abilitata su runner CI verso OpenAI/Supabase.
- Permessi per creare/aggiornare GitHub Actions secrets (o sistema CI equivalente).
- Secret manager/vault accessibile ai maintainer designati con audit logging attivo.
- Dataset staging aggiornato con volume richieste >=300 per run P95 e dati sintetici coerenti.
- Accesso al dashboard di monitoring (Supabase, OpenAI quota) per raccolta evidenze e alert.
## File List
- `apps/api/ENV_TEST_SETUP.md` (riferimento setup test)
- `apps/api/ENV_TEST_TEMPLATE.txt`
- `scripts/validation/database_connectivity_test.py`
- `scripts/validation/database_integrity_audit.py`
- `scripts/validation/generate_test_tokens.py` (nuovo - generazione JWT per test)
- `apps/api/tests/test_sync_job_integration.py`
- `scripts/perf/p95_local_test.js`
- `scripts/perf/summarize_p95.py`
- `scripts/perf/run_p95.ps1`
- `scripts/perf/.env.staging.local` (nuovo - config test locale)
- `docs/reports/rag-production-readiness-summary.md`
- `reports/metrics-p95-20251014-145728.md` (nuovo - test locale completato)
- `reports/p95_k6_20251014-145728.json` (nuovo - raw k6 output)
- `reports/metrics-p95-YYYYMMDD.md` (template per future run)
- `reports/db_connectivity_test_YYYYMMDD.log` (nuovo)
- `reports/test-sync-job-YYYYMMDD.log` (nuovo)
- `docs/screenshots/p95-dashboard-YYYYMMDD.png` (pending)
- `reports/secrets-scan-YYYYMMDD.txt` (nuovo)
- `docs/operations/secrets-rotation.md` (aggiornato)
- `docs/operations/openai-supabase-fallback-runbook.md` (nuovo/aggiornato)
- `docs/monitoring/external-services-dashboard.md` (nuovo/aggiornato)
## Dev Notes (Scrum Master)
- Verificato allineamento nomenclature env tra codice e docs; non sono emerse discrepanze bloccanti. La presenza di `.env` locali è correttamente esclusa da git. È necessario migrare tutte le esecuzioni in CI a secrets runtime e confermare che nessun secret appaia nei log.
- Integrare controlli contro i rischi High identificati (secret leakage, downtime servizi esterni, metriche P95 inattendibili) seguendo `docs/qa/assessments/2.8-risk-20251013.md`.
- Applicare il piano di test completo definito in `docs/qa/assessments/2.8-test-design-20251013.md`, includendo scenario negativo su OpenAI rate limit e tracciando tutte le evidenze.

**Update 2025-10-14** (Story 2.8.1):
- Test P95 locale completato parzialmente: chat endpoint OK (P95=1.07s, <1s target), sync-jobs bloccato per bottleneck classification (11.4s/req)
- Creati tool: `scripts/validation/generate_test_tokens.py` (JWT generation), `.env.staging.local` configurazione
- Reports: `reports/p95_k6_20251014-145728.json`, `reports/metrics-p95-20251014-145728.md`
- Pending: confronto dashboard Supabase, ottimizzazione classification pipeline

## QA Results

### Review Date: 2025-10-13

### Reviewed By: Quinn (Test Architect & Quality Advisor)

- Secret management ancora da validare con evidence vault + scanner attivi (`docs/qa/assessments/2.8-risk-20251013.md` - SEC-201).
- Resilienza verso downtime/rate limit OpenAI/Supabase non esercitata; fallback plan da provare (`docs/qa/assessments/2.8-risk-20251013.md` - OPS-201).
- Raccolta P95 reale necessita campionamento >=300 richieste e confronto dashboard (`docs/qa/assessments/2.8-test-design-20251013.md` - TD-P95-01/02).

### Gate Status

Gate: CONCERNS -> docs/qa/gates/2.8-ripristino-infrastruttura-esterna-openai-supabase-e-riattivazione-pipeline-e2e-p95.yml

