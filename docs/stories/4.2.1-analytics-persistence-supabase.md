# Story 4.2.1: Analytics Data Persistence to Supabase

**Status:** Backlog (Technical Debt Mitigation)

## Metadata
- **ID**: 4.2.1
- **Type**: Technical Debt / Enhancement
- **Epic**: Epic 4 — Post-MVP Enhancements
- **Priority**: High (Technical Debt from Story 4.2)
- **Complexity**: Medium-High
- **Effort Estimate**: 8-12 ore
- **Parent Story**: Story 4.2 — Analytics Dashboard
- **Technical Debt Reference**: R-4.2-1 (Data volatility in-memory)

---

## Story

**As a** Professore (Admin),  
**I want** dati analytics storicizzati e persistenti su database,  
**so that** posso analizzare trend temporali, confrontare performance nel tempo e identificare pattern di lungo periodo senza perdere dati al riavvio del sistema.

**Business Value**: Trasformare dashboard analytics da monitoring real-time a strumento di analisi storica, abilitando insights temporali e data-driven content optimization basati su dati aggregati nel tempo.

---

## Context & Technical Debt Background

### Origin: Story 4.2 Technical Debt Acceptance

**Technical Debt ID**: R-4.2-1  
**Description**: Story 4.2 implementa analytics dashboard con dati aggregati da store in-memory volatili (`chat_messages_store`, `feedback_store`, `ag_latency_samples_ms`). Dati persi al riavvio server.

**Impact Accepted in MVP**:
- ❌ No trend analysis temporali
- ❌ No confronto performance storico
- ❌ No analytics oltre sessione server corrente
- ⚠️ Business value dashboard limitato a monitoring real-time

**Trade-off Rationale (Story 4.2)**: Velocità implementazione MVP (5-7h) vs completezza funzionale. Dashboard funzionale per monitoring immediato, persistence differita a Phase 2.

**Acceptance Decision**: Tech Lead + Product Owner — 2025-10-02  
**Documentation**: 
- `docs/stories/4.2.analytics-dashboard.md` L236 (Risk table)
- `docs/qa/assessments/4.2-risk-20251002.md` L56-94 (Risk assessment)
- `docs/qa/assessments/4.2-po-validate-20251002.md` L294-326 (PO acceptance)

---

## Problem Statement

**Current State (Story 4.2 MVP)**:
- Analytics endpoint `GET /api/v1/admin/analytics` aggrega dati da in-memory stores
- Aggregations: query counts, feedback ratios, latency percentiles
- Data sources: `chat_messages_store`, `feedback_store`, `ag_latency_samples_ms`
- **Limitation**: Dati volatili, persi al restart container/deployment

**Desired State (Story 4.2.1)**:
- Eventi analytics (query, feedback, latency) persistiti in Supabase table `analytics_events`
- Aggregazioni calcolate su dati storici con query SQL
- Retention policy: 90 giorni storico (configurable)
- Backward compatibility: Story 4.2 dashboard funziona senza modifiche frontend

---

## Acceptance Criteria

1. **Database Schema**: Table `analytics_events` creata in Supabase con campi: `id`, `event_type`, `session_id_hash`, `query_text`, `feedback_vote`, `latency_ms`, `timestamp`, `metadata`

2. **Event Capture**: Eventi analytics registrati automaticamente:
   - Ogni query utente → insert evento `type='query'`
   - Ogni feedback thumbs up/down → insert evento `type='feedback'`
   - Ogni AG response → insert evento `type='latency'` con timing

3. **Batch Insert Strategy**: Eventi accumulati in buffer in-memory, insert batch ogni 5 minuti (configurable) per ridurre overhead DB

4. **Aggregation Refactoring**: Endpoint `/api/v1/admin/analytics` refactored per query Supabase:
   - Top queries: `SELECT query_text, COUNT(*) FROM analytics_events WHERE type='query' GROUP BY query_text ORDER BY COUNT(*) DESC LIMIT 10`
   - Feedback ratio: Aggregazione su eventi `type='feedback'`
   - Performance p95/p99: Percentile calculation su eventi `type='latency'`

5. **Retention Policy**: Scheduled job (Supabase cron o backend scheduler) elimina eventi > 90 giorni automaticamente

6. **Backward Compatibility**: Dashboard frontend (Story 4.2) funziona senza modifiche, response contract identico

7. **Migration Path**: Script migrazione popola `analytics_events` con dati in-memory correnti (se presenti al momento deploy)

8. **Performance**: Aggregation query < 500ms (p95) con dataset 10,000 eventi (same target Story 4.2)

9. **Privacy**: `session_id` hashato SHA256 prima di insert (consistente con Story 4.2 AC7)

10. **Monitoring**: Log insert batch success/failure, alert se batch failing > 3 consecutive attempts

---

## Technical Implementation

### Database Schema

**File**: `supabase/migrations/20251003000000_create_analytics_events.sql`

```sql
-- Analytics Events Table
CREATE TABLE IF NOT EXISTS analytics_events (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    event_type TEXT NOT NULL CHECK (event_type IN ('query', 'feedback', 'latency')),
    session_id_hash TEXT NOT NULL,  -- SHA256 hash, NOT raw session_id
    query_text TEXT,  -- For type='query'
    feedback_vote TEXT CHECK (feedback_vote IN ('up', 'down', NULL)),  -- For type='feedback'
    latency_ms INTEGER,  -- For type='latency'
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    metadata JSONB,  -- Extensible for future fields
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Indexes for performance
CREATE INDEX idx_analytics_events_type ON analytics_events(event_type);
CREATE INDEX idx_analytics_events_timestamp ON analytics_events(timestamp DESC);
CREATE INDEX idx_analytics_events_session ON analytics_events(session_id_hash);
CREATE INDEX idx_analytics_events_query_text ON analytics_events(query_text) WHERE event_type = 'query';

-- Row Level Security (RLS)
ALTER TABLE analytics_events ENABLE ROW LEVEL SECURITY;

-- Policy: Admin read-only (service_role has full access)
CREATE POLICY "Admin can read analytics events"
    ON analytics_events
    FOR SELECT
    USING (auth.jwt() ->> 'role' = 'admin');

-- Retention policy: Auto-delete events > 90 days
-- Scheduled via Supabase cron or backend job
CREATE OR REPLACE FUNCTION delete_old_analytics_events()
RETURNS void AS $$
BEGIN
    DELETE FROM analytics_events 
    WHERE timestamp < NOW() - INTERVAL '90 days';
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

---

### Backend Implementation

**File da Modificare/Creare**:
- `apps/api/api/analytics/analytics.py` (refactor aggregation logic)
- `apps/api/api/analytics/event_logger.py` (nuovo modulo batch insert)
- `apps/api/api/chat/routes.py` (add event logging on query/response)
- `apps/api/api/feedback/routes.py` (add event logging on feedback)

---

#### Event Logger Module

**File**: `apps/api/api/analytics/event_logger.py`

```python
from typing import Literal
from datetime import datetime
import asyncio
from collections import deque
from supabase import create_client, Client
import os
import hashlib

# Config
BATCH_SIZE = 100
BATCH_INTERVAL_SECONDS = 300  # 5 minutes
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

# In-memory buffer
event_buffer: deque = deque(maxlen=1000)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)


def hash_session_id(session_id: str) -> str:
    """SHA256 hash session_id for privacy."""
    return hashlib.sha256(session_id.encode()).hexdigest()[:16]


async def log_analytics_event(
    event_type: Literal["query", "feedback", "latency"],
    session_id: str,
    query_text: str | None = None,
    feedback_vote: Literal["up", "down"] | None = None,
    latency_ms: int | None = None,
    metadata: dict | None = None
):
    """Add event to buffer for batch insert."""
    event = {
        "event_type": event_type,
        "session_id_hash": hash_session_id(session_id),
        "query_text": query_text,
        "feedback_vote": feedback_vote,
        "latency_ms": latency_ms,
        "timestamp": datetime.utcnow().isoformat(),
        "metadata": metadata or {}
    }
    event_buffer.append(event)
    
    # Trigger batch insert if buffer full
    if len(event_buffer) >= BATCH_SIZE:
        await flush_event_buffer()


async def flush_event_buffer():
    """Batch insert events to Supabase."""
    if not event_buffer:
        return
    
    events_to_insert = list(event_buffer)
    event_buffer.clear()
    
    try:
        result = supabase.table("analytics_events").insert(events_to_insert).execute()
        print(f"[Analytics] Flushed {len(events_to_insert)} events to Supabase")
    except Exception as e:
        print(f"[Analytics] ERROR flushing events: {e}")
        # Re-add to buffer for retry (with limit to prevent infinite growth)
        for event in events_to_insert[:100]:
            event_buffer.append(event)


async def start_batch_flush_scheduler():
    """Background task: flush buffer every N seconds."""
    while True:
        await asyncio.sleep(BATCH_INTERVAL_SECONDS)
        await flush_event_buffer()
```

---

#### Integration Points

**File**: `apps/api/api/chat/routes.py` (modify existing)

```python
from api.analytics.event_logger import log_analytics_event

@router.post("/sessions/{session_id}/messages")
async def create_message(
    session_id: str,
    request: ChatRequest,
    # ... existing params
):
    # Existing AG logic...
    response = await generate_response(request.content)
    
    # NEW: Log analytics events
    await log_analytics_event(
        event_type="query",
        session_id=session_id,
        query_text=request.content
    )
    
    await log_analytics_event(
        event_type="latency",
        session_id=session_id,
        latency_ms=response.generation_time_ms
    )
    
    return response
```

**File**: `apps/api/api/feedback/routes.py` (modify existing)

```python
from api.analytics.event_logger import log_analytics_event

@router.post("/messages/{message_id}/feedback")
async def submit_feedback(
    message_id: str,
    feedback: FeedbackRequest,
    # ... existing params
):
    # Existing feedback storage...
    
    # NEW: Log analytics event
    await log_analytics_event(
        event_type="feedback",
        session_id=feedback.session_id,
        feedback_vote=feedback.vote
    )
    
    return {"ok": True}
```

---

#### Aggregation Refactoring

**File**: `apps/api/api/analytics/analytics.py` (refactor existing)

```python
from supabase import create_client, Client
import os

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_ROLE_KEY")
)


async def get_top_queries(limit: int = 10) -> list[QueryStat]:
    """Query top queries from Supabase."""
    result = supabase.rpc(
        'get_top_queries',
        {'query_limit': limit}
    ).execute()
    
    return [
        QueryStat(
            query_text=row['query_text'],
            count=row['count'],
            last_queried_at=row['last_queried_at']
        )
        for row in result.data
    ]


async def get_feedback_summary() -> FeedbackSummary:
    """Aggregate feedback from Supabase."""
    result = supabase.table("analytics_events") \
        .select("feedback_vote") \
        .eq("event_type", "feedback") \
        .execute()
    
    votes = [row['feedback_vote'] for row in result.data]
    up = votes.count('up')
    down = votes.count('down')
    ratio = up / (up + down) if (up + down) > 0 else 0.0
    
    return FeedbackSummary(
        thumbs_up=up,
        thumbs_down=down,
        ratio=ratio
    )


async def get_performance_metrics() -> PerformanceMetrics:
    """Calculate p95/p99 from Supabase."""
    result = supabase.table("analytics_events") \
        .select("latency_ms") \
        .eq("event_type", "latency") \
        .order("latency_ms", desc=False) \
        .execute()
    
    latencies = [row['latency_ms'] for row in result.data]
    sample_count = len(latencies)
    
    if sample_count == 0:
        return PerformanceMetrics(latency_p95_ms=0, latency_p99_ms=0, sample_count=0)
    
    p95_idx = int(sample_count * 0.95)
    p99_idx = int(sample_count * 0.99)
    
    return PerformanceMetrics(
        latency_p95_ms=latencies[p95_idx],
        latency_p99_ms=latencies[p99_idx],
        sample_count=sample_count
    )
```

**Supabase Function** (for optimized top queries):

```sql
-- File: supabase/migrations/20251003000001_analytics_functions.sql

CREATE OR REPLACE FUNCTION get_top_queries(query_limit INTEGER DEFAULT 10)
RETURNS TABLE (
    query_text TEXT,
    count BIGINT,
    last_queried_at TIMESTAMPTZ
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        ae.query_text,
        COUNT(*) as count,
        MAX(ae.timestamp) as last_queried_at
    FROM analytics_events ae
    WHERE ae.event_type = 'query'
        AND ae.query_text IS NOT NULL
    GROUP BY ae.query_text
    ORDER BY count DESC
    LIMIT query_limit;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

---

### Migration Strategy

**File**: `apps/api/scripts/migrate_analytics_to_supabase.py`

```python
"""
One-time migration: Populate analytics_events from in-memory stores.
Run once during Story 4.2.1 deployment.
"""

from api.analytics.event_logger import supabase, hash_session_id
from api.chat.stores import chat_messages_store, feedback_store, ag_latency_samples_ms
from datetime import datetime, timedelta

def migrate_existing_data():
    """Migrate in-memory data to Supabase (one-time)."""
    events = []
    
    # Migrate queries
    for session_id, messages in chat_messages_store.items():
        for msg in messages:
            if msg.get("role") == "user":
                events.append({
                    "event_type": "query",
                    "session_id_hash": hash_session_id(session_id),
                    "query_text": msg["content"],
                    "timestamp": msg.get("timestamp", datetime.utcnow().isoformat())
                })
    
    # Migrate feedback
    for feedback_id, feedback in feedback_store.items():
        events.append({
            "event_type": "feedback",
            "session_id_hash": hash_session_id(feedback["session_id"]),
            "feedback_vote": feedback["vote"],
            "timestamp": feedback.get("timestamp", datetime.utcnow().isoformat())
        })
    
    # Migrate latency samples (synthetic timestamps for demo)
    base_time = datetime.utcnow()
    for i, latency_ms in enumerate(ag_latency_samples_ms):
        events.append({
            "event_type": "latency",
            "session_id_hash": "migration_batch",
            "latency_ms": latency_ms,
            "timestamp": (base_time - timedelta(minutes=i)).isoformat()
        })
    
    # Batch insert
    if events:
        result = supabase.table("analytics_events").insert(events).execute()
        print(f"Migrated {len(events)} analytics events to Supabase")
    else:
        print("No in-memory data to migrate")

if __name__ == "__main__":
    migrate_existing_data()
```

---

## Dependencies

**Prerequisiti**:
- ✅ Story 4.2 (Analytics Dashboard) - Done (provides dashboard UI consuming analytics endpoint)
- ✅ Supabase project attivo con service_role_key configurato
- ✅ Supabase migrations infra (folder `supabase/migrations/` già esistente)

**Dipendenze Esterne**:
- ✅ Supabase Python Client già installato (dependency Story 1.x, 2.x)
- ✅ PostgreSQL 14+ (Supabase standard)

---

## Out of Scope

- **Real-time analytics**: No WebSocket/SSE updates, dashboard mantiene refresh manuale (Story 4.2 pattern)
- **Advanced querying**: No custom date range filters in UI (future Story 4.2.3)
- **Data export**: No CSV/PDF export (future Story 4.2.2)
- **Multi-tenancy**: Single project analytics, no per-course separation (future enhancement)
- **Anomaly detection**: No ML-based alerting su anomalie pattern (future Epic 5)

---

## Risks

| ID | Description | Probability | Impact | Mitigation |
|----|-------------|-------------|--------|------------|
| R-4.2.1-1 | Migration script data loss se in-memory stores grandi | Bassa | Alto | Batch insert con error handling; backup in-memory stores pre-migration |
| R-4.2.1-2 | Supabase quota limits su insert rate | Media | Medio | Batch insert ogni 5 min (max ~300 events/batch); monitor quota usage |
| R-4.2.1-3 | Aggregation query slow con dataset > 10k eventi | Media | Medio | Indexes ottimizzati; materialized views se necessario (Phase 3) |
| R-4.2.1-4 | Background flush scheduler memory leak | Bassa | Medio | Bounded buffer (maxlen=1000); monitoring memory usage |
| R-4.2.1-5 | Retention policy job failure accumula eventi old | Bassa | Basso | Scheduled job monitoring; manual cleanup script fallback |

---

## Testing Strategy

### Unit Tests (Backend)

**File**: `apps/api/tests/test_analytics_persistence.py`

**Test Cases**: 8 test cases
1. Event logger: log_analytics_event aggiunge a buffer correttamente
2. Event logger: hash_session_id produce hash deterministic
3. Flush buffer: batch insert success con 10 eventi mock
4. Flush buffer: error handling retry logic
5. Aggregation: get_top_queries query Supabase correttamente
6. Aggregation: get_feedback_summary calcola ratio corretto
7. Aggregation: get_performance_metrics calcola p95/p99 corretto
8. Migration script: populate events from in-memory stores

**Coverage Target**: ≥85%

---

### Integration Tests

**File**: `apps/api/tests/test_analytics_integration.py`

**Test Cases**: 5 test cases
1. End-to-end: POST query → evento logged → flush → SELECT query eventi presenti
2. End-to-end: POST feedback → evento logged → aggregation feedback ratio aggiornato
3. Performance: Aggregation query < 500ms con 10,000 eventi in Supabase
4. Retention: delete_old_analytics_events elimina eventi > 90 giorni
5. Backward compatibility: Endpoint `/api/v1/admin/analytics` response identico pre/post Story 4.2.1

**Coverage Target**: ≥80%

---

### E2E Tests

**File**: `apps/web/tests/story-4.2.1.spec.ts`

**Scenarios**: 3 test cases (focus su backward compatibility)
1. Admin dashboard analytics: data display funziona identico a Story 4.2 (no UI changes)
2. Refresh dashboard: nuovi dati persistiti visibili dopo server restart (new capability)
3. Historical data: query eseguita ieri visibile in top queries oggi (trend analysis)

**Duration Target**: < 15 secondi totali

---

## Success Metrics

- **Data Retention**: ✅ 100% eventi analytics persistiti (zero data loss post-deploy)
- **Performance**: Aggregation query < 500ms (p95) con 10,000 eventi
- **Adoption**: PO utilizza trend analysis per identificare ≥2 pattern temporali entro 2 settimane deploy
- **System Health**: Zero batch insert failures per 7 giorni consecutivi

---

## Implementation Notes

### Fase 1: Database Setup (Giorno 1)

**Prerequisiti**:
1. [ ] Creare migration file `20251003000000_create_analytics_events.sql`
2. [ ] Applicare migration su Supabase dev environment
3. [ ] Verificare indexes creati correttamente
4. [ ] Test RLS policies con admin JWT

**Implementation**:
1. Migration SQL con table + indexes + RLS
2. Test insert manuale via Supabase SQL editor
3. Verificare performance SELECT con dataset mock 1000 eventi

**Acceptance**: Table `analytics_events` creata, indexes funzionanti, RLS testato

---

### Fase 2: Event Logger Module (Giorno 2-3)

**Prerequisiti**:
1. [ ] Installare/verificare `supabase-py` client
2. [ ] Configurare env vars `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY`

**Implementation**:
1. Creare `event_logger.py` con buffer + batch insert
2. Implementare `log_analytics_event()` function
3. Implementare `flush_event_buffer()` con error handling
4. Background scheduler con asyncio
5. Unit tests event logger (8 test cases)

**Acceptance**: Unit tests PASS, batch insert funziona su Supabase dev

---

### Fase 3: Integration Points (Giorno 4)

**Prerequisiti**:
1. [ ] Event logger module testato e funzionante
2. [ ] Identificare tutti call sites: chat routes, feedback routes

**Implementation**:
1. Modify `apps/api/api/chat/routes.py`: add `log_analytics_event()` calls
2. Modify `apps/api/api/feedback/routes.py`: add `log_analytics_event()` calls
3. Start background scheduler on app startup
4. Integration tests (5 test cases)

**Acceptance**: Eventi logged correttamente, integration tests PASS

---

### Fase 4: Aggregation Refactoring (Giorno 5-6)

**Prerequisiti**:
1. [ ] Dataset mock 1000+ eventi in Supabase dev
2. [ ] Supabase functions deployed (`get_top_queries`)

**Implementation**:
1. Refactor `analytics.py`: sostituire in-memory aggregation con Supabase queries
2. Implementare `get_top_queries()`, `get_feedback_summary()`, `get_performance_metrics()`
3. Benchmark performance query aggregation
4. Unit tests aggregation refactored (6 test cases Story 4.2)

**Acceptance**: Endpoint `/api/v1/admin/analytics` funziona con dati Supabase, performance < 500ms

---

### Fase 5: Migration & Testing (Giorno 7-8)

**Prerequisiti**:
1. [ ] Backend aggregation refactoring completato
2. [ ] E2E environment preparato

**Implementation**:
1. Implementare migration script `migrate_analytics_to_supabase.py`
2. Test migration su dev con dati in-memory mock
3. E2E tests backward compatibility (3 scenarios)
4. Regression tests Story 4.2 (10 scenarios re-run)

**Acceptance**: E2E tests PASS, dashboard funziona identico, zero regressioni

---

### Fase 6: Retention Policy & Monitoring (Giorno 9)

**Prerequisiti**:
1. [ ] Supabase cron jobs configurabili (o backend scheduler)

**Implementation**:
1. Scheduled job: `delete_old_analytics_events()` run daily
2. APM monitoring: batch insert success rate, aggregation latency
3. Alert setup: batch failures > 3 consecutive
4. Documentation update: `admin-setup-guide.md` section analytics persistence

**Acceptance**: Retention policy testato, monitoring attivo

---

## File Locations

- **Migration SQL**: `supabase/migrations/20251003000000_create_analytics_events.sql`
- **Supabase Functions**: `supabase/migrations/20251003000001_analytics_functions.sql`
- **Event Logger**: `apps/api/api/analytics/event_logger.py` (nuovo)
- **Aggregation Refactor**: `apps/api/api/analytics/analytics.py` (modifica)
- **Chat Integration**: `apps/api/api/chat/routes.py` (modifica)
- **Feedback Integration**: `apps/api/api/feedback/routes.py` (modifica)
- **Migration Script**: `apps/api/scripts/migrate_analytics_to_supabase.py` (nuovo)
- **Backend Tests**: `apps/api/tests/test_analytics_persistence.py` (nuovo)
- **Integration Tests**: `apps/api/tests/test_analytics_integration.py` (nuovo)
- **E2E Tests**: `apps/web/tests/story-4.2.1.spec.ts` (nuovo)
- **Documentation**: `docs/admin-setup-guide.md` (aggiornamento)

---

## Tasks / Subtasks

### Database Setup
- [ ] Creare migration `20251003000000_create_analytics_events.sql`
- [ ] Applicare migration su Supabase dev
- [ ] Creare Supabase function `get_top_queries`
- [ ] Verificare indexes e RLS policies
- [ ] Test manual insert/select via SQL editor

### Backend Implementation
- [ ] Implementare `event_logger.py` module
- [ ] Implementare `log_analytics_event()` function
- [ ] Implementare `flush_event_buffer()` batch insert
- [ ] Background scheduler startup
- [ ] Refactor `analytics.py` aggregation logic
- [ ] Integration in `chat/routes.py`
- [ ] Integration in `feedback/routes.py`
- [ ] Unit tests event logger (8 test case)
- [ ] Unit tests aggregation refactored (6 test case)
- [ ] Integration tests (5 test case)

### Migration & Testing
- [ ] Implementare migration script
- [ ] Test migration su dev environment
- [ ] E2E tests backward compatibility (3 scenarios)
- [ ] Regression tests Story 4.2 (10 scenarios re-run)
- [ ] Performance benchmark aggregation 10k eventi

### Monitoring & Documentation
- [ ] Scheduled job retention policy
- [ ] APM monitoring setup (batch insert, aggregation latency)
- [ ] Alert configuration (batch failures)
- [ ] Update `admin-setup-guide.md` section analytics
- [ ] Migration runbook documentation

---

## References

### Parent Story
- Story 4.2: `docs/stories/4.2.analytics-dashboard.md`

### Technical Debt Documentation
- Risk R-4.2-1: `docs/qa/assessments/4.2-risk-20251002.md` L56-94
- PO Acceptance: `docs/qa/assessments/4.2-po-validate-20251002.md` L294-326

### Architecture & Patterns
- Supabase RLS: `docs/architecture/sezione-10-sicurezza-e-performance.md`
- Database Migrations: `docs/architecture/sezione-7-struttura-unificata-del-progetto.md`
- Testing Strategy: `docs/architecture/sezione-11-strategia-di-testing.md`

### Related Stories
- Story 2.1 (Document Ingestion): Supabase usage patterns
- Story 3.2 (Augmented Generation): `chat_messages_store` source
- Story 3.4 (Feedback System): `feedback_store` source

---

## Change Log

| Date | Author | Change Description |
|------|--------|-------------------|
| 2025-10-02 | Product Owner | Story created — Technical Debt R-4.2-1 mitigation backlog ticket |
| 2025-10-02 | Product Owner | Defined acceptance criteria, schema design, batch insert strategy |
| 2025-10-02 | Product Owner | Linked to Story 4.2 validation conditional GO approval |

---

**Status**: 🔴 Backlog — High Priority (Technical Debt Mitigation)  
**Parent Story**: Story 4.2 (Analytics Dashboard) — Conditional GO Dependency  
**Created By**: Product Owner  
**Priority Justification**: Unlock full business value analytics dashboard (trend analysis, historical insights)  
**Target Sprint**: Post-MVP Sprint 3 (after Story 4.2 deployment + monitoring)  
**Estimated Start Date**: TBD (post Story 4.2 production validation)

---

## Appendix: Technical Debt Formalization

### Technical Debt Acceptance Record

**Debt ID**: TD-4.2-R-4.2-1  
**Created**: 2025-10-02  
**Accepted By**: Tech Lead + Product Owner  
**Acceptance Document**: `docs/qa/assessments/4.2-po-validate-20251002.md` L294-326

**Debt Description**: Analytics dashboard (Story 4.2) usa dati in-memory volatili, limitando analisi storica.

**Business Impact**:
- Dashboard utile solo per monitoring real-time sessione corrente
- No trend analysis temporali → reduced business value

**Mitigation Plan**:
- **Immediate** (Story 4.2 MVP): Dashboard funzionale con dati volatili, admin training su limitation
- **Short-term** (Story 4.2.1): Supabase persistence, unlock trend analysis
- **Long-term** (Phase 3): Materialized views, advanced analytics features

**Cost of Delay**:
- **Week 1-2**: Acceptable (MVP validation period)
- **Week 3-4**: Medium cost (admin frustration loss dati)
- **Month 2+**: High cost (business value dashboard significantly reduced)

**Monitoring**:
- Admin feedback survey Week 2 post Story 4.2 deploy
- PO usage analytics utilità dashboard volatile

**Repayment Trigger**: Story 4.2 production stable + admin feedback positive → prioritize Story 4.2.1 Sprint 3

---

**Technical Debt Status**: ✅ FORMALIZED  
**Mitigation Story**: Story 4.2.1 — This Document  
**Approval Date**: 2025-10-02  
**Review Date**: Post Story 4.2 Deploy + 2 Weeks

