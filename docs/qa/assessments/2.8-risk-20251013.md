# Risk Profile — Story 2.8: Ripristino Infrastruttura Esterna e P95 Reale

Data: 2025-10-13 (QA update: 2025-10-13 19:30 CET)  
Revisore: Quinn (Test Architect & Quality Advisor)

## Executive Summary

- Rischi attivi totali: 7
- Critici: 0, Alti: 3, Medi: 4, Bassi: 0
- Risk Score complessivo: 65/100 (High×10 + Medium×5)
- Principali esposizioni: gestione segreti OpenAI/Supabase nei pipeline runner, disponibilità dei servizi esterni durante gli E2E, affidabilità delle metriche P95 raccolte.

### Riepilogo per Gate (`risk_summary`)

```yaml
risk_summary:
  totals:
    critical: 0
    high: 3
    medium: 4
    low: 0
  highest:
    id: SEC-201
    score: 6
    title: "Secret sprawl o leakage per chiavi OpenAI/Supabase nei runner CI"
  recommendations:
    must_fix:
      - "Stabilire vault/secrets manager unico e audit trail per chiavi OpenAI/Supabase, evitando esposizione nei log CI"
      - "Garantire fallback/monitoring per downtime OpenAI e Supabase durante E2E, con retry e circuit breaker documentati"
      - "Validare pipeline E2E in staging con dati sintetici prima di raccogliere P95 reali, assicurando script p95 non falsati"
    monitor:
      - "Controllare quota/budget OpenAI dopo riattivazione test e stabilire alert su consumo"
      - "Monitorare job di sync Supabase e log p95 per spotting regressioni o report corrotti"
```

## Contesto aggiornato

- Story 2.7 ha completato hardening DB senza riattivare E2E e raccolta P95 reale; Story 2.8 riaccende i servizi esterni e pipeline.
- Configurazioni correnti in `.env` locale includono chiavi reali, ma il repository le ignora (`.gitignore`). Occorre consolidare gestione secrets nei runner CI.
- Script disponibili per health DB (`database_connectivity_test.py`, `database_integrity_audit.py`), per P95 (`p95_local_test.js`, `summarize_p95.py`), e test sync job (`test_sync_job_integration.py`). Dipendono da OpenAI/Supabase in stato sano.
- Reports precedenti (`docs/reports/rag-production-readiness-summary.md`, `reports/metrics-p95-20251013.md`) mostrano E2E/P95 bloccati da servizi esterni non disponibili.

## Risk Matrix

| ID | Categoria | Descrizione | Probabilità | Impatto | Score | Priorità | Stato |
|----|-----------|-------------|-------------|---------|-------|----------|-------|
| SEC-201 | Security | Secret sprawl/leakage delle chiavi OpenAI/Supabase nel CI o nei log dei test | Medium (2) | High (3) | 6 | High | Aperto |
| OPS-201 | Operational | Pipeline E2E dipende da OpenAI/Supabase che possono essere indisponibili/rate-limited durante run CI | Medium (2) | High (3) | 6 | High | Aperto |
| PERF-201 | Performance | Metriche P95 raccolte con dataset/ambiente non rappresentativo distorcono decisioni di go-live | Medium (2) | High (3) | 6 | High | Aperto |
| OPS-202 | Operational | Configurazione secrets test/staging non sincronizzata tra team → fallimenti ambienti e ritardi | Medium (2) | Medium (2) | 4 | Medium | Aperto |
| OPS-203 | Operational | Script di validazione (connectivity/integrity) non automatizzati → regressioni non detectate | Medium (2) | Medium (2) | 4 | Medium | Aperto |
| PERF-202 | Performance | Mismatch tra p95 raccolti da script vs dashboard Supabase genera dati incoerenti | Medium (2) | Medium (2) | 4 | Medium | Aperto |
| DATA-201 | Data | Log/report contenenti secrets o PII salvati in repo (`reports/*.log`) | Low-Medium (1.5) | Medium (2) | 3 | Medium | Aperto |

## Detailed Risk Register

### SEC-201 — Secret sprawl/leakage per chiavi OpenAI/Supabase
- **Evidence:** `.env` locale contiene chiavi attive; Story 2.8 richiede spostarle in CI secrets (`TEST_*`). Documentazione attuale (`apps/api/ENV_TEST_SETUP.md`) non impone vault centralizzato.
- **Probability:** Media — passaggio a CI introduce rischio di copia/incolla in log o repository.
- **Impact:** Alto — esposizione service role key o OpenAI key comporta compromissione costosa.
- **Mitigations:**
  - Utilizzare secret manager (GitHub Actions encrypted secrets o HashiCorp Vault) con rotation automatica.
  - Sanitizzare log pipeline; vietare `print(os.environ)`; usare placeholder quando si validano env.
  - Aggiornare runbook in `docs/operations/secrets-rotation.md` con processi di leak response.
- **Testing Focus:** Esecuzione dry-run pipeline con log review; verifica che i secrets non emergano.
- **Monitoring:** Audit su accesso secrets; alert su nuove variabili definite manualmente.

### OPS-201 — Downtime/rate limit servizi esterni durante pipeline
- **Evidence:** Story 2.7 mostrava errori 401/timeouts su p95 locale quando servizi esterni non disponibili (`reports/metrics-p95-20251013.md`). E2E riattivati dipendono da OpenAI API e Supabase.
- **Probability:** Media — i servizi possono limitare richieste test.
- **Impact:** Alto — blocca pipeline, impedisce release readiness.
- **Mitigations:**
  - Implementare retry/backoff e fallback (mock) per step non critici.
  - Schedulare finestre di esecuzione test entro quote disponibili; definire rate limit config.
  - Prevedere modalità “degraded” con skip soft e gating manuale.
- **Testing Focus:** Eseguire test in condizioni di quota ridotta; simulare downtime con circuit breaker.
- **Monitoring:** Alert su HTTP 429/5xx da OpenAI e health metrics Supabase.

### PERF-201 — Metriche P95 non rappresentative
- **Evidence:** Story 2.7 raccoglieva P95 dalla macchina locale con servizi mancanti; ora si pianifica misurazione “reale”.
- **Probability:** Media — se dataset/test data non replicano prod, i risultati sono fuorvianti.
- **Impact:** Alto — decisioni di capacity/performance sbagliate.
- **Mitigations:**
  - Definire dataset sintetico realistico e volume target.
  - Confrontare script P95 con dashboard Supabase e log APM; documentare gap.
  - Stabilire requisito min sample size (>N richieste) prima di presentare metriche.
- **Testing Focus:** Stress test con k6 e tracking delta vs baseline.
- **Monitoring:** Trend P95/P99; allarmi su deviazioni >20%.

### OPS-202 — Config secrets non sincronizzati
- **Evidence:** Il template `ENV_TEST_TEMPLATE.txt` richiede sostituzioni manuali; rischio di mismatch tra team/runner.
- **Probability:** Media — errori copy/paste frequenti.
- **Impact:** Medio — fallimenti run, tempi di diagnosi lunghi.
- **Mitigations:**
  - Centralizzare definizioni env in script/Ansible o secret manager.
  - Versionare file template con commenti chiari; aggiungere check di completamento.
- **Testing Focus:** Pipeline pre-flight che fallisce se placeholder non sostituiti.
- **Monitoring:** Log pipeline per variabili mancanti.

### OPS-203 — Script health non automatizzati
- **Evidence:** Story 2.8 pianifica run manuali per `database_connectivity_test.py` e `database_integrity_audit.py`.
- **Probability:** Media — senza automation, check possono essere saltati.
- **Impact:** Medio — degradi DB non rilevati finché non esplode in prod.
- **Mitigations:**
  - Integrare gli script in pipeline notturne con alert.
  - Salvare output versionato con timestamp e diff.
- **Testing Focus:** CI run programmata; assicurarsi exit code >0 on failure.
- **Monitoring:** Dashboard con ultimo esito health check.

### PERF-202 — Disallineamento metriche P95 script vs dashboard
- **Evidence:** Story chiede sia script locale sia dashboard Supabase; doppia fonte può divergere.
- **Probability:** Media — definizioni diverse (endpoint vs DB) e time window.
- **Impact:** Medio — report conflittuali generano sfiducia.
- **Mitigations:**
  - Definire schema di aggregazione unificato e finestra temporale.
  - Annotare in report l’origine dati; includere checksum/log.
- **Testing Focus:** Confronto manuale e automatizzato (diff) tra fonti.
- **Monitoring:** Alert su differenze >10% tra fonti.

### DATA-201 — Log report con secrets/PII
- **Evidence:** Deliverables includono `reports/db_connectivity_test_YYYYMMDD.log`; necessario sanificare output.
- **Probability:** 1.5 (tra bassa e media) — script potrebbero stampare URL con key.
- **Impact:** Medio — leak dei log condivisi.
- **Mitigations:**
  - Redigere linee guida per redaction; usare strumenti per mascherare.
  - Archiviare log sensibili fuori repo o cifrati.
- **Testing Focus:** Review manuale log; unit test per scrubber.
- **Monitoring:** Scan sicurezza (trufflehog) su repo/report.

## Prioritized Recommendations

**Priorità 1 — Must fix (High risks)**
- Mettere in produzione secret management centralizzato + rotazione documentata per chiavi OpenAI/Supabase.
- Stabilire piano di continuità per test E2E (mock/fallback) e monitorare quota servizi esterni.
- Validare qualità delle metriche P95 con dataset rappresentativo e definire soglie minime di campionamento.

**Priorità 2 — Medium risks**
- Automatizzare script health check (Supabase connectivity/integrity) e diff dei risultati.
- Aggiornare template/env checklist per prevenire mismatch; implementare pre-flight check.
- Allineare pipeline di raccolta P95 con dashboard Supabase, usando etichette e time window condivisa.
- Sanitizzare report e log prima di archiviarli (redaction automatica, storage sicuro).

**Priorità 3 — Continuous controls**
- Abilitare alert su consumo quota OpenAI e sugli errori 429/5xx.
- Eseguire audit trimestrale su secrets: chi accede, quando ruotano, come si revocano.
- Integrare risk review nei meeting di readiness e aggiornare `docs/reports/rag-production-readiness-summary.md` con stato E2E/P95.

## Monitoring & Telemetry Requirements

- Dashboard centralizzata con: quota OpenAI, latency/errore Supabase, stato job sync, esiti E2E.
- Alerting su fallimento pipeline E2E, su output health check “FAIL”, e su differenze P95 >20% tra fonti.
- Logging sanificato; scanner (trufflehog/ggshield) run settimanale sui report.

## Residual Risk Score & Gate Recommendation

- **Story Risk Score:** 65/100 (3 High ×10 + 4 Medium ×5).
- **Gate Recommendation:** CONCERNS — mitigare i tre rischi High o predisporre controlli compensativi prima di marcare la story “Ready for Review”.

## Hook Reference

```
Risk profile: docs/qa/assessments/2.8-risk-20251013.md
```

