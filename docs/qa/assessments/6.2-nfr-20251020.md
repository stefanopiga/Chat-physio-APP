# NFR Assessment – Story 6.2 (Watcher Configuration, Diagnostics & Stabilization)

Date: 2025-10-20
Story: docs/stories/6.2.watcher-configuration-diagnostics-and-stabilization.md
Scope: reliability, performance, observability, security, configurability, maintainability, testability

## Context & Inputs
- Story status: Ready for Done; all ACs checked in story file.
- QA config: .bmad-core/core-config.yaml (qa.qaLocation: docs/qa)
- Implemented scope highlights:
  - Deterministic LLM binding via Settings; timeout validation per env.
  - Redis cache health-check with graceful degradation and fast timeouts.
  - Precedence and diagnostics for `.env` vs process env; redacted settings printer.
  - Single-file diagnostic runner with JSON report; ops debug scripts (Win/Unix).
  - Tests: config precedence, Redis graceful, classifier binding, diag runner; coverage claims >=85% on target modules (noted 95% in story notes).

## Targets (found/missing)
- Reliability: Graceful Redis handling, timeout validation, diag runner, extensive tests (found).
- Performance: Classification timeout default 20s; no explicit throughput/latency SLOs; no perf budget on diag runner (partial).
- Observability: Structured logs, debug scripts, diag JSON reports, settings printer with redaction (found).
- Security: Secrets redaction in settings printer; potential exposure risk via logs if env values logged without masking (partial risk; mitigated by redaction policy if consistently applied).
- Configurability: Clear precedence (Settings → env → defaults); paths resolved and created with source logging (found).
- Maintainability: Focused modules with tests and isolation utility (`reset_settings()`); docs updated (found).
- Testability: Autouse fixture for settings reset; deterministic mocks; E2E diag test present (found).

## Assessment by NFR
- Reliability – PASS
  - Positives: Redis unavailability does not block; fast failure with warning; validated timeout floors by env; diag runner provides reproducible checks; unit+E2E tests cover critical flows.
  - Notes: Consider adding periodic self-check cron for watcher health and alerting hooks.

- Performance – CONCERNS
  - Positives: Timeouts are configurable and validated; diag runner surfaces timing.
  - Gaps: No explicit SLOs for classification duration or watcher end-to-end latency; no load/perf tests for ingestion burst scenarios; cache hit-rate targets undefined.

- Observability – PASS
  - Positives: Ops scripts collect Docker/Redis logs; diag JSON snapshots; structured logging for classifier init and config precedence; troubleshooting runbook updated.
  - Notes: Ensure log fields are consistent and easily queriable (event names, correlation IDs).

- Security – PASS (advisory)
  - Positives: Settings printer redacts secrets; minimal diagnostic surface in production; Redis optional.
  - Risks: Avoid printing raw URLs with credentials (e.g., `REDIS_URL`) in logs or debug outputs; confirm `.env` access limited and not baked into images; consider CI secret scan.

- Configurability – PASS
  - Positives: Settings precedence clarified; ingestion paths sourced from Settings with mkdir and provenance logs; environment-specific guidance provided.

- Maintainability – PASS
  - Positives: Small focused modules for diagnostics/config; tests isolate Settings cache via `reset_settings()`; docs and scripts align with code.
  - Notes: Keep troubleshooting docs in sync as flags evolve.

- Testability – PASS
  - Positives: Unit suites for precedence, Redis, classifier, diag; E2E diag; autouse fixture ensures deterministic env.
  - Notes: Add smoke test for ops scripts to validate archive structure, if CI permits.

## Deterministic Result
- Statuses: Reliability=PASS, Performance=CONCERNS, Observability=PASS, Security=PASS, Configurability=PASS, Maintainability=PASS, Testability=PASS
- Quality score (advisory): 100 - (10*1 CONCERNS) = 90

## Gate YAML (paste‑ready)
nfr_validation:
  _assessed: [reliability, performance, observability, security, configurability, maintainability, testability]
  reliability:
    status: PASS
    notes: "Graceful Redis; timeout floors; diag runner; tests cover critical flows"
  performance:
    status: CONCERNS
    notes: "No explicit SLOs/load tests; define p95 targets and burst ingestion checks"
  observability:
    status: PASS
    notes: "Structured logs, ops scripts, diag JSON, troubleshooting runbook"
  security:
    status: PASS
    notes: "Secrets redaction in settings; ensure no raw credential logs; enable CI secret scan"
  configurability:
    status: PASS
    notes: "Settings→env→defaults precedence; path provisioning with provenance logs"
  maintainability:
    status: PASS
    notes: "Modular changes; isolation utilities; docs updated"
  testability:
    status: PASS
    notes: "Autouse settings reset; unit+E2E diag tests"

## Recommendations
- Define performance SLOs for watcher/classification (e.g., p95 classification ≤ 8s dev, ≤ 5s prod; watcher end-to-end ≤ 30s for typical PDFs) and add scripted load tests.
- Add lightweight smoke test for `scripts/ops/watcher-debug.*` to assert archive layout and key sections.
- Ensure all logs avoid leaking secrets (mask `REDIS_URL` credentials) and include correlation IDs for multi-step ingestion traces.
- Consider periodic watcher health check with alerting (e.g., daily diag run on a sample file in staging).
- Track and report cache hit-rate for classification cache to inform timeout and cost tuning.

## Evidence Pointers
- Story: docs/stories/6.2.watcher-configuration-diagnostics-and-stabilization.md
- Tests: apps/api/tests/test_watcher_config.py, apps/api/tests/test_redis_graceful.py, apps/api/tests/test_classifier_binding.py, apps/api/tests/test_watcher_diag.py
- Code: apps/api/api/config.py, apps/api/api/ingestion/config.py, apps/api/api/knowledge_base/classifier.py, apps/api/api/diagnostics/redis_check.py, apps/api/api/ingestion/run_diag.py
- Docs: docs/operations/monitoring.md (Troubleshooting Watcher)

--
Generated by QA NFR assessment for Story 6.2.

