# NFR Assessment: 6.5

Date: 2025-10-20
Reviewer: Quinn
Story: docs/stories/6.5.llm-generation-fix-e2e-validation.md
Config: .bmad-core/core-config.yaml (qa.qaLocation: docs/qa)

## Summary

- Security: CONCERNS – Ensure no secrets in logs; rate limiting OK on admin, verify generation endpoint auth and input validation; enforce RUN_E2E_REAL gating to avoid accidental external calls.
- Performance: CONCERNS – Targets implied (p95 < 2000ms retrieval per 6.4 note, generation latency tracked locally). Need explicit threshold for generation endpoint and token/latency metrics per AC5.
- Reliability: CONCERNS – Retry/backoff policy defined in story; not yet implemented in tests or code paths for generation. Add 429 handling, timeouts, and idempotent cleanup in E2E.
- Maintainability: CONCERNS – E2E test not yet present; markers and reporting scaffolding needed; ensure pytest marker registration and CI gating.

## Evidence Pointers

- Chat LLM init: apps/api/api/services/chat_service.py:112
- Admin router: apps/api/api/routers/admin.py:1
- Classifier LLM (nano default temp): apps/api/api/knowledge_base/classifier.py:45
- Test infra and E2E guidelines: apps/api/tests/README_E2E_TESTS.md:227

## Critical Issues

1) Missing nano temperature guard in chat LLM
- Risk: Hard failure with gpt-5-nano if temperature=0 injected via env.
- Fix: In get_llm(), skip temperature override for models containing "nano" and log decision.

2) Retry/backoff not enforced on generation path
- Risk: Flaky E2E on transient OpenAI errors; inconsistent CI results.
- Fix: Implement retry with exponential backoff, special 429 handling (60s wait without counting toward retries), and test coverage.

## Quick Wins

- Add nano guard in get_llm() with structured logging (≤1h).
- Implement simple retry/backoff wrapper for generation path and assert in tests (2–3h).
- Wire minimal metrics writers for AC5 (latency, tokens, cost) to reports/e2e/* (1–2h).

## Suggested Targets

- Generation endpoint p95 latency: ≤ 2.5s with 5 chunks average.
- Success rate: ≥ 98% for non-rate-limited runs.
- Token metrics: record prompt, completion, and total; daily cost budget alert at $100.

## Gate YAML (copy/paste)

```yaml
nfr_validation:
  _assessed: [security, performance, reliability, maintainability]
  security:
    status: CONCERNS
    notes: >-
      Ensure auth on generation endpoint; avoid logging secrets; enforce RUN_E2E_REAL gating;
      admin rate limiting present via SlowAPI; validate input sanitization for messages API.
  performance:
    status: CONCERNS
    notes: >-
      Define explicit generation p95 target; implement AC5 metrics (latency/tokens/cost);
      retrieval perf baseline referenced in 6.4; add simple timers around generation.
  reliability:
    status: CONCERNS
    notes: >-
      Implement retry/backoff for OpenAI (2 retries, 1/2/4s; 60s wait on 429 not counting);
      add timeouts and structured error logs; ensure E2E cleans test data.
  maintainability:
    status: CONCERNS
    notes: >-
      Add e2e test with @pytest.mark.e2e and RUN_E2E_REAL gate; register markers;
      write reports to reports/e2e/; document how to run and skip in CI.
```

NFR assessment: docs/qa/assessments/6.5-nfr-20251020.md

Gate NFR block ready → paste into qa.qaLocation/gates/6.5-llm-generation-fix-e2e-validation.yml under nfr_validation

