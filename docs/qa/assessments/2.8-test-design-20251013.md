# Test Design — Story 2.8: Ripristino Infrastruttura Esterna e P95 Reale

Data: 2025-10-13  
Autore: Quinn (Test Architect & Quality Advisor)

## 1. Executive Overview

- **Obiettivo Story:** riabilitare OpenAI + Supabase in ambiente test/staging, riaccendere pipeline E2E e raccogliere metriche P95 reali.  
- **Goal QA:** garantire che i flussi critici funzionino end-to-end con servizi esterni reali, che i secrets siano gestiti in modo sicuro e che le metriche P95 siano affidabili.
- **Test Focus:** Secret management, health Supabase, integrazione OpenAI, esecuzione pipeline E2E, raccolta/analisi P95.

## 2. Scope & Assunzioni

**In scope**
- Validazione configurazione secrets in CI/staging (`TEST_*` vars).  
- Health check Supabase (connectivity, integrity, job sync).  
- Verifica API OpenAI e fallback in caso di quota esaurita.  
- Pipeline E2E (chat, knowledge base ingest, sync job).  
- Raccolta metriche P95 via script (`p95_local_test.js`, `summarize_p95.py`) e confronto con dashboard Supabase.

**Out of scope**
- Upgrade plan Postgres o modifiche schema DB (coperti da Story 2.7).  
- Load test massivo multi-utente (>N paralleli) oltre ai requisiti p95 baseline.  
- Test security approfonditi (pentest) su secrets manager.

**Assunzioni**
- Ambiente test/staging isolato con accesso a Supabase e OpenAI (quota adeguata).  
- Runner CI con rete uscente verso servizi esterni.  
- Dataset sintetico caricato in Supabase compatibile con pipeline E2E.

## 3. Test Strategy

| Livello | Scopo | Approccio | Owner |
|--------|-------|-----------|-------|
| Unit | Mascheramento log, fallback client OpenAI | Aggiornare/aggiungere unit con mocking | Dev |
| Integration | Health script Supabase, OpenAI availability check | Esecuzione `database_connectivity_test.py`, test sync job, curl OpenAI | QA/Dev |
| E2E | Chat flow con servizi esterni reali + ingest KB | Pipeline CI (Playwright/pytest) | QA |
| Performance Spot | P95 su endpoint chat / ingest | Script k6 (`p95_local_test.js`) + `summarize_p95.py` | Perf Guild |
| Monitoring Validation | Dashboard Supabase, alert rate limit | Review manuale + screenshot | QA |

## 4. Test Inventory & Traceability

| ID | Requirement / AC | Test artefact | Note |
|----|------------------|---------------|------|
| TD-OPENAI-01 | AC1.1/AC1.2 | Script `curl /models` + CI step `check-openai-key` | Fallisce se quota/chiave non valida. |
| TD-OPENAI-02 | AC1.3 | Budget screenshot audit | Manuale; allegare in deliverable. |
| TD-SUPA-01 | AC2.1 | `database_connectivity_test.py` | Log `reports/db_connectivity_test_YYYYMMDD.log`. |
| TD-SUPA-02 | AC2.2 | `test_sync_job_integration.py` | Riabilitare in CI; valutare fixture per staging. |
| TD-SUPA-03 | AC2.3 | Health check job sync (SQL + dashboard) | Automatizzare query `pg_stat_activity`. |
| TD-E2E-01 | AC3.1 | Workflow CI `ci-e2e.yml` | Verifica variabili `TEST_*`. |
| TD-E2E-02 | AC3.2 | Playwright/pytest E2E run | Output JUnit/HTML archiviato. |
| TD-P95-01 | AC3.3 | `p95_local_test.js` + `summarize_p95.py` | Verificare >N richieste, log output. |
| TD-P95-02 | AC3.3 | Dashboard Supabase screenshot | Confronto script vs dashboard. |
| TD-SEC-01 | AC4.1 | Secret scan (trufflehog) + log review | No secret leakage in repo/log. |
| TD-SEC-02 | AC4.2 | Checklist rotazione secrets | Confermare owner e cadenzario. |

## 5. Test Scenarios (Gherkin)

### Scenario 1 — Validazione chiavi OpenAI in CI
```
Given il runner CI ha caricato i secrets TEST_OPENAI_API_KEY
When eseguo il job "validate-openai"
Then la chiamata a https://api.openai.com/v1/models risponde 200
And il log non contiene la key in chiaro
```

### Scenario 2 — Supabase connectivity & permessi
```
Given i secrets TEST_SUPABASE_URL e TEST_SUPABASE_SERVICE_KEY sono configurati
When eseguo scripts/validation/database_connectivity_test.py
Then lo script termina con exit code 0
And il log riportato in reports/db_connectivity_test_YYYYMMDD.log mostra "All checks PASS"
```

### Scenario 3 — Sync job integration
```
Given la pipeline CI dispone delle credenziali Supabase valide
And il dataset staging contiene job di sync schedulati
When eseguo pytest -k "test_sync_job_integration"
Then tutti i test passano senza skip
And il log mostra durata e ID job verificati
```

### Scenario 4 — E2E chat con servizi reali
```
Given OpenAI e Supabase sono raggiungibili
And le variabili TEST_* sono esportate nel workflow E2E
When eseguo la suite E2E "chat end-to-end"
Then la conversazione produce risposte con status 200
And nessun fallback mock viene utilizzato
```

### Scenario 5 — Raccolta P95 affidabile
```
Given l'ambiente staging è in stato stabile
When lancio scripts/perf/p95_local_test.js con 500 request
And processa i risultati con summarize_p95.py
Then il report reports/metrics-p95-YYYYMMDD.md contiene p95 <= soglia definita
And screenshot dashboard Supabase mostra un valore coerente (±10%)
```

### Scenario 6 — Secret hygiene nei log
```
Given i job CI archiviano log test e health check
When analizzo i file reports/*.log
Then non trovo stringhe che combaciano con SUPABASE_SERVICE_ROLE_KEY o OPENAI_API_KEY
And eventuali valori PII sono mascherati
```

## 6. Validation Coverage Matrix

| Area | Strumento | Frequenza | Evidenza |
|------|-----------|-----------|----------|
| Secrets CI | GitHub Actions secrets audit, trufflehog | Per push, settimanale | Report scan + screenshot settings |
| OpenAI health | curl + quota dashboard | Per run E2E | Log job + screenshot usage |
| Supabase health | connectivity/integrity scripts | Daily/nightly | `reports/db_connectivity_test_*.log` |
| Sync job | pytest integration | Ogni pipeline `develop` | JUnit report |
| E2E chat | Playwright/pytest | Ogni PR mainline | HTML/JUnit report |
| P95 metrics | k6 script + dashboard | Post deploy & weekly | `reports/metrics-p95-*.md`, screenshot |
| Log hygiene | Manual + automated scan | Ogni release | Checklist redaction |

## 7. Test Data & Environment

- **Supabase dataset:** utenti test, documenti knowledge base sintetici, job sync abilitati.  
- **OpenAI:** chiave test con budget max (es. 50 USD/mese); preferire modello `gpt-4o-mini` o simile per costi E2E.  
- **Env files:** `.env.test.local` (locale), secrets CI `TEST_*`.  
- **Monitoring tools:** Supabase dashboard, OpenAI usage charts, internal Grafana (se disponibile).

## 8. Tooling & Automation

- **CI Workflows:** estendere pipeline esistente con step `validate-openai`, `supabase-health`, `run-e2e`, `collect-p95`.  
- **Scripts:** `scripts/validation/database_connectivity_test.py`, `scripts/validation/database_integrity_audit.py`, `scripts/perf/p95_local_test.js`, `scripts/perf/summarize_p95.py`.  
- **Scanning:** `trufflehog`, `detect-secrets` per log/repository; `jq` per parsing API.  
- **Reporting:** esportare JUnit, HTML e snapshot in `reports/` seguendo naming YYYYMMDD.

## 9. Exit Criteria

- Tutti gli scenari TD-OPENAI-01/02, TD-SUPA-01/02/03, TD-E2E-01/02, TD-P95-01/02, TD-SEC-01/02 PASS.  
- Nessun secret in log/report; audit trufflehog clean.  
- P95 reale rilevato <= target definito (8s) con delta ≤10% vs dashboard Supabase.  
- Documentazione aggiornata con evidenze (report, screenshot, checklist).  
- Monitoring attivo con alert configurati per quota OpenAI, errori 429/5xx, job sync failure.

## 10. Open Questions / Notes

1. Confermare modello OpenAI da usare per test (potrebbe differire da produzione per costi).  
2. Definire soglia minima di richieste per considerare la metrica P95 “valida” (es. ≥300).  
3. Stabilire chi gestisce la rotazione secrets (`Owner`) e la frequenza (monthly/quarterly).  
4. Valutare se i test E2E devono coprire anche fallback/resilienza quando OpenAI non risponde (scenario negativo).

---
**Hook:** `Test design: docs/qa/assessments/2.8-test-design-20251013.md`

